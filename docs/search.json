[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "REPORTE QUARTO",
    "section": "",
    "text": "print(\"Hola Mundo\")\n\nfor i in range(10):\n    print(f\"Elemento del for i: {i}\")\n\nHola Mundo\nElemento del for i: 0\nElemento del for i: 1\nElemento del for i: 2\nElemento del for i: 3\nElemento del for i: 4\nElemento del for i: 5\nElemento del for i: 6\nElemento del for i: 7\nElemento del for i: 8\nElemento del for i: 9\n\n\n\nimport altair as alt\n\nfrom vega_datasets import data\ncars = data.cars()\nprint(cars.head())\n\n                        Name  Miles_per_Gallon  Cylinders  Displacement  \\\n0  chevrolet chevelle malibu              18.0          8         307.0   \n1          buick skylark 320              15.0          8         350.0   \n2         plymouth satellite              18.0          8         318.0   \n3              amc rebel sst              16.0          8         304.0   \n4                ford torino              17.0          8         302.0   \n\n   Horsepower  Weight_in_lbs  Acceleration       Year Origin  \n0       130.0           3504          12.0 1970-01-01    USA  \n1       165.0           3693          11.5 1970-01-01    USA  \n2       150.0           3436          11.0 1970-01-01    USA  \n3       150.0           3433          12.0 1970-01-01    USA  \n4       140.0           3449          10.5 1970-01-01    USA  \n\n\n\nalt.Chart(cars).mark_point().encode(\n    x=\"Miles_per_Gallon\"\n)\n\n\n\n\n\n\n\n\nalt.Chart(cars).mark_point().encode(\n    x=\"Miles_per_Gallon\",\n    y=\"Horsepower\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Horsepower\"]\n).interactive()\n\n\n\n\n\n\n\n\nalt.Chart(cars).mark_point().encode(\n    x=\"Miles_per_Gallon\",\n    y=\"Horsepower\",\n    color=\"Origin\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Horsepower\"]\n).interactive()\n\n\n\n\n\n\n\n\nalt.Chart(cars).mark_point().encode(\n    x=\"Weight_in_lbs\",\n    y=\"Miles_per_Gallon\",\n    color=\"Origin\",\n    shape=\"Origin\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Weight_in_lbs\"]\n).interactive()\n\n\n\n\n\n\n\n\nalt.Chart(cars).mark_point().encode(\n    x=\"Weight_in_lbs\"\n)\n\n\n\n\n\n\n\n\nalt.Chart(cars).mark_point().encode(\n    x=\"Weight_in_lbs\",\n    y=\"Miles_per_Gallon\",\n    color=\"Origin\",\n    shape=\"Origin\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Weight_in_lbs\"]\n).interactive()\n\n\n\n\n\n\n\n\nalt.Chart(cars).mark_point().encode(\n    x=\"Weight_in_lbs\",\n    y=\"Miles_per_Gallon\",\n    color=\"Origin\",\n    shape=\"Origin\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Weight_in_lbs\"]\n).interactive()\n\n\n\n\n\n\n\n\nalt.Chart(cars).mark_point().encode(\n    x=\"Weight_in_lbs\",\n    y=\"Miles_per_Gallon\",\n    color=\"Origin\",\n    shape=\"Origin\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Weight_in_lbs\"]\n).interactive()\n\n\n\n\n\n\n\n\nalt.Chart(cars).mark_point().encode(\n    x=\"Weight_in_lbs\",\n    y=\"Miles_per_Gallon\",\n    color=\"Origin\",\n    shape=\"Origin\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Weight_in_lbs\"]\n).interactive()\n\n\n\n\n\n\n\n\nalt.Chart(cars).mark_point().encode(\n    x=\"Weight_in_lbs\",\n    y=\"Miles_per_Gallon\",\n    color=\"Origin\",\n    shape=\"Origin\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Weight_in_lbs\"]\n).interactive()\n\n\n\n\n\n\n\n\nalt.Chart(cars).mark_point().encode(\n    x=\"Miles_per_Gallon\",\n    y=\"Horsepower\",\n    color=\"Origin\",\n    shape=\"Origin\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Horsepower\"]\n).interactive()\n\n\n\n\n\n\n\n\n\n\nGrafico de barra\nNúmero de Obervaciones\n\n\ncount() De Cada uno de los Origenes\n\nalt.Chart(cars).mark_bar().encode(\n    alt.X('Origin',sort='-y'),\n    alt.Y('count()')\n) \n\n\n\n\n\n\n\n\n\n\nmean() de cada uno de los origines\n\nalt.Chart(cars).mark_bar().encode(\n    alt.X('Origin',sort='-y'),\n    alt.Y('mean(Weight_in_lbs)'),\n    tooltip=['Origin','mean(Weight_in_lbs)']\n).properties(\n    width=200,\n    height=200\n)\n\n\n\n\n\n\n\n\n\n\n\nalt.Chart(cars).mark_line(point=True).encode(\n    alt.X('Year'),\n    alt.Y('mean(Weight_in_lbs)'),\n    alt.Color('Origin'),\n    tooltip=['Origin','mean(Weight_in_lbs)']\n).properties(\n    width=600,\n    height=600\n).interactive()",
    "crumbs": [
      "Home",
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#graficas-de-prueba",
    "href": "index.html#graficas-de-prueba",
    "title": "REPORTE QUARTO",
    "section": "",
    "text": "print(\"Hola Mundo\")\n\nfor i in range(10):\n    print(f\"Elemento del for i: {i}\")\n\nHola Mundo\nElemento del for i: 0\nElemento del for i: 1\nElemento del for i: 2\nElemento del for i: 3\nElemento del for i: 4\nElemento del for i: 5\nElemento del for i: 6\nElemento del for i: 7\nElemento del for i: 8\nElemento del for i: 9\n\n\n\nimport altair as alt\n\nfrom vega_datasets import data\ncars = data.cars()\nprint(cars.head())\n\n                        Name  Miles_per_Gallon  Cylinders  Displacement  \\\n0  chevrolet chevelle malibu              18.0          8         307.0   \n1          buick skylark 320              15.0          8         350.0   \n2         plymouth satellite              18.0          8         318.0   \n3              amc rebel sst              16.0          8         304.0   \n4                ford torino              17.0          8         302.0   \n\n   Horsepower  Weight_in_lbs  Acceleration       Year Origin  \n0       130.0           3504          12.0 1970-01-01    USA  \n1       165.0           3693          11.5 1970-01-01    USA  \n2       150.0           3436          11.0 1970-01-01    USA  \n3       150.0           3433          12.0 1970-01-01    USA  \n4       140.0           3449          10.5 1970-01-01    USA  \n\n\n\nalt.Chart(cars).mark_point().encode(\n    x=\"Miles_per_Gallon\"\n)\n\n\n\n\n\n\n\n\nalt.Chart(cars).mark_point().encode(\n    x=\"Miles_per_Gallon\",\n    y=\"Horsepower\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Horsepower\"]\n).interactive()\n\n\n\n\n\n\n\n\nalt.Chart(cars).mark_point().encode(\n    x=\"Miles_per_Gallon\",\n    y=\"Horsepower\",\n    color=\"Origin\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Horsepower\"]\n).interactive()\n\n\n\n\n\n\n\n\nalt.Chart(cars).mark_point().encode(\n    x=\"Weight_in_lbs\",\n    y=\"Miles_per_Gallon\",\n    color=\"Origin\",\n    shape=\"Origin\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Weight_in_lbs\"]\n).interactive()\n\n\n\n\n\n\n\n\nalt.Chart(cars).mark_point().encode(\n    x=\"Weight_in_lbs\"\n)\n\n\n\n\n\n\n\n\nalt.Chart(cars).mark_point().encode(\n    x=\"Weight_in_lbs\",\n    y=\"Miles_per_Gallon\",\n    color=\"Origin\",\n    shape=\"Origin\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Weight_in_lbs\"]\n).interactive()\n\n\n\n\n\n\n\n\nalt.Chart(cars).mark_point().encode(\n    x=\"Weight_in_lbs\",\n    y=\"Miles_per_Gallon\",\n    color=\"Origin\",\n    shape=\"Origin\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Weight_in_lbs\"]\n).interactive()\n\n\n\n\n\n\n\n\nalt.Chart(cars).mark_point().encode(\n    x=\"Weight_in_lbs\",\n    y=\"Miles_per_Gallon\",\n    color=\"Origin\",\n    shape=\"Origin\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Weight_in_lbs\"]\n).interactive()\n\n\n\n\n\n\n\n\nalt.Chart(cars).mark_point().encode(\n    x=\"Weight_in_lbs\",\n    y=\"Miles_per_Gallon\",\n    color=\"Origin\",\n    shape=\"Origin\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Weight_in_lbs\"]\n).interactive()\n\n\n\n\n\n\n\n\nalt.Chart(cars).mark_point().encode(\n    x=\"Weight_in_lbs\",\n    y=\"Miles_per_Gallon\",\n    color=\"Origin\",\n    shape=\"Origin\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Weight_in_lbs\"]\n).interactive()\n\n\n\n\n\n\n\n\nalt.Chart(cars).mark_point().encode(\n    x=\"Miles_per_Gallon\",\n    y=\"Horsepower\",\n    color=\"Origin\",\n    shape=\"Origin\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Horsepower\"]\n).interactive()",
    "crumbs": [
      "Home",
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#agregaciones",
    "href": "index.html#agregaciones",
    "title": "REPORTE QUARTO",
    "section": "",
    "text": "Grafico de barra\nNúmero de Obervaciones\n\n\ncount() De Cada uno de los Origenes\n\nalt.Chart(cars).mark_bar().encode(\n    alt.X('Origin',sort='-y'),\n    alt.Y('count()')\n) \n\n\n\n\n\n\n\n\n\n\nmean() de cada uno de los origines\n\nalt.Chart(cars).mark_bar().encode(\n    alt.X('Origin',sort='-y'),\n    alt.Y('mean(Weight_in_lbs)'),\n    tooltip=['Origin','mean(Weight_in_lbs)']\n).properties(\n    width=200,\n    height=200\n)\n\n\n\n\n\n\n\n\n\n\n\nalt.Chart(cars).mark_line(point=True).encode(\n    alt.X('Year'),\n    alt.Y('mean(Weight_in_lbs)'),\n    alt.Color('Origin'),\n    tooltip=['Origin','mean(Weight_in_lbs)']\n).properties(\n    width=600,\n    height=600\n).interactive()",
    "crumbs": [
      "Home",
      "Welcome"
    ]
  },
  {
    "objectID": "Task/Task1-AG.html",
    "href": "Task/Task1-AG.html",
    "title": "Task 1 Machine Learning",
    "section": "",
    "text": "Calculate and compare basic statistics (mean, standard deviation, variance) for the variables X and Y in the four subsets (Series I, II, III, and IV), and propose appropriate visualizations based on your findings.\n\n\n\n\n\n\nMean\nStandard deviation\nVariance #### Based on your results, answer the following questions:\nWhich datasets appear most similar based on their statistical properties?\nWhich one might show a different behavior or pattern?\nAre there signs of potential outliers or anomalies? #### Propose one or more visualizations that would help you better explore the relationships between X and Y in each dataset. You may use any visualization libraries you prefer (e.g., matplotlib, seaborn, altair, etc.). Note: Focus on reasoning from the data and your statistical analysis — no prior assumptions about the datasets are needed.\n\n\n\nCode\n%pip install vega_datasets\n\n# Importar librerías y cargar datos\nimport pandas as pd\nfrom vega_datasets import data\n\n# Cargar dataset\nanscombe = data.anscombe()\nanscombe\n\n\n\nRequirement already satisfied: vega_datasets in /opt/homebrew/lib/python3.11/site-packages (0.9.0)\n\nRequirement already satisfied: pandas in /opt/homebrew/lib/python3.11/site-packages (from vega_datasets) (2.3.2)\n\nRequirement already satisfied: numpy&gt;=1.23.2 in /opt/homebrew/lib/python3.11/site-packages (from pandas-&gt;vega_datasets) (2.2.6)\n\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /Users/usr-s3c/Library/Python/3.11/lib/python/site-packages (from pandas-&gt;vega_datasets) (2.9.0.post0)\n\nRequirement already satisfied: pytz&gt;=2020.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas-&gt;vega_datasets) (2025.2)\n\nRequirement already satisfied: tzdata&gt;=2022.7 in /opt/homebrew/lib/python3.11/site-packages (from pandas-&gt;vega_datasets) (2025.2)\n\nRequirement already satisfied: six&gt;=1.5 in /opt/homebrew/lib/python3.11/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas-&gt;vega_datasets) (1.16.0)\n\n\n\n[notice] A new release of pip is available: 25.1.1 -&gt; 25.2\n\n[notice] To update, run: /opt/homebrew/opt/python@3.11/bin/python3.11 -m pip install --upgrade pip\n\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n\n\n\n\n\n\n\n\nSeries\nX\nY\n\n\n\n\n0\nI\n10\n8.04\n\n\n1\nI\n8\n6.95\n\n\n2\nI\n13\n7.58\n\n\n3\nI\n9\n8.81\n\n\n4\nI\n11\n8.33\n\n\n5\nI\n14\n9.96\n\n\n6\nI\n6\n7.24\n\n\n7\nI\n4\n4.26\n\n\n8\nI\n12\n10.84\n\n\n9\nI\n7\n4.81\n\n\n10\nI\n5\n5.68\n\n\n11\nII\n10\n9.14\n\n\n12\nII\n8\n8.14\n\n\n13\nII\n13\n8.74\n\n\n14\nII\n9\n8.77\n\n\n15\nII\n11\n9.26\n\n\n16\nII\n14\n8.10\n\n\n17\nII\n6\n6.13\n\n\n18\nII\n4\n3.10\n\n\n19\nII\n12\n9.13\n\n\n20\nII\n7\n7.26\n\n\n21\nII\n5\n4.74\n\n\n22\nIII\n10\n7.46\n\n\n23\nIII\n8\n6.77\n\n\n24\nIII\n13\n12.74\n\n\n25\nIII\n9\n7.11\n\n\n26\nIII\n11\n7.81\n\n\n27\nIII\n14\n8.84\n\n\n28\nIII\n6\n6.08\n\n\n29\nIII\n4\n5.39\n\n\n30\nIII\n12\n8.15\n\n\n31\nIII\n7\n6.42\n\n\n32\nIII\n5\n5.73\n\n\n33\nIV\n8\n6.58\n\n\n34\nIV\n8\n5.76\n\n\n35\nIV\n8\n7.71\n\n\n36\nIV\n8\n8.84\n\n\n37\nIV\n8\n8.47\n\n\n38\nIV\n8\n7.04\n\n\n39\nIV\n8\n5.25\n\n\n40\nIV\n19\n12.50\n\n\n41\nIV\n8\n5.56\n\n\n42\nIV\n8\n7.91\n\n\n43\nIV\n8\n6.89\n\n\n\n\n\n\n\n\n\nCode\nstats = anscombe.groupby(\"Series\").agg(\n    mean_x=(\"X\",\"mean\"),\n    std_x=(\"X\",\"std\"),\n    var_x=(\"X\",\"var\"),\n    mean_y=(\"Y\",\"mean\"),\n    std_y=(\"Y\",\"std\"),\n    var_y=(\"Y\",\"var\")\n).reset_index()\n\nprint(stats)\n\n\n  Series  mean_x     std_x  var_x    mean_y     std_y     var_y\n0      I     9.0  3.316625   11.0  7.500000  2.032890  4.132640\n1     II     9.0  3.316625   11.0  7.500909  2.031657  4.127629\n2    III     9.0  3.316625   11.0  7.500000  2.030424  4.122620\n3     IV     9.0  3.316625   11.0  7.500909  2.030579  4.123249\n\n\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.lmplot(data=anscombe, x=\"X\", y=\"Y\", col=\"Series\",\n           hue=\"Series\", col_wrap=2, height=8,\n           fit_reg=False)\nplt.suptitle(\"Scatter plots por Serie\", y=1.02)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(8,6))\nsns.scatterplot(data=anscombe, x=\"X\", y=\"Y\", hue=\"Series\", style=\"Series\", s=80)\n\nplt.title(\"Scatter plot combinado de todas las Series\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.legend(title=\"Series\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(1, 2, figsize=(14,6))\n\nfor idx, var in enumerate([\"X\", \"Y\"]):\n    ax = axes[idx]\n    sns.boxplot(data=anscombe, x=\"Series\", y=var, ax=ax)\n    ax.set_title(f\"Distribución de {var} con outliers\")\n    \n    # Para cada serie, detectamos y marcamos los outliers\n    for serie, group in anscombe.groupby(\"Series\"):\n        q1, q3 = group[var].quantile([0.25, 0.75])\n        iqr = q3 - q1\n        low, high = q1 - 1.5*iqr, q3 + 1.5*iqr\n        \n        # Filtrar outliers\n        outliers = group[(group[var] &lt; low) | (group[var] &gt; high)]\n        \n        # Anotar cada outlier encontrado\n        for i, row in outliers.iterrows():\n            ax.text(\n                x=[\"I\",\"II\",\"III\",\"IV\"].index(serie),  # posición en eje X\n                y=row[var] + 0.3,                      # un poco arriba del punto\n                s=f\"{row[var]}\",                       # valor numérico\n                color=\"red\", ha=\"center\", fontsize=9, fontweight=\"bold\"\n            )\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Ajustamos estilo\nsns.set_style(\"whitegrid\")\n\n# Crear figura con 2 filas (X y Y) y 4 columnas (Series I–IV)\nfig, axes = plt.subplots(2, 4, figsize=(16, 8))\n\n# Fila superior: distribuciones de X\nfor idx, serie in enumerate(anscombe[\"Series\"].unique()):\n    data = anscombe[anscombe[\"Series\"] == serie]\n    sns.histplot(data=data, x=\"X\", bins=6, kde=True, color=\"skyblue\", ax=axes[0, idx])\n    axes[0, idx].set_title(f\"Serie {serie} - Distribución de X\")\n    axes[0, idx].set_ylabel(\"Frecuencia\")\n    axes[0, idx].set_xlabel(\"X\")\n\n# Fila inferior: distribuciones de Y\nfor idx, serie in enumerate(anscombe[\"Series\"].unique()):\n    data = anscombe[anscombe[\"Series\"] == serie]\n    sns.histplot(data=data, x=\"Y\", bins=6, kde=True, color=\"lightgreen\", ax=axes[1, idx])\n    axes[1, idx].set_title(f\"Serie {serie} - Distribución de Y\")\n    axes[1, idx].set_ylabel(\"Frecuencia\")\n    axes[1, idx].set_xlabel(\"Y\")\n\n# Título general\nplt.suptitle(\"Distribución de frecuencias de X y Y por serie\", fontsize=16, y=1.02)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport altair as alt\n\n# Si tu DataFrame se llama 'anscombe' y tiene columnas: Series, X, Y\n\n# 4 paneles (I, II, III, IV), puntos interactivos y tooltips\nchart = (\n    alt.Chart(anscombe)\n    .mark_point(size=60)\n    .encode(\n        x=alt.X('X:Q', scale=alt.Scale(zero=False), axis=alt.Axis(title='X')),\n        y=alt.Y('Y:Q', scale=alt.Scale(zero=False), axis=alt.Axis(title='Y')),\n        color=alt.Color('Series:N', legend=alt.Legend(title='Series')),\n        tooltip=['Series:N', 'X:Q', 'Y:Q']\n    )\n    .facet(column=alt.Column('Series:N', title='Series'))  # 4 paneles en fila\n    .properties(\n        title='Cuarteto de Anscombe – Misma estadística, diferentes distribuciones'\n    )\n    .interactive()\n)\n\nchart\n\n\n\n\n\n\n\n\n\n\nCode\nimport altair as alt\n\nbox_x = alt.Chart(anscombe).mark_boxplot().encode(\n    x='Series:N',\n    y='X:Q',\n    color='Series:N'\n).properties(title=\"Boxplot de X por Serie\")\n\nbox_y = alt.Chart(anscombe).mark_boxplot().encode(\n    x='Series:N',\n    y='Y:Q',\n    color='Series:N'\n).properties(title=\"Boxplot de Y por Serie\")\n\nbox_x | box_y\n\n\n\n\n\n\n\n\n\n\nCode\nregression = alt.Chart(anscombe).transform_regression(\n    'X', 'Y', groupby=['Series'], params=True\n).mark_line().encode(\n    x='X',\n    y='Y',\n    color='Series:N'\n)\n\npoints = alt.Chart(anscombe).mark_point().encode(\n    x='X',\n    y='Y',\n    color='Series:N'\n)\n\n(points + regression).facet('Series:N', columns=2)\n\n\n\n\n\n\n\n\n\n\nCode\nimport altair as alt\n\nreg_line = alt.Chart(anscombe).transform_regression(\n    'X', 'Y', groupby=['Series'], extent=[4, 20]\n).mark_line(size=2).encode(\n    x='X',\n    y='Y',\n    color='Series:N'\n)\n\npoints = alt.Chart(anscombe).mark_circle(size=70).encode(\n    x='X',\n    y='Y',\n    color='Series:N',\n    tooltip=['Series','X','Y']\n)\n\n(points + reg_line).facet('Series:N', columns=2).properties(\n    title=\"Anscombe: scatter + regresión lineal\"\n).interactive()\n\n\n\n\n\n\n\n\n\n\n\nCode\nresiduals = alt.Chart(anscombe).transform_regression(\n    'X', 'Y', groupby=['Series'], params=True\n).transform_calculate(\n    residual=\"datum.Y - datum.Y_pred\"\n).mark_circle(size=70).encode(\n    x='X',\n    y='residual:Q',\n    color='Series:N'\n).facet('Series:N', columns=2).properties(\n    title=\"Anscombe: Residuales del ajuste lineal\"\n)\nresiduals\n\n\n\n\n\n\n\n\n\n\nCode\nviolin_x = alt.Chart(anscombe).transform_density(\n    'X', groupby=['Series'], as_=['X','density']\n).mark_area(orient='horizontal').encode(\n    y='X:Q',\n    x=alt.X('density:Q', stack='center'),\n    color='Series:N'\n).facet('Series:N', columns=2).properties(title=\"Distribución de X por Serie\")\n\nviolin_y = alt.Chart(anscombe).transform_density(\n    'Y', groupby=['Series'], as_=['Y','density']\n).mark_area(orient='horizontal').encode(\n    y='Y:Q',\n    x=alt.X('density:Q', stack='center'),\n    color='Series:N'\n).facet('Series:N', columns=2).properties(title=\"Distribución de Y por Serie\")\n\nviolin_x & violin_y\n\n\n\n\n\n\n\n\n\n\nCode\nimport pandas as pd\n\n# Calcular correlaciones\ncorrs = anscombe.groupby(\"Series\").apply(\n    lambda g: pd.Series({\n        \"Pearson\": g[\"X\"].corr(g[\"Y\"], method=\"pearson\"),\n        \"Spearman\": g[\"X\"].corr(g[\"Y\"], method=\"spearman\")\n    })\n).reset_index()\n\nheatmap = alt.Chart(corrs.melt('Series')).mark_rect().encode(\n    x='variable:N',\n    y='Series:N',\n    color='value:Q'\n).properties(title=\"Correlaciones Pearson vs Spearman\")\n\ntext = alt.Chart(corrs.melt('Series')).mark_text(baseline='middle').encode(\n    x='variable:N',\n    y='Series:N',\n    text=alt.Text('value:Q', format=\".2f\"),\n    color=alt.value('black')\n)\n\nheatmap + text\n\n\n/var/folders/8j/llgk77912n1_x415m3ds8q7h0000gn/T/ipykernel_82735/3816396753.py:4: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  corrs = anscombe.groupby(\"Series\").apply("
  },
  {
    "objectID": "Task/Task1-AG.html#maestría-ia-27-08-2025",
    "href": "Task/Task1-AG.html#maestría-ia-27-08-2025",
    "title": "Task 1 Machine Learning",
    "section": "",
    "text": "Calculate and compare basic statistics (mean, standard deviation, variance) for the variables X and Y in the four subsets (Series I, II, III, and IV), and propose appropriate visualizations based on your findings.\n\n\n\n\n\n\nMean\nStandard deviation\nVariance #### Based on your results, answer the following questions:\nWhich datasets appear most similar based on their statistical properties?\nWhich one might show a different behavior or pattern?\nAre there signs of potential outliers or anomalies? #### Propose one or more visualizations that would help you better explore the relationships between X and Y in each dataset. You may use any visualization libraries you prefer (e.g., matplotlib, seaborn, altair, etc.). Note: Focus on reasoning from the data and your statistical analysis — no prior assumptions about the datasets are needed.\n\n\n\nCode\n%pip install vega_datasets\n\n# Importar librerías y cargar datos\nimport pandas as pd\nfrom vega_datasets import data\n\n# Cargar dataset\nanscombe = data.anscombe()\nanscombe\n\n\n\nRequirement already satisfied: vega_datasets in /opt/homebrew/lib/python3.11/site-packages (0.9.0)\n\nRequirement already satisfied: pandas in /opt/homebrew/lib/python3.11/site-packages (from vega_datasets) (2.3.2)\n\nRequirement already satisfied: numpy&gt;=1.23.2 in /opt/homebrew/lib/python3.11/site-packages (from pandas-&gt;vega_datasets) (2.2.6)\n\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /Users/usr-s3c/Library/Python/3.11/lib/python/site-packages (from pandas-&gt;vega_datasets) (2.9.0.post0)\n\nRequirement already satisfied: pytz&gt;=2020.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas-&gt;vega_datasets) (2025.2)\n\nRequirement already satisfied: tzdata&gt;=2022.7 in /opt/homebrew/lib/python3.11/site-packages (from pandas-&gt;vega_datasets) (2025.2)\n\nRequirement already satisfied: six&gt;=1.5 in /opt/homebrew/lib/python3.11/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas-&gt;vega_datasets) (1.16.0)\n\n\n\n[notice] A new release of pip is available: 25.1.1 -&gt; 25.2\n\n[notice] To update, run: /opt/homebrew/opt/python@3.11/bin/python3.11 -m pip install --upgrade pip\n\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n\n\n\n\n\n\n\n\nSeries\nX\nY\n\n\n\n\n0\nI\n10\n8.04\n\n\n1\nI\n8\n6.95\n\n\n2\nI\n13\n7.58\n\n\n3\nI\n9\n8.81\n\n\n4\nI\n11\n8.33\n\n\n5\nI\n14\n9.96\n\n\n6\nI\n6\n7.24\n\n\n7\nI\n4\n4.26\n\n\n8\nI\n12\n10.84\n\n\n9\nI\n7\n4.81\n\n\n10\nI\n5\n5.68\n\n\n11\nII\n10\n9.14\n\n\n12\nII\n8\n8.14\n\n\n13\nII\n13\n8.74\n\n\n14\nII\n9\n8.77\n\n\n15\nII\n11\n9.26\n\n\n16\nII\n14\n8.10\n\n\n17\nII\n6\n6.13\n\n\n18\nII\n4\n3.10\n\n\n19\nII\n12\n9.13\n\n\n20\nII\n7\n7.26\n\n\n21\nII\n5\n4.74\n\n\n22\nIII\n10\n7.46\n\n\n23\nIII\n8\n6.77\n\n\n24\nIII\n13\n12.74\n\n\n25\nIII\n9\n7.11\n\n\n26\nIII\n11\n7.81\n\n\n27\nIII\n14\n8.84\n\n\n28\nIII\n6\n6.08\n\n\n29\nIII\n4\n5.39\n\n\n30\nIII\n12\n8.15\n\n\n31\nIII\n7\n6.42\n\n\n32\nIII\n5\n5.73\n\n\n33\nIV\n8\n6.58\n\n\n34\nIV\n8\n5.76\n\n\n35\nIV\n8\n7.71\n\n\n36\nIV\n8\n8.84\n\n\n37\nIV\n8\n8.47\n\n\n38\nIV\n8\n7.04\n\n\n39\nIV\n8\n5.25\n\n\n40\nIV\n19\n12.50\n\n\n41\nIV\n8\n5.56\n\n\n42\nIV\n8\n7.91\n\n\n43\nIV\n8\n6.89\n\n\n\n\n\n\n\n\n\nCode\nstats = anscombe.groupby(\"Series\").agg(\n    mean_x=(\"X\",\"mean\"),\n    std_x=(\"X\",\"std\"),\n    var_x=(\"X\",\"var\"),\n    mean_y=(\"Y\",\"mean\"),\n    std_y=(\"Y\",\"std\"),\n    var_y=(\"Y\",\"var\")\n).reset_index()\n\nprint(stats)\n\n\n  Series  mean_x     std_x  var_x    mean_y     std_y     var_y\n0      I     9.0  3.316625   11.0  7.500000  2.032890  4.132640\n1     II     9.0  3.316625   11.0  7.500909  2.031657  4.127629\n2    III     9.0  3.316625   11.0  7.500000  2.030424  4.122620\n3     IV     9.0  3.316625   11.0  7.500909  2.030579  4.123249\n\n\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.lmplot(data=anscombe, x=\"X\", y=\"Y\", col=\"Series\",\n           hue=\"Series\", col_wrap=2, height=8,\n           fit_reg=False)\nplt.suptitle(\"Scatter plots por Serie\", y=1.02)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(8,6))\nsns.scatterplot(data=anscombe, x=\"X\", y=\"Y\", hue=\"Series\", style=\"Series\", s=80)\n\nplt.title(\"Scatter plot combinado de todas las Series\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.legend(title=\"Series\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(1, 2, figsize=(14,6))\n\nfor idx, var in enumerate([\"X\", \"Y\"]):\n    ax = axes[idx]\n    sns.boxplot(data=anscombe, x=\"Series\", y=var, ax=ax)\n    ax.set_title(f\"Distribución de {var} con outliers\")\n    \n    # Para cada serie, detectamos y marcamos los outliers\n    for serie, group in anscombe.groupby(\"Series\"):\n        q1, q3 = group[var].quantile([0.25, 0.75])\n        iqr = q3 - q1\n        low, high = q1 - 1.5*iqr, q3 + 1.5*iqr\n        \n        # Filtrar outliers\n        outliers = group[(group[var] &lt; low) | (group[var] &gt; high)]\n        \n        # Anotar cada outlier encontrado\n        for i, row in outliers.iterrows():\n            ax.text(\n                x=[\"I\",\"II\",\"III\",\"IV\"].index(serie),  # posición en eje X\n                y=row[var] + 0.3,                      # un poco arriba del punto\n                s=f\"{row[var]}\",                       # valor numérico\n                color=\"red\", ha=\"center\", fontsize=9, fontweight=\"bold\"\n            )\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Ajustamos estilo\nsns.set_style(\"whitegrid\")\n\n# Crear figura con 2 filas (X y Y) y 4 columnas (Series I–IV)\nfig, axes = plt.subplots(2, 4, figsize=(16, 8))\n\n# Fila superior: distribuciones de X\nfor idx, serie in enumerate(anscombe[\"Series\"].unique()):\n    data = anscombe[anscombe[\"Series\"] == serie]\n    sns.histplot(data=data, x=\"X\", bins=6, kde=True, color=\"skyblue\", ax=axes[0, idx])\n    axes[0, idx].set_title(f\"Serie {serie} - Distribución de X\")\n    axes[0, idx].set_ylabel(\"Frecuencia\")\n    axes[0, idx].set_xlabel(\"X\")\n\n# Fila inferior: distribuciones de Y\nfor idx, serie in enumerate(anscombe[\"Series\"].unique()):\n    data = anscombe[anscombe[\"Series\"] == serie]\n    sns.histplot(data=data, x=\"Y\", bins=6, kde=True, color=\"lightgreen\", ax=axes[1, idx])\n    axes[1, idx].set_title(f\"Serie {serie} - Distribución de Y\")\n    axes[1, idx].set_ylabel(\"Frecuencia\")\n    axes[1, idx].set_xlabel(\"Y\")\n\n# Título general\nplt.suptitle(\"Distribución de frecuencias de X y Y por serie\", fontsize=16, y=1.02)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport altair as alt\n\n# Si tu DataFrame se llama 'anscombe' y tiene columnas: Series, X, Y\n\n# 4 paneles (I, II, III, IV), puntos interactivos y tooltips\nchart = (\n    alt.Chart(anscombe)\n    .mark_point(size=60)\n    .encode(\n        x=alt.X('X:Q', scale=alt.Scale(zero=False), axis=alt.Axis(title='X')),\n        y=alt.Y('Y:Q', scale=alt.Scale(zero=False), axis=alt.Axis(title='Y')),\n        color=alt.Color('Series:N', legend=alt.Legend(title='Series')),\n        tooltip=['Series:N', 'X:Q', 'Y:Q']\n    )\n    .facet(column=alt.Column('Series:N', title='Series'))  # 4 paneles en fila\n    .properties(\n        title='Cuarteto de Anscombe – Misma estadística, diferentes distribuciones'\n    )\n    .interactive()\n)\n\nchart\n\n\n\n\n\n\n\n\n\n\nCode\nimport altair as alt\n\nbox_x = alt.Chart(anscombe).mark_boxplot().encode(\n    x='Series:N',\n    y='X:Q',\n    color='Series:N'\n).properties(title=\"Boxplot de X por Serie\")\n\nbox_y = alt.Chart(anscombe).mark_boxplot().encode(\n    x='Series:N',\n    y='Y:Q',\n    color='Series:N'\n).properties(title=\"Boxplot de Y por Serie\")\n\nbox_x | box_y\n\n\n\n\n\n\n\n\n\n\nCode\nregression = alt.Chart(anscombe).transform_regression(\n    'X', 'Y', groupby=['Series'], params=True\n).mark_line().encode(\n    x='X',\n    y='Y',\n    color='Series:N'\n)\n\npoints = alt.Chart(anscombe).mark_point().encode(\n    x='X',\n    y='Y',\n    color='Series:N'\n)\n\n(points + regression).facet('Series:N', columns=2)\n\n\n\n\n\n\n\n\n\n\nCode\nimport altair as alt\n\nreg_line = alt.Chart(anscombe).transform_regression(\n    'X', 'Y', groupby=['Series'], extent=[4, 20]\n).mark_line(size=2).encode(\n    x='X',\n    y='Y',\n    color='Series:N'\n)\n\npoints = alt.Chart(anscombe).mark_circle(size=70).encode(\n    x='X',\n    y='Y',\n    color='Series:N',\n    tooltip=['Series','X','Y']\n)\n\n(points + reg_line).facet('Series:N', columns=2).properties(\n    title=\"Anscombe: scatter + regresión lineal\"\n).interactive()\n\n\n\n\n\n\n\n\n\n\n\nCode\nresiduals = alt.Chart(anscombe).transform_regression(\n    'X', 'Y', groupby=['Series'], params=True\n).transform_calculate(\n    residual=\"datum.Y - datum.Y_pred\"\n).mark_circle(size=70).encode(\n    x='X',\n    y='residual:Q',\n    color='Series:N'\n).facet('Series:N', columns=2).properties(\n    title=\"Anscombe: Residuales del ajuste lineal\"\n)\nresiduals\n\n\n\n\n\n\n\n\n\n\nCode\nviolin_x = alt.Chart(anscombe).transform_density(\n    'X', groupby=['Series'], as_=['X','density']\n).mark_area(orient='horizontal').encode(\n    y='X:Q',\n    x=alt.X('density:Q', stack='center'),\n    color='Series:N'\n).facet('Series:N', columns=2).properties(title=\"Distribución de X por Serie\")\n\nviolin_y = alt.Chart(anscombe).transform_density(\n    'Y', groupby=['Series'], as_=['Y','density']\n).mark_area(orient='horizontal').encode(\n    y='Y:Q',\n    x=alt.X('density:Q', stack='center'),\n    color='Series:N'\n).facet('Series:N', columns=2).properties(title=\"Distribución de Y por Serie\")\n\nviolin_x & violin_y\n\n\n\n\n\n\n\n\n\n\nCode\nimport pandas as pd\n\n# Calcular correlaciones\ncorrs = anscombe.groupby(\"Series\").apply(\n    lambda g: pd.Series({\n        \"Pearson\": g[\"X\"].corr(g[\"Y\"], method=\"pearson\"),\n        \"Spearman\": g[\"X\"].corr(g[\"Y\"], method=\"spearman\")\n    })\n).reset_index()\n\nheatmap = alt.Chart(corrs.melt('Series')).mark_rect().encode(\n    x='variable:N',\n    y='Series:N',\n    color='value:Q'\n).properties(title=\"Correlaciones Pearson vs Spearman\")\n\ntext = alt.Chart(corrs.melt('Series')).mark_text(baseline='middle').encode(\n    x='variable:N',\n    y='Series:N',\n    text=alt.Text('value:Q', format=\".2f\"),\n    color=alt.value('black')\n)\n\nheatmap + text\n\n\n/var/folders/8j/llgk77912n1_x415m3ds8q7h0000gn/T/ipykernel_82735/3816396753.py:4: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  corrs = anscombe.groupby(\"Series\").apply("
  },
  {
    "objectID": "PDF/Index.html",
    "href": "PDF/Index.html",
    "title": "REPORTE QUARTO",
    "section": "",
    "text": "Code\nprint(\"Hola Mundo\")\n\nfor i in range(10):\n    print(f\"Elemento del for i: {i}\")\n\n\nHola Mundo\nElemento del for i: 0\nElemento del for i: 1\nElemento del for i: 2\nElemento del for i: 3\nElemento del for i: 4\nElemento del for i: 5\nElemento del for i: 6\nElemento del for i: 7\nElemento del for i: 8\nElemento del for i: 9\n\n\n\n\nCode\nimport altair as alt\n\nfrom vega_datasets import data\ncars = data.cars()\nprint(cars.head())\n\n\n                        Name  Miles_per_Gallon  Cylinders  Displacement  \\\n0  chevrolet chevelle malibu              18.0          8         307.0   \n1          buick skylark 320              15.0          8         350.0   \n2         plymouth satellite              18.0          8         318.0   \n3              amc rebel sst              16.0          8         304.0   \n4                ford torino              17.0          8         302.0   \n\n   Horsepower  Weight_in_lbs  Acceleration       Year Origin  \n0       130.0           3504          12.0 1970-01-01    USA  \n1       165.0           3693          11.5 1970-01-01    USA  \n2       150.0           3436          11.0 1970-01-01    USA  \n3       150.0           3433          12.0 1970-01-01    USA  \n4       140.0           3449          10.5 1970-01-01    USA  \n\n\n\n\nCode\nalt.Chart(cars).mark_point().encode(\n    x=\"Miles_per_Gallon\"\n)\n\n\n\n\n\n\n\n\n\n\nCode\nalt.Chart(cars).mark_point().encode(\n    x=\"Miles_per_Gallon\",\n    y=\"Horsepower\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Horsepower\"]\n).interactive()\n\n\n\n\n\n\n\n\n\n\nCode\nalt.Chart(cars).mark_point().encode(\n    x=\"Miles_per_Gallon\",\n    y=\"Horsepower\",\n    color=\"Origin\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Horsepower\"]\n).interactive()\n\n\n\n\n\n\n\n\n\n\nCode\nalt.Chart(cars).mark_point().encode(\n    x=\"Weight_in_lbs\",\n    y=\"Miles_per_Gallon\",\n    color=\"Origin\",\n    shape=\"Origin\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Weight_in_lbs\"]\n).interactive()\n\n\n\n\n\n\n\n\n\nList\n\nPrimero"
  },
  {
    "objectID": "PDF/Index.html#graficas-de-prueba",
    "href": "PDF/Index.html#graficas-de-prueba",
    "title": "REPORTE QUARTO",
    "section": "",
    "text": "Code\nprint(\"Hola Mundo\")\n\nfor i in range(10):\n    print(f\"Elemento del for i: {i}\")\n\n\nHola Mundo\nElemento del for i: 0\nElemento del for i: 1\nElemento del for i: 2\nElemento del for i: 3\nElemento del for i: 4\nElemento del for i: 5\nElemento del for i: 6\nElemento del for i: 7\nElemento del for i: 8\nElemento del for i: 9\n\n\n\n\nCode\nimport altair as alt\n\nfrom vega_datasets import data\ncars = data.cars()\nprint(cars.head())\n\n\n                        Name  Miles_per_Gallon  Cylinders  Displacement  \\\n0  chevrolet chevelle malibu              18.0          8         307.0   \n1          buick skylark 320              15.0          8         350.0   \n2         plymouth satellite              18.0          8         318.0   \n3              amc rebel sst              16.0          8         304.0   \n4                ford torino              17.0          8         302.0   \n\n   Horsepower  Weight_in_lbs  Acceleration       Year Origin  \n0       130.0           3504          12.0 1970-01-01    USA  \n1       165.0           3693          11.5 1970-01-01    USA  \n2       150.0           3436          11.0 1970-01-01    USA  \n3       150.0           3433          12.0 1970-01-01    USA  \n4       140.0           3449          10.5 1970-01-01    USA  \n\n\n\n\nCode\nalt.Chart(cars).mark_point().encode(\n    x=\"Miles_per_Gallon\"\n)\n\n\n\n\n\n\n\n\n\n\nCode\nalt.Chart(cars).mark_point().encode(\n    x=\"Miles_per_Gallon\",\n    y=\"Horsepower\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Horsepower\"]\n).interactive()\n\n\n\n\n\n\n\n\n\n\nCode\nalt.Chart(cars).mark_point().encode(\n    x=\"Miles_per_Gallon\",\n    y=\"Horsepower\",\n    color=\"Origin\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Horsepower\"]\n).interactive()\n\n\n\n\n\n\n\n\n\n\nCode\nalt.Chart(cars).mark_point().encode(\n    x=\"Weight_in_lbs\",\n    y=\"Miles_per_Gallon\",\n    color=\"Origin\",\n    shape=\"Origin\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Weight_in_lbs\"]\n).interactive()\n\n\n\n\n\n\n\n\n\nList\n\nPrimero"
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "Exploratory Data Analysis (EDA) is one of the fundamental steps in any data science process. It allows us to understand the structure, detect anomalies, and uncover patterns in the data before modeling.\n\n“Without EDA, you’re not doing data science, you’re just guessing.”\n\nEDA combines statistics, programming, and visualization to explore datasets. This report is designed to help you practice these core skills using real-world data.\n\n\nWe will use the movies dataset from vega-datasets, which includes information about thousands of films such as their ratings, genres, duration, and box office revenue.\nLet’s load and preview the dataset:\n\n\nCode\nimport pandas as pd\nimport altair as alt\nfrom vega_datasets import data\n\n# Load dataset\nmovies = data.movies()\n\n# Show first rows\nmovies.head()\n\n\n\n\n\n\n\n\n\nTitle\nUS_Gross\nWorldwide_Gross\nUS_DVD_Sales\nProduction_Budget\nRelease_Date\nMPAA_Rating\nRunning_Time_min\nDistributor\nSource\nMajor_Genre\nCreative_Type\nDirector\nRotten_Tomatoes_Rating\nIMDB_Rating\nIMDB_Votes\n\n\n\n\n0\nThe Land Girls\n146083.0\n146083.0\nNaN\n8000000.0\nJun 12 1998\nR\nNaN\nGramercy\nNone\nNone\nNone\nNone\nNaN\n6.1\n1071.0\n\n\n1\nFirst Love, Last Rites\n10876.0\n10876.0\nNaN\n300000.0\nAug 07 1998\nR\nNaN\nStrand\nNone\nDrama\nNone\nNone\nNaN\n6.9\n207.0\n\n\n2\nI Married a Strange Person\n203134.0\n203134.0\nNaN\n250000.0\nAug 28 1998\nNone\nNaN\nLionsgate\nNone\nComedy\nNone\nNone\nNaN\n6.8\n865.0\n\n\n3\nLet's Talk About Sex\n373615.0\n373615.0\nNaN\n300000.0\nSep 11 1998\nNone\nNaN\nFine Line\nNone\nComedy\nNone\nNone\n13.0\nNaN\nNaN\n\n\n4\nSlam\n1009819.0\n1087521.0\nNaN\n1000000.0\nOct 09 1998\nR\nNaN\nTrimark\nOriginal Screenplay\nDrama\nContemporary Fiction\nNone\n62.0\n3.4\n165.0\n\n\n\n\n\n\n\nNow, let’s examine the shape (number of rows and columns) of the dataset:\n\n\nCode\nmovies.shape\n\n\n(3201, 16)\n\n\nThis tells us how many entries (rows) and features (columns) are present in the dataset.\n\n\n\nBefore diving deeper into the data, it’s useful to explore some key metadata:\n\n✅ The column names and their data types\n⚠️ The presence of missing values\n📊 Summary statistics for numeric columns\n\n\n\nUnderstanding the structure of the dataset helps us know what type of data we’re dealing with.\n\n\nCode\nmovies.dtypes\n\n\nTitle                      object\nUS_Gross                  float64\nWorldwide_Gross           float64\nUS_DVD_Sales              float64\nProduction_Budget         float64\nRelease_Date               object\nMPAA_Rating                object\nRunning_Time_min          float64\nDistributor                object\nSource                     object\nMajor_Genre                object\nCreative_Type              object\nDirector                   object\nRotten_Tomatoes_Rating    float64\nIMDB_Rating               float64\nIMDB_Votes                float64\ndtype: object\n\n\nWe can also use .info() for a more complete summary, including non-null counts:\n\n\nCode\n# Overview of the dataset\nmovies.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3201 entries, 0 to 3200\nData columns (total 16 columns):\n #   Column                  Non-Null Count  Dtype  \n---  ------                  --------------  -----  \n 0   Title                   3200 non-null   object \n 1   US_Gross                3194 non-null   float64\n 2   Worldwide_Gross         3194 non-null   float64\n 3   US_DVD_Sales            564 non-null    float64\n 4   Production_Budget       3200 non-null   float64\n 5   Release_Date            3201 non-null   object \n 6   MPAA_Rating             2596 non-null   object \n 7   Running_Time_min        1209 non-null   float64\n 8   Distributor             2969 non-null   object \n 9   Source                  2836 non-null   object \n 10  Major_Genre             2926 non-null   object \n 11  Creative_Type           2755 non-null   object \n 12  Director                1870 non-null   object \n 13  Rotten_Tomatoes_Rating  2321 non-null   float64\n 14  IMDB_Rating             2988 non-null   float64\n 15  IMDB_Votes              2988 non-null   float64\ndtypes: float64(8), object(8)\nmemory usage: 400.3+ KB\n\n\n\n\n\n\nDetecting and handling missing values is a critical step in any EDA process. Missing data can bias analysis or break downstream models if not handled properly.\n\nDetect patterns in missingness\nIdentify if some columns are almost entirely null\nDecide whether to drop or impute certain variables\n\n\n\nLet’s start by computing the percentage of missing values in each column:\n\n\nCode\nnan_percent = movies.isna().mean() * 100\nnan_percent_sorted = nan_percent.sort_values(ascending=False).round(2)\nnan_percent_sorted\n\n\nUS_DVD_Sales              82.38\nRunning_Time_min          62.23\nDirector                  41.58\nRotten_Tomatoes_Rating    27.49\nMPAA_Rating               18.90\nCreative_Type             13.93\nSource                    11.40\nMajor_Genre                8.59\nDistributor                7.25\nIMDB_Rating                6.65\nIMDB_Votes                 6.65\nUS_Gross                   0.22\nWorldwide_Gross            0.22\nTitle                      0.03\nProduction_Budget          0.03\nRelease_Date               0.00\ndtype: float64\n\n\n\n\n\nTo visualize missing values with Altair, we need to reshape the data into a long format where each missing value is a row:\n\n\nCode\nmovies_nans = movies.isna().reset_index().melt(\n    id_vars='index',\n    var_name='column',\n    value_name=\"NaN\"\n)\nmovies_nans\n\n\n\n\n\n\n\n\n\nindex\ncolumn\nNaN\n\n\n\n\n0\n0\nTitle\nFalse\n\n\n1\n1\nTitle\nFalse\n\n\n2\n2\nTitle\nFalse\n\n\n3\n3\nTitle\nFalse\n\n\n4\n4\nTitle\nFalse\n\n\n...\n...\n...\n...\n\n\n51211\n3196\nIMDB_Votes\nFalse\n\n\n51212\n3197\nIMDB_Votes\nTrue\n\n\n51213\n3198\nIMDB_Votes\nFalse\n\n\n51214\n3199\nIMDB_Votes\nFalse\n\n\n51215\n3200\nIMDB_Votes\nFalse\n\n\n\n\n51216 rows × 3 columns\n\n\n\n\n\n\nThis heatmap shows where missing values occur across rows and columns. Patterns may indicate:\n\nColumns with consistently missing values\nEntire rows with large gaps\nCorrelated missingness between variables\n\nTo avoid limitations in the number of rows rendered by Altair, we disable the max rows warning:\n\n\nCode\nalt.data_transformers.disable_max_rows()\n\n\nDataTransformerRegistry.enable('default')\n\n\nNow we can create the heatmap:\n\n\nCode\nalt.Chart(movies_nans).mark_rect().encode(\n    alt.X('index:O'),\n    alt.Y('column'),\n    alt.Color('NaN')\n).properties(\n    width=1000\n)\n\n\n\n\n\n\n\n\nThis plot can help identify columns or rows with critical data issues.\n\n\n\nIn many real-world cases, we may decide to remove columns that have too many missing values. Let’s set a threshold of 70%:\n\n\nCode\nthreshold_nan = 70 # in percent\ncols_to_drop = nan_percent[nan_percent&gt;threshold_nan].index\ncols_to_drop\n\n\nIndex(['US_DVD_Sales'], dtype='object')\n\n\nThese columns have more than 70% missing values and may not be useful for analysis.\n\n\n\n\nFinally, we drop the selected columns and inspect the updated dataset:\n\n\nCode\nmovies_cleaned = movies.drop(columns=cols_to_drop)\nmovies_cleaned\n\n\n\n\n\n\n\n\n\nTitle\nUS_Gross\nWorldwide_Gross\nProduction_Budget\nRelease_Date\nMPAA_Rating\nRunning_Time_min\nDistributor\nSource\nMajor_Genre\nCreative_Type\nDirector\nRotten_Tomatoes_Rating\nIMDB_Rating\nIMDB_Votes\n\n\n\n\n0\nThe Land Girls\n146083.0\n146083.0\n8000000.0\nJun 12 1998\nR\nNaN\nGramercy\nNone\nNone\nNone\nNone\nNaN\n6.1\n1071.0\n\n\n1\nFirst Love, Last Rites\n10876.0\n10876.0\n300000.0\nAug 07 1998\nR\nNaN\nStrand\nNone\nDrama\nNone\nNone\nNaN\n6.9\n207.0\n\n\n2\nI Married a Strange Person\n203134.0\n203134.0\n250000.0\nAug 28 1998\nNone\nNaN\nLionsgate\nNone\nComedy\nNone\nNone\nNaN\n6.8\n865.0\n\n\n3\nLet's Talk About Sex\n373615.0\n373615.0\n300000.0\nSep 11 1998\nNone\nNaN\nFine Line\nNone\nComedy\nNone\nNone\n13.0\nNaN\nNaN\n\n\n4\nSlam\n1009819.0\n1087521.0\n1000000.0\nOct 09 1998\nR\nNaN\nTrimark\nOriginal Screenplay\nDrama\nContemporary Fiction\nNone\n62.0\n3.4\n165.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3196\nZack and Miri Make a Porno\n31452765.0\n36851125.0\n24000000.0\nOct 31 2008\nR\n101.0\nWeinstein Co.\nOriginal Screenplay\nComedy\nContemporary Fiction\nKevin Smith\n65.0\n7.0\n55687.0\n\n\n3197\nZodiac\n33080084.0\n83080084.0\n85000000.0\nMar 02 2007\nR\n157.0\nParamount Pictures\nBased on Book/Short Story\nThriller/Suspense\nDramatization\nDavid Fincher\n89.0\nNaN\nNaN\n\n\n3198\nZoom\n11989328.0\n12506188.0\n35000000.0\nAug 11 2006\nPG\nNaN\nSony Pictures\nBased on Comic/Graphic Novel\nAdventure\nSuper Hero\nPeter Hewitt\n3.0\n3.4\n7424.0\n\n\n3199\nThe Legend of Zorro\n45575336.0\n141475336.0\n80000000.0\nOct 28 2005\nPG\n129.0\nSony Pictures\nRemake\nAdventure\nHistorical Fiction\nMartin Campbell\n26.0\n5.7\n21161.0\n\n\n3200\nThe Mask of Zorro\n93828745.0\n233700000.0\n65000000.0\nJul 17 1998\nPG-13\n136.0\nSony Pictures\nRemake\nAdventure\nHistorical Fiction\nMartin Campbell\n82.0\n6.7\n4789.0\n\n\n\n\n3201 rows × 15 columns"
  },
  {
    "objectID": "eda.html#dataset",
    "href": "eda.html#dataset",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "We will use the movies dataset from vega-datasets, which includes information about thousands of films such as their ratings, genres, duration, and box office revenue.\nLet’s load and preview the dataset:\n\n\nCode\nimport pandas as pd\nimport altair as alt\nfrom vega_datasets import data\n\n# Load dataset\nmovies = data.movies()\n\n# Show first rows\nmovies.head()\n\n\n\n\n\n\n\n\n\nTitle\nUS_Gross\nWorldwide_Gross\nUS_DVD_Sales\nProduction_Budget\nRelease_Date\nMPAA_Rating\nRunning_Time_min\nDistributor\nSource\nMajor_Genre\nCreative_Type\nDirector\nRotten_Tomatoes_Rating\nIMDB_Rating\nIMDB_Votes\n\n\n\n\n0\nThe Land Girls\n146083.0\n146083.0\nNaN\n8000000.0\nJun 12 1998\nR\nNaN\nGramercy\nNone\nNone\nNone\nNone\nNaN\n6.1\n1071.0\n\n\n1\nFirst Love, Last Rites\n10876.0\n10876.0\nNaN\n300000.0\nAug 07 1998\nR\nNaN\nStrand\nNone\nDrama\nNone\nNone\nNaN\n6.9\n207.0\n\n\n2\nI Married a Strange Person\n203134.0\n203134.0\nNaN\n250000.0\nAug 28 1998\nNone\nNaN\nLionsgate\nNone\nComedy\nNone\nNone\nNaN\n6.8\n865.0\n\n\n3\nLet's Talk About Sex\n373615.0\n373615.0\nNaN\n300000.0\nSep 11 1998\nNone\nNaN\nFine Line\nNone\nComedy\nNone\nNone\n13.0\nNaN\nNaN\n\n\n4\nSlam\n1009819.0\n1087521.0\nNaN\n1000000.0\nOct 09 1998\nR\nNaN\nTrimark\nOriginal Screenplay\nDrama\nContemporary Fiction\nNone\n62.0\n3.4\n165.0\n\n\n\n\n\n\n\nNow, let’s examine the shape (number of rows and columns) of the dataset:\n\n\nCode\nmovies.shape\n\n\n(3201, 16)\n\n\nThis tells us how many entries (rows) and features (columns) are present in the dataset."
  },
  {
    "objectID": "eda.html#first-steps",
    "href": "eda.html#first-steps",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "Before diving deeper into the data, it’s useful to explore some key metadata:\n\n✅ The column names and their data types\n⚠️ The presence of missing values\n📊 Summary statistics for numeric columns\n\n\n\nUnderstanding the structure of the dataset helps us know what type of data we’re dealing with.\n\n\nCode\nmovies.dtypes\n\n\nTitle                      object\nUS_Gross                  float64\nWorldwide_Gross           float64\nUS_DVD_Sales              float64\nProduction_Budget         float64\nRelease_Date               object\nMPAA_Rating                object\nRunning_Time_min          float64\nDistributor                object\nSource                     object\nMajor_Genre                object\nCreative_Type              object\nDirector                   object\nRotten_Tomatoes_Rating    float64\nIMDB_Rating               float64\nIMDB_Votes                float64\ndtype: object\n\n\nWe can also use .info() for a more complete summary, including non-null counts:\n\n\nCode\n# Overview of the dataset\nmovies.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3201 entries, 0 to 3200\nData columns (total 16 columns):\n #   Column                  Non-Null Count  Dtype  \n---  ------                  --------------  -----  \n 0   Title                   3200 non-null   object \n 1   US_Gross                3194 non-null   float64\n 2   Worldwide_Gross         3194 non-null   float64\n 3   US_DVD_Sales            564 non-null    float64\n 4   Production_Budget       3200 non-null   float64\n 5   Release_Date            3201 non-null   object \n 6   MPAA_Rating             2596 non-null   object \n 7   Running_Time_min        1209 non-null   float64\n 8   Distributor             2969 non-null   object \n 9   Source                  2836 non-null   object \n 10  Major_Genre             2926 non-null   object \n 11  Creative_Type           2755 non-null   object \n 12  Director                1870 non-null   object \n 13  Rotten_Tomatoes_Rating  2321 non-null   float64\n 14  IMDB_Rating             2988 non-null   float64\n 15  IMDB_Votes              2988 non-null   float64\ndtypes: float64(8), object(8)\nmemory usage: 400.3+ KB"
  },
  {
    "objectID": "eda.html#missing-values",
    "href": "eda.html#missing-values",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "Detecting and handling missing values is a critical step in any EDA process. Missing data can bias analysis or break downstream models if not handled properly.\n\nDetect patterns in missingness\nIdentify if some columns are almost entirely null\nDecide whether to drop or impute certain variables\n\n\n\nLet’s start by computing the percentage of missing values in each column:\n\n\nCode\nnan_percent = movies.isna().mean() * 100\nnan_percent_sorted = nan_percent.sort_values(ascending=False).round(2)\nnan_percent_sorted\n\n\nUS_DVD_Sales              82.38\nRunning_Time_min          62.23\nDirector                  41.58\nRotten_Tomatoes_Rating    27.49\nMPAA_Rating               18.90\nCreative_Type             13.93\nSource                    11.40\nMajor_Genre                8.59\nDistributor                7.25\nIMDB_Rating                6.65\nIMDB_Votes                 6.65\nUS_Gross                   0.22\nWorldwide_Gross            0.22\nTitle                      0.03\nProduction_Budget          0.03\nRelease_Date               0.00\ndtype: float64\n\n\n\n\n\nTo visualize missing values with Altair, we need to reshape the data into a long format where each missing value is a row:\n\n\nCode\nmovies_nans = movies.isna().reset_index().melt(\n    id_vars='index',\n    var_name='column',\n    value_name=\"NaN\"\n)\nmovies_nans\n\n\n\n\n\n\n\n\n\nindex\ncolumn\nNaN\n\n\n\n\n0\n0\nTitle\nFalse\n\n\n1\n1\nTitle\nFalse\n\n\n2\n2\nTitle\nFalse\n\n\n3\n3\nTitle\nFalse\n\n\n4\n4\nTitle\nFalse\n\n\n...\n...\n...\n...\n\n\n51211\n3196\nIMDB_Votes\nFalse\n\n\n51212\n3197\nIMDB_Votes\nTrue\n\n\n51213\n3198\nIMDB_Votes\nFalse\n\n\n51214\n3199\nIMDB_Votes\nFalse\n\n\n51215\n3200\nIMDB_Votes\nFalse\n\n\n\n\n51216 rows × 3 columns\n\n\n\n\n\n\nThis heatmap shows where missing values occur across rows and columns. Patterns may indicate:\n\nColumns with consistently missing values\nEntire rows with large gaps\nCorrelated missingness between variables\n\nTo avoid limitations in the number of rows rendered by Altair, we disable the max rows warning:\n\n\nCode\nalt.data_transformers.disable_max_rows()\n\n\nDataTransformerRegistry.enable('default')\n\n\nNow we can create the heatmap:\n\n\nCode\nalt.Chart(movies_nans).mark_rect().encode(\n    alt.X('index:O'),\n    alt.Y('column'),\n    alt.Color('NaN')\n).properties(\n    width=1000\n)\n\n\n\n\n\n\n\n\nThis plot can help identify columns or rows with critical data issues.\n\n\n\nIn many real-world cases, we may decide to remove columns that have too many missing values. Let’s set a threshold of 70%:\n\n\nCode\nthreshold_nan = 70 # in percent\ncols_to_drop = nan_percent[nan_percent&gt;threshold_nan].index\ncols_to_drop\n\n\nIndex(['US_DVD_Sales'], dtype='object')\n\n\nThese columns have more than 70% missing values and may not be useful for analysis."
  },
  {
    "objectID": "eda.html#cleaned-dataset",
    "href": "eda.html#cleaned-dataset",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "Finally, we drop the selected columns and inspect the updated dataset:\n\n\nCode\nmovies_cleaned = movies.drop(columns=cols_to_drop)\nmovies_cleaned\n\n\n\n\n\n\n\n\n\nTitle\nUS_Gross\nWorldwide_Gross\nProduction_Budget\nRelease_Date\nMPAA_Rating\nRunning_Time_min\nDistributor\nSource\nMajor_Genre\nCreative_Type\nDirector\nRotten_Tomatoes_Rating\nIMDB_Rating\nIMDB_Votes\n\n\n\n\n0\nThe Land Girls\n146083.0\n146083.0\n8000000.0\nJun 12 1998\nR\nNaN\nGramercy\nNone\nNone\nNone\nNone\nNaN\n6.1\n1071.0\n\n\n1\nFirst Love, Last Rites\n10876.0\n10876.0\n300000.0\nAug 07 1998\nR\nNaN\nStrand\nNone\nDrama\nNone\nNone\nNaN\n6.9\n207.0\n\n\n2\nI Married a Strange Person\n203134.0\n203134.0\n250000.0\nAug 28 1998\nNone\nNaN\nLionsgate\nNone\nComedy\nNone\nNone\nNaN\n6.8\n865.0\n\n\n3\nLet's Talk About Sex\n373615.0\n373615.0\n300000.0\nSep 11 1998\nNone\nNaN\nFine Line\nNone\nComedy\nNone\nNone\n13.0\nNaN\nNaN\n\n\n4\nSlam\n1009819.0\n1087521.0\n1000000.0\nOct 09 1998\nR\nNaN\nTrimark\nOriginal Screenplay\nDrama\nContemporary Fiction\nNone\n62.0\n3.4\n165.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3196\nZack and Miri Make a Porno\n31452765.0\n36851125.0\n24000000.0\nOct 31 2008\nR\n101.0\nWeinstein Co.\nOriginal Screenplay\nComedy\nContemporary Fiction\nKevin Smith\n65.0\n7.0\n55687.0\n\n\n3197\nZodiac\n33080084.0\n83080084.0\n85000000.0\nMar 02 2007\nR\n157.0\nParamount Pictures\nBased on Book/Short Story\nThriller/Suspense\nDramatization\nDavid Fincher\n89.0\nNaN\nNaN\n\n\n3198\nZoom\n11989328.0\n12506188.0\n35000000.0\nAug 11 2006\nPG\nNaN\nSony Pictures\nBased on Comic/Graphic Novel\nAdventure\nSuper Hero\nPeter Hewitt\n3.0\n3.4\n7424.0\n\n\n3199\nThe Legend of Zorro\n45575336.0\n141475336.0\n80000000.0\nOct 28 2005\nPG\n129.0\nSony Pictures\nRemake\nAdventure\nHistorical Fiction\nMartin Campbell\n26.0\n5.7\n21161.0\n\n\n3200\nThe Mask of Zorro\n93828745.0\n233700000.0\n65000000.0\nJul 17 1998\nPG-13\n136.0\nSony Pictures\nRemake\nAdventure\nHistorical Fiction\nMartin Campbell\n82.0\n6.7\n4789.0\n\n\n\n\n3201 rows × 15 columns"
  },
  {
    "objectID": "eda.html#univariate-analysis-quantitative",
    "href": "eda.html#univariate-analysis-quantitative",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "2.1 Univariate Analysis: Quantitative",
    "text": "2.1 Univariate Analysis: Quantitative\nA univariate analysis focuses on examining a single numeric variable to understand its distribution, shape, central tendency, and spread. One of the most common tools for this is the histogram.\nIn this case, we’ll explore the distribution of the movie runtime (Running_Time_min).\n\n2.1.1 Basic Histogram\nWe start by creating a histogram to visualize the distribution of running times:\n\n\nCode\nalt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('Running_Time_min',bin=alt.Bin(maxbins=30)),\n    alt.Y('count()')\n).properties(\n    title='Histogram of Movie Runtimes (30 bins)'\n)\n\n\n\n\n\n\n\n\nThis chart shows how many movies fall into each time interval (bin). However, histograms can look quite different depending on the number and size of bins used.\n\n\n2.1.2 Effect of Bin Size\nLet’s compare how the histogram shape changes with different bin sizes:\n\n\nCode\nhistogram_1 = alt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('Running_Time_min',bin=alt.Bin(maxbins=8)),\n    alt.Y('count()')\n)\n\nhistogram_2 = alt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('Running_Time_min',bin=alt.Bin(maxbins=10)),\n    alt.Y('count()')\n)\n\nhistogram_1 | histogram_2\n\n\n\n\n\n\n\n\nEven though both plots use the same data, the choice of bin size changes the visual interpretation. A small number of bins may hide details, while too many bins can make it harder to spot trends.\n\n\n2.1.3 Density plots, or Kernel Density Estimate (KDE)\nDensity plots offer a smoothed alternative to histograms. Instead of using rectangular bins to count data points, they estimate the probability density function by placing bell-shaped curves (kernels) at each observation and summing them.\nThis approach helps reduce the visual noise and jaggedness that can occur in histograms and gives a clearer picture of the underlying distribution.\n\n\nCode\nalt.Chart(movies_cleaned).transform_density(\n    'Running_Time_min',\n    as_=['Running_Time_min','density'],\n).mark_area().encode(\n    alt.X('Running_Time_min'),\n    alt.Y('density:Q')\n).properties(\n    title=\"Movies runtime\"\n)\n\n\n\n\n\n\n\n\n\n\n2.1.4 Grouped Density plot\nWe can also compare distributions across groups by splitting the KDE by a categorical variable using the groupby parameter. This helps us see how the distribution differs between categories, such as genres.\n\n\nCode\nselection = alt.selection_point(fields=['Major_Genre'], bind='legend')\n\nalt.Chart(movies_cleaned).transform_density(\n    'Running_Time_min',\n    groupby=['Major_Genre'],\n    as_=['Running_Time_min', 'density'],\n).mark_area(opacity=0.5).encode(\n    alt.X('Running_Time_min'),\n    alt.Y('density:Q', stack=None),\n    alt.Color('Major_Genre'),\n    opacity=alt.condition(selection, \n        alt.value(1), \n        alt.value(0.05)\n    )\n).add_params(\n    selection\n).properties(\n    title=\"Movies Runtime by Genre (Interactive Filter)\"\n).interactive()\n\n\n\n\n\n\n\n\nThe transparency (opacity=0.5) allows us to observe overlapping distributions and ensures that small density areas are not completely hidden behind larger ones.\nFrom this plot, we can observe, for example, that Drama movies have runtimes nearly as long as the longest Adventure movies, even though their overall distributions differ."
  },
  {
    "objectID": "eda.html#bivariate-analysis-categorical-vs-quantitative",
    "href": "eda.html#bivariate-analysis-categorical-vs-quantitative",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "2.2 Bivariate Analysis: Categorical vs Quantitative",
    "text": "2.2 Bivariate Analysis: Categorical vs Quantitative\nBivariate analysis examines the relationship between two variables. In this case, we focus on one categorical variable (e.g., genre) and one quantitative variable (e.g., revenue), which is a very common scenario in exploratory data analysis.\nThis type of analysis is useful to: - Compare average or median values across categories. - Detect outliers or high-variance groups. - Understand distributional differences across categories.\nBelow are several effective visualizations for this analysis.\n\n2.2.1 Basic Barchart\nBar charts are effective for comparing aggregated values (like the mean) across different groups. However, they hide the distribution and variation within each group.\n\n\nCode\nalt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('mean(Worldwide_Gross)'),\n    alt.Y(\"Major_Genre\")\n).properties(\n    title=\"Average Worldwide Gross by Genre\"\n)\n\n\n\n\n\n\n\n\nThis bar chart shows the mean Worldwide Gross per genre. It is useful for identifying which genres are more profitable on average, but does not show how spread out the data is.\n\n\n2.2.2 Tick Plot\nTo visualize individual data points, we use a tick plot. This helps uncover variability within genres and detect outliers.\n\n\nCode\nalt.Chart(movies_cleaned).mark_tick().encode(\n    alt.X('Worldwide_Gross'),\n    alt.Y(\"Major_Genre\"),\n    alt.Tooltip('Title:N')\n).properties(\n    title=\"Individual Gross per Movie by Genre\"\n)\n\n\n\n\n\n\n\n\n\n\n2.2.3 Heatmaps\nHeatmaps can summarize the frequency of data points across both axes (quantitative and categorical) using color intensity. It’s particularly useful for spotting patterns without getting overwhelmed by individual points.\n\n\nCode\nalt.Chart(movies_cleaned).mark_rect().encode(\n    alt.X('Worldwide_Gross',bin=alt.Bin(maxbins=100)),\n    alt.Y(\"Major_Genre\"),\n    alt.Color('count()'),\n    alt.Tooltip('count()')\n).properties(\n    title=\"Heatmap of Movie Counts by Gross and Genre\"\n)\n\n\n\n\n\n\n\n\nThis heatmap shows how frequently movies from each genre fall into different revenue ranges.\n\n\n2.2.4 Boxplot\nBoxplots are useful for comparing distributions across categories and identifying outliers. Boxplots summarize a distribution using five statistics:\n\nMedian (Q2)\nFirst Quartile (Q1)\nThird Quartile (Q3)\nLower Whisker (Q1 - 1.5 × IQR)\nUpper Whisker (Q3 + 1.5 × IQR)\n\n\n\nCode\nalt.Chart(movies_cleaned).mark_boxplot().encode(\n    alt.X('Worldwide_Gross'),\n    alt.Y(\"Major_Genre\")\n).properties(\n    title=\"Boxplot of Worldwide Gross by Genre\"\n)\n\n\n\n\n\n\n\n\n\n\n2.2.5 Side-by-side: Boxplot and Bar Chart\nTo contrast aggregated values (bar chart) with the full distribution (boxplot), we can display them together:\n\n\nCode\nbar = alt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('mean(Worldwide_Gross)'),\n    alt.Y(\"Major_Genre\")\n)\n\nbox = alt.Chart(movies_cleaned).mark_boxplot().encode(\n    alt.X('mean(Worldwide_Gross)'),\n    alt.Y(\"Major_Genre\")\n)\n\nbox | bar\n\n\n\n\n\n\n\n\nThis comparison reveals whether the mean is a good representative of the genre, or whether the data is skewed or contains outliers that affect the average"
  },
  {
    "objectID": "eda.html#bivariate-analysis-quantitative-vs-quantitative",
    "href": "eda.html#bivariate-analysis-quantitative-vs-quantitative",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "2.3 Bivariate Analysis: Quantitative vs Quantitative",
    "text": "2.3 Bivariate Analysis: Quantitative vs Quantitative\nWhen analyzing two quantitative (numerical) variables simultaneously, we aim to discover possible relationships, trends, or correlations. This type of bivariate analysis can reveal whether increases in one variable are associated with increases or decreases in another (positive or negative correlation), or if there’s no relationship at all. The most common and intuitive visualization for this is the scatterplot.\n\n2.3.1 Scatterplots\nScatter plots are effective visualizations for exploring two-dimensional distributions, allowing us to identify patterns, trends, clusters, or outliers.\nLet’s start by visualizing how movies are rated across two popular online platforms:\n\nIMDb\n\nRotten Tomatoes\n\nAre movies rated similarly on different platforms?\n\n\nCode\nalt.Chart(movies_cleaned).mark_point().encode(\n    alt.X('IMDB_Rating'),\n    alt.Y('Rotten_Tomatoes_Rating')\n).properties(\n    title=\"IMDB vs Rotten Tomatoes Ratings\"\n)\n\n\n\n\n\n\n\n\n\n\n2.3.2 Scatterplot Saturation\nScatterplots can become saturated when too many points overlap in a small area of the chart, making it difficult to distinguish dense regions from sparse ones. For example, when plotting financial variables like production budget versus worldwide gross:\n\n\nCode\nsaturated = alt.Chart(movies_cleaned).mark_point().encode(\n    alt.X('Production_Budget'),\n    alt.Y('Worldwide_Gross')\n).properties(\n    title=\"Saturated Scatterplot: Budget vs Gross\"\n)\nsaturated\n\n\n\n\n\n\n\n\n\n\n2.3.3 Using Binned Heatmap to Reduce Saturation\nTo address saturation, we can bin both variables and use a heatmap where the color intensity represents the number of movies that fall into each rectangular region of the grid. This makes dense areas more interpretable\n\n\nCode\nheatmap_scatter = alt.Chart(movies_cleaned).mark_rect().encode(\n    alt.X('Production_Budget', bin=alt.Bin(maxbins=60)),\n    alt.Y('Worldwide_Gross', bin=alt.Bin(maxbins=60)),\n    alt.Color('count()'),\n    alt.Tooltip('count()')\n).properties(\n    title=\"Binned Heatmap: Budget vs Gross\"\n)\nheatmap_scatter\n\n\n\n\n\n\n\n\n\n\n2.3.4 Side-by-side Comparison\nCompare the raw scatterplot with the heatmap representation:\n\n\nCode\nsaturated | heatmap_scatter"
  },
  {
    "objectID": "eda.html#bivariate-analysis-categorical-vs-categorical",
    "href": "eda.html#bivariate-analysis-categorical-vs-categorical",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "2.4 Bivariate Analysis: Categorical vs Categorical",
    "text": "2.4 Bivariate Analysis: Categorical vs Categorical\nWhen working with two categorical variables, bivariate analysis helps us understand how categories from one variable relate or are distributed across the other. For example, we might want to know how different movie genres are rated according to the MPAA rating system. Visualization techniques like grouped bar charts and faceted plots can reveal patterns, associations, or class imbalances.\n\n2.4.1 Basic Faceted Bar Chart\nWe begin by exploring how movies are rated (MPAA_Rating) across different genres (Major_Genre). A faceted bar chart allows us to visualize this relationship by plotting a bar chart per genre, helping to identify genre-specific rating distributions.\n\n\nCode\nalt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('count()'),\n    alt.Y('MPAA_Rating'),\n    alt.Color('MPAA_Rating')\n).facet(\n    'Major_Genre'\n)\n\n\n\n\n\n\n\n\n\n\n2.4.2 Vertical Faceting for Alignment\nFaceting horizontally can make comparisons across genres harder when the x-axis is misaligned. By specifying columns=1, we lay out the facets vertically, making it easier to compare counts across genres.\n\n\nCode\nalt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('count()'),\n    alt.Y('MPAA_Rating'),\n    alt.Color('MPAA_Rating')\n).facet(\n    'Major_Genre',\n    columns=1\n)\n\n\n\n\n\n\n\n\n\n\n2.4.3 Dependent vs Independent Axis Scaling\nBy default, facet plots share the same x-axis scale (dependent scale), which allows for easier comparison across panels. However, when the number of observations varies greatly between genres, this shared scale can compress some charts.\nWe can instead use independent x-axis scaling for each facet. This highlights the relative distribution within each genre.\n\n\nCode\nshared_scale = alt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('count()'),\n    alt.Y('MPAA_Rating'),\n    alt.Color('MPAA_Rating')\n).facet(\n    'Major_Genre',\n    columns=4\n)\n\nindependent_scale = alt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('count()'),\n    alt.Y('MPAA_Rating'),\n    alt.Color('MPAA_Rating')\n).facet(\n    'Major_Genre',\n    columns=4\n).resolve_scale(x='independent')\n\nshared_scale | independent_scale\n\n\n\n\n\n\n\n\nThe left panel (shared scale) makes absolute comparisons between genres, while the right panel (independent scale) makes within-genre comparisons more readable.\n\n\n2.4.4 Heatmaps\nHeatmaps are effective for visualizing the relationship between two categorical variables when the goal is to display counts or frequency of occurrences. They map the number of observations to color, providing an intuitive view of which category pairs are most or least common.\nWe can enhance this basic representation by also using marker size, combining both color intensity and circle area to represent counts more effectively. This dual encoding can improve interpretation, especially when printed in grayscale or when there are subtle color differences.\n\n\nCode\nheatmap_color = alt.Chart(movies_cleaned).mark_rect().encode(\n    alt.X('MPAA_Rating'),\n    alt.Y('Major_Genre', sort='color'),\n    alt.Color('count()')\n).properties(\n    title=\"Heatmap with Color (Count of Movies)\"\n)\n\nheatmap_size = alt.Chart(movies_cleaned).mark_circle().encode(\n    alt.X('MPAA_Rating'),\n    alt.Y('Major_Genre', sort='color'),\n    alt.Color('count()'),\n    alt.Size('count()')\n).properties(\n    title=\"Heatmap with Color + Size (Count of Movies)\"\n)\n\nheatmap_color | heatmap_size"
  },
  {
    "objectID": "eda.html#multivariate-analysis",
    "href": "eda.html#multivariate-analysis",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "2.5 Multivariate Analysis",
    "text": "2.5 Multivariate Analysis\nMultivariate analysis helps us understand the interactions and relationships among multiple variables simultaneously. In the context of numerical features, it is useful to explore pairwise distributions, correlations, and detect potential clusters or anomalies.\nWhen the number of variables is large, repeated charts such as histograms or scatter plot matrices help us summarize patterns efficiently and consistently across all numerical dimensions.\n\n2.5.1 Repeated Histograms for Numerical Columns\nWe first identify and isolate all numerical columns from the dataset. Then we repeat a histogram for each of these columns to understand the individual distributions. This overview is helpful to detect skewness, outliers, or binning decisions that affect how data is grouped visually.\n\n\nCode\n# Select only numerical columns\nnumerical_columns = movies_cleaned.select_dtypes('number').columns.tolist()\n\n\n\n\nCode\nalt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X(alt.repeat(),type='quantitative',bin=alt.Bin(maxbins=30)),\n    alt.Y('count()')\n).properties(\n    width=150,\n    height=150\n).repeat(\n    numerical_columns,\n    columns=4\n)\n\n\n\n\n\n\n\n\n\n\n2.5.2 Scatter Plot Matrix (Pairplot)\nA scatter plot matrix shows the pairwise relationships between all numerical variables. This is a common exploratory tool to detect:\n\nCorrelations between variables\nOutliers or clusters\nRelationships useful for prediction models (e.g., to predict rating or budget)\n\nWe focus especially on the plots below the diagonal, as they are not duplicated.\n\n\nCode\nalt.Chart(movies_cleaned).mark_point().encode(\n    alt.X(alt.repeat('column'),type='quantitative'),\n    alt.Y(alt.repeat('row'),type='quantitative'),\n    alt.Tooltip('Title:N')\n).properties(\n    width=100,\n    height=100\n).repeat(\n    column=numerical_columns,\n    row=numerical_columns\n)\n\n\n\n\n\n\n\n\n\n\n2.5.3 Heatmap Matrix\nWhen scatter plots become too saturated (many overlapping points), heatmaps offer a better alternative by binning the numeric values and encoding the count in color intensity.\n\n\nCode\nalt.Chart(movies_cleaned).mark_rect().encode(\n    alt.X(alt.repeat('column'),type='quantitative',bin=alt.Bin(maxbins=30)),\n    alt.Y(alt.repeat('row'),type='quantitative',bin=alt.Bin(maxbins=30)),\n    alt.Color('count()'),\n    alt.Tooltip('count()')\n).properties(\n    width=100,\n    height=100\n).repeat(\n    column=numerical_columns,\n    row=numerical_columns\n).resolve_scale(\n    color='independent'\n)\n\n\n\n\n\n\n\n\nTo gain deeper insights into the dataset, it’s important to analyze how numerical variables behave across different categories. This type of multivariate analysis allows us to:\n\nCompare distributions across categories\nDetect outliers within categories\nObserve central tendency (median, quartiles) and spread (range, IQR)\n\nBoxplots are particularly effective for this purpose. In the following visualizations, we explore these relationships by repeating plots across combinations of categorical and numerical features.\n\n\n2.5.4 Filter Categorical Columns\nFirst, we select the relevant categorical columns, excluding identifiers and text-heavy variables like movie titles or director names.\n\n\nCode\ncategorical_columns =  movies_cleaned.select_dtypes('object').columns.to_list()\n\ncategorical_columns_remove = ['Title','Release_Date','Distributor','Director']\n\ncategorical_filtered = [col for col in categorical_columns if col not in categorical_columns_remove]\n\n\n\n\n2.5.5 Repeated Boxplots: Categorical vs Numerical\nWe repeat boxplots using combinations of categorical (rows) and numerical (columns) features. This matrix layout gives a clear visual overview of how numerical values are distributed within each category.\n\n\nCode\nalt.Chart(movies_cleaned).mark_boxplot().encode(\n    alt.X(alt.repeat('column'),type='quantitative'),\n    alt.Y(alt.repeat('row'),type='nominal'),\n    alt.Size('count()')\n).properties(\n    width=200,\n    height=200\n).repeat(\n    column=numerical_columns,\n    row=categorical_filtered\n)\n\n\n\n\n\n\n\n\n\n\n2.5.6 Faceted Boxplots\nFor more focused analysis, we can facet the boxplots using a specific categorical variable like MPAA_Rating, and repeat the chart by different categorical rows. This lets us keep the numerical axis fixed while comparing how categories vary across different classes (e.g., movie ratings).\n\n\nCode\nalt.Chart(movies_cleaned).mark_boxplot().encode(\n    alt.X('Running_Time_min', type='quantitative'),\n    alt.Y(alt.repeat('row'),type='nominal'),\n    alt.Size('count()'),\n    alt.Tooltip('Title:N')\n).properties(\n    width=100,\n    height=100\n).facet(\n    column='MPAA_Rating'\n).repeat(\n    row=categorical_filtered\n)"
  },
  {
    "objectID": "Task/Quarto 2/Index.html",
    "href": "Task/Quarto 2/Index.html",
    "title": "REPORTE QUARTO",
    "section": "",
    "text": "Code\nprint(\"Hola Mundo\")\n\nfor i in range(10):\n    print(f\"Elemento del for i: {i}\")\n\n\nHola Mundo\nElemento del for i: 0\nElemento del for i: 1\nElemento del for i: 2\nElemento del for i: 3\nElemento del for i: 4\nElemento del for i: 5\nElemento del for i: 6\nElemento del for i: 7\nElemento del for i: 8\nElemento del for i: 9\n\n\n\n\nCode\nimport altair as alt\n\nfrom vega_datasets import data\ncars = data.cars()\nprint(cars.head())\n\n\n                        Name  Miles_per_Gallon  Cylinders  Displacement  \\\n0  chevrolet chevelle malibu              18.0          8         307.0   \n1          buick skylark 320              15.0          8         350.0   \n2         plymouth satellite              18.0          8         318.0   \n3              amc rebel sst              16.0          8         304.0   \n4                ford torino              17.0          8         302.0   \n\n   Horsepower  Weight_in_lbs  Acceleration       Year Origin  \n0       130.0           3504          12.0 1970-01-01    USA  \n1       165.0           3693          11.5 1970-01-01    USA  \n2       150.0           3436          11.0 1970-01-01    USA  \n3       150.0           3433          12.0 1970-01-01    USA  \n4       140.0           3449          10.5 1970-01-01    USA  \n\n\n\n\nCode\nalt.Chart(cars).mark_point().encode(\n    x=\"Miles_per_Gallon\"\n)\n\n\n\n\n\n\n\n\n\n\nCode\nalt.Chart(cars).mark_point().encode(\n    x=\"Miles_per_Gallon\",\n    y=\"Horsepower\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Horsepower\"]\n).interactive()\n\n\n\n\n\n\n\n\n\n\nCode\nalt.Chart(cars).mark_point().encode(\n    x=\"Miles_per_Gallon\",\n    y=\"Horsepower\",\n    color=\"Origin\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Horsepower\"]\n).interactive()\n\n\n\n\n\n\n\n\n\n\nCode\nalt.Chart(cars).mark_point().encode(\n    x=\"Weight_in_lbs\",\n    y=\"Miles_per_Gallon\",\n    color=\"Origin\",\n    shape=\"Origin\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Weight_in_lbs\"]\n).interactive()\n\n\n\n\n\n\n\n\n\nList\n\nPrimero"
  },
  {
    "objectID": "Task/Quarto 2/Index.html#graficas-de-prueba",
    "href": "Task/Quarto 2/Index.html#graficas-de-prueba",
    "title": "REPORTE QUARTO",
    "section": "",
    "text": "Code\nprint(\"Hola Mundo\")\n\nfor i in range(10):\n    print(f\"Elemento del for i: {i}\")\n\n\nHola Mundo\nElemento del for i: 0\nElemento del for i: 1\nElemento del for i: 2\nElemento del for i: 3\nElemento del for i: 4\nElemento del for i: 5\nElemento del for i: 6\nElemento del for i: 7\nElemento del for i: 8\nElemento del for i: 9\n\n\n\n\nCode\nimport altair as alt\n\nfrom vega_datasets import data\ncars = data.cars()\nprint(cars.head())\n\n\n                        Name  Miles_per_Gallon  Cylinders  Displacement  \\\n0  chevrolet chevelle malibu              18.0          8         307.0   \n1          buick skylark 320              15.0          8         350.0   \n2         plymouth satellite              18.0          8         318.0   \n3              amc rebel sst              16.0          8         304.0   \n4                ford torino              17.0          8         302.0   \n\n   Horsepower  Weight_in_lbs  Acceleration       Year Origin  \n0       130.0           3504          12.0 1970-01-01    USA  \n1       165.0           3693          11.5 1970-01-01    USA  \n2       150.0           3436          11.0 1970-01-01    USA  \n3       150.0           3433          12.0 1970-01-01    USA  \n4       140.0           3449          10.5 1970-01-01    USA  \n\n\n\n\nCode\nalt.Chart(cars).mark_point().encode(\n    x=\"Miles_per_Gallon\"\n)\n\n\n\n\n\n\n\n\n\n\nCode\nalt.Chart(cars).mark_point().encode(\n    x=\"Miles_per_Gallon\",\n    y=\"Horsepower\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Horsepower\"]\n).interactive()\n\n\n\n\n\n\n\n\n\n\nCode\nalt.Chart(cars).mark_point().encode(\n    x=\"Miles_per_Gallon\",\n    y=\"Horsepower\",\n    color=\"Origin\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Horsepower\"]\n).interactive()\n\n\n\n\n\n\n\n\n\n\nCode\nalt.Chart(cars).mark_point().encode(\n    x=\"Weight_in_lbs\",\n    y=\"Miles_per_Gallon\",\n    color=\"Origin\",\n    shape=\"Origin\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Weight_in_lbs\"]\n).interactive()\n\n\n\n\n\n\n\n\n\nList\n\nPrimero"
  },
  {
    "objectID": "cnj.html",
    "href": "cnj.html",
    "title": "Visualización de Data",
    "section": "",
    "text": "Nombre del dataset: Número de causas ingresadas, resueltas y en trámite a nivel nacional (ámbitos COIP, COGEP y Constitucional).\n\nObjetivo: Mostrar estadísticas judiciales de Ecuador sobre cuántas causas se han ingresado, resuelto o se encuentran en trámite, desglosadas por materia, tipo de acción, delito, provincia, cantón, judicatura e instancia.\n\n\n\n\n\n\nPeríodo: Desde 2017 hasta la fecha de corte.\n\nÚltima actualización: 17 de septiembre de 2025.\n\nPeriodicidad: Mensual (se actualiza cada mes).\n\n\n\n\n\n\nOrganismo responsable: Consejo de la Judicatura de Ecuador.\n\nDepartamento: Dirección Nacional de Estudios Jurimétricos y Estadística Judicial.\n\nLicencia: Creative Commons Attribution (CC-BY).\n\n\n\n\n\nEl dataset incluye información desagregada en los siguientes campos:\n\nAño y Mes\n\nMateria (penal, constitucional, administrativo, etc.)\n\nTipo de acción / clase de causa\n\nDelito (cuando aplica)\n\nProvincia y Cantón\n\nJudicatura e Instancia\n\nNúmero de causas:\n\nIngresadas\n\nResueltas\n\nEn trámite\n\n\n\n\n\n\n\n\nCode\nimport pandas as pd\nimport altair as alt\n\n# Ruta local del archivo .ods\nfile = \"/Users/usr-s3c/Documents/Maestria_YTech/Machine-Learning/cnj.csv\"\n# Leer la primera hoja del archivo\ndf = pd.read_csv(file, sep=\";\", low_memory=False)\n\n# Mostrar las primeras filas\nprint(df.head(20))\n\n\n                       Materia            TipoAccion  \\\n0   ADOLESCENTE INFRACTOR COIP  ACCION PENAL PUBLICA   \n1   ADOLESCENTE INFRACTOR COIP  ACCION PENAL PUBLICA   \n2   ADOLESCENTE INFRACTOR COIP  ACCION PENAL PUBLICA   \n3   ADOLESCENTE INFRACTOR COIP  ACCION PENAL PUBLICA   \n4   ADOLESCENTE INFRACTOR COIP  ACCION PENAL PUBLICA   \n5   ADOLESCENTE INFRACTOR COIP  ACCION PENAL PUBLICA   \n6   ADOLESCENTE INFRACTOR COIP  ACCION PENAL PUBLICA   \n7   ADOLESCENTE INFRACTOR COIP  ACCION PENAL PUBLICA   \n8   ADOLESCENTE INFRACTOR COIP  ACCION PENAL PUBLICA   \n9   ADOLESCENTE INFRACTOR COIP  ACCION PENAL PUBLICA   \n10  ADOLESCENTE INFRACTOR COIP  ACCION PENAL PUBLICA   \n11  ADOLESCENTE INFRACTOR COIP  ACCION PENAL PUBLICA   \n12  ADOLESCENTE INFRACTOR COIP  ACCION PENAL PUBLICA   \n13  ADOLESCENTE INFRACTOR COIP  ACCION PENAL PUBLICA   \n14  ADOLESCENTE INFRACTOR COIP  ACCION PENAL PUBLICA   \n15  ADOLESCENTE INFRACTOR COIP  ACCION PENAL PUBLICA   \n16  ADOLESCENTE INFRACTOR COIP  ACCION PENAL PUBLICA   \n17  ADOLESCENTE INFRACTOR COIP  ACCION PENAL PUBLICA   \n18  ADOLESCENTE INFRACTOR COIP  ACCION PENAL PUBLICA   \n19  ADOLESCENTE INFRACTOR COIP  ACCION PENAL PUBLICA   \n\n                           Delito   Provincia           Canton  \\\n0   ART. 084 DESAPARICION FORZADA       AZUAY           CUENCA   \n1   ART. 084 DESAPARICION FORZADA       AZUAY           CUENCA   \n2   ART. 084 DESAPARICION FORZADA   SUCUMBIOS  GONZALO PIZARRO   \n3               ART. 088 AGRESION       AZUAY           CUENCA   \n4               ART. 088 AGRESION       AZUAY           CUENCA   \n5               ART. 088 AGRESION       AZUAY           CUENCA   \n6               ART. 088 AGRESION       AZUAY           CUENCA   \n7               ART. 088 AGRESION       AZUAY           CUENCA   \n8               ART. 088 AGRESION       AZUAY           CUENCA   \n9               ART. 088 AGRESION       AZUAY           CUENCA   \n10              ART. 088 AGRESION       AZUAY           CUENCA   \n11              ART. 088 AGRESION       AZUAY         GUALACEO   \n12              ART. 088 AGRESION       AZUAY         GUALACEO   \n13              ART. 088 AGRESION      CARCHI           TULCAN   \n14              ART. 088 AGRESION  CHIMBORAZO         RIOBAMBA   \n15              ART. 088 AGRESION  CHIMBORAZO         RIOBAMBA   \n16              ART. 088 AGRESION  CHIMBORAZO         RIOBAMBA   \n17              ART. 088 AGRESION  CHIMBORAZO         RIOBAMBA   \n18              ART. 088 AGRESION  CHIMBORAZO         RIOBAMBA   \n19              ART. 088 AGRESION    COTOPAXI        LATACUNGA   \n\n                               Judicatura            Instancia Periodo  \\\n0                       UJ FMNA DE CUENCA  UNIDADES JUDICIALES  1/1/17   \n1                       UJ FMNA DE CUENCA  UNIDADES JUDICIALES  1/1/18   \n2   UJ MULTICOMPETENTE DE GONZALO PIZARRO  UNIDADES JUDICIALES  1/1/17   \n3                       UJ FMNA DE CUENCA  UNIDADES JUDICIALES  1/1/16   \n4                       UJ FMNA DE CUENCA  UNIDADES JUDICIALES  1/1/17   \n5                       UJ FMNA DE CUENCA  UNIDADES JUDICIALES  1/1/18   \n6                       UJ FMNA DE CUENCA  UNIDADES JUDICIALES  1/1/19   \n7                       UJ FMNA DE CUENCA  UNIDADES JUDICIALES  1/1/20   \n8                       UJ FMNA DE CUENCA  UNIDADES JUDICIALES  1/1/21   \n9                       UJ FMNA DE CUENCA  UNIDADES JUDICIALES  1/1/23   \n10                      UJ FMNA DE CUENCA  UNIDADES JUDICIALES  1/1/24   \n11                    UJ FMNA DE GUALACEO  UNIDADES JUDICIALES  1/1/17   \n12                    UJ FMNA DE GUALACEO  UNIDADES JUDICIALES  1/1/18   \n13                      UJ FMNA DE TULCAN  UNIDADES JUDICIALES  1/1/16   \n14                    UJ FMNA DE RIOBAMBA  UNIDADES JUDICIALES  1/1/16   \n15                    UJ FMNA DE RIOBAMBA  UNIDADES JUDICIALES  1/1/17   \n16                    UJ FMNA DE RIOBAMBA  UNIDADES JUDICIALES  1/1/19   \n17                    UJ FMNA DE RIOBAMBA  UNIDADES JUDICIALES  1/1/24   \n18                    UJ FMNA DE RIOBAMBA  UNIDADES JUDICIALES  1/1/25   \n19                   UJ FMNA DE LATACUNGA  UNIDADES JUDICIALES  1/1/17   \n\n    Ingreso  Resuelta  Tramite  Mes  Anio  \n0         3         2        1    8  2025  \n1        57        49        9    8  2025  \n2         2         2        0    8  2025  \n3         2         1        1    8  2025  \n4         1         0        2    8  2025  \n5         1         1        2    8  2025  \n6         2         1        3    8  2025  \n7         3         3        3    8  2025  \n8         2         2        3    8  2025  \n9         2         1        4    8  2025  \n10        1         0        5    8  2025  \n11        1         1        0    8  2025  \n12        1         1        0    8  2025  \n13        1         1        0    8  2025  \n14        2         1        1    8  2025  \n15        1         1        1    8  2025  \n16        4         1        4    8  2025  \n17        2         0        6    8  2025  \n18        1         0        7    8  2025  \n19        1         1        0    8  2025  \n\n\n\n\nCode\n# Mostrar nombres de todas las columnas\nprint(df.columns.tolist())\n\n\n['Materia', 'TipoAccion', 'Delito', 'Provincia', 'Canton', 'Judicatura', 'Instancia', 'Periodo', 'Ingreso', 'Resuelta', 'Tramite', 'Mes', 'Anio']\n\n\n\n\nCode\nprint(\"Número de columnas:\", len(df.columns))\n\n\nNúmero de columnas: 13\n\n\n\n\nCode\nprint(df.info())\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 235644 entries, 0 to 235643\nData columns (total 13 columns):\n #   Column      Non-Null Count   Dtype \n---  ------      --------------   ----- \n 0   Materia     235644 non-null  object\n 1   TipoAccion  235644 non-null  object\n 2   Delito      235644 non-null  object\n 3   Provincia   235644 non-null  object\n 4   Canton      235644 non-null  object\n 5   Judicatura  235644 non-null  object\n 6   Instancia   235644 non-null  object\n 7   Periodo     235644 non-null  object\n 8   Ingreso     235644 non-null  int64 \n 9   Resuelta    235644 non-null  int64 \n 10  Tramite     235644 non-null  int64 \n 11  Mes         235644 non-null  int64 \n 12  Anio        235644 non-null  int64 \ndtypes: int64(5), object(8)\nmemory usage: 23.4+ MB\nNone\n\n\n\n\nCode\ndf.head(20)\n\n\n\n\n\n\n\n\n\nMateria\nTipoAccion\nDelito\nProvincia\nCanton\nJudicatura\nInstancia\nPeriodo\nIngreso\nResuelta\nTramite\nMes\nAnio\n\n\n\n\n0\nADOLESCENTE INFRACTOR COIP\nACCION PENAL PUBLICA\nART. 084 DESAPARICION FORZADA\nAZUAY\nCUENCA\nUJ FMNA DE CUENCA\nUNIDADES JUDICIALES\n1/1/17\n3\n2\n1\n8\n2025\n\n\n1\nADOLESCENTE INFRACTOR COIP\nACCION PENAL PUBLICA\nART. 084 DESAPARICION FORZADA\nAZUAY\nCUENCA\nUJ FMNA DE CUENCA\nUNIDADES JUDICIALES\n1/1/18\n57\n49\n9\n8\n2025\n\n\n2\nADOLESCENTE INFRACTOR COIP\nACCION PENAL PUBLICA\nART. 084 DESAPARICION FORZADA\nSUCUMBIOS\nGONZALO PIZARRO\nUJ MULTICOMPETENTE DE GONZALO PIZARRO\nUNIDADES JUDICIALES\n1/1/17\n2\n2\n0\n8\n2025\n\n\n3\nADOLESCENTE INFRACTOR COIP\nACCION PENAL PUBLICA\nART. 088 AGRESION\nAZUAY\nCUENCA\nUJ FMNA DE CUENCA\nUNIDADES JUDICIALES\n1/1/16\n2\n1\n1\n8\n2025\n\n\n4\nADOLESCENTE INFRACTOR COIP\nACCION PENAL PUBLICA\nART. 088 AGRESION\nAZUAY\nCUENCA\nUJ FMNA DE CUENCA\nUNIDADES JUDICIALES\n1/1/17\n1\n0\n2\n8\n2025\n\n\n5\nADOLESCENTE INFRACTOR COIP\nACCION PENAL PUBLICA\nART. 088 AGRESION\nAZUAY\nCUENCA\nUJ FMNA DE CUENCA\nUNIDADES JUDICIALES\n1/1/18\n1\n1\n2\n8\n2025\n\n\n6\nADOLESCENTE INFRACTOR COIP\nACCION PENAL PUBLICA\nART. 088 AGRESION\nAZUAY\nCUENCA\nUJ FMNA DE CUENCA\nUNIDADES JUDICIALES\n1/1/19\n2\n1\n3\n8\n2025\n\n\n7\nADOLESCENTE INFRACTOR COIP\nACCION PENAL PUBLICA\nART. 088 AGRESION\nAZUAY\nCUENCA\nUJ FMNA DE CUENCA\nUNIDADES JUDICIALES\n1/1/20\n3\n3\n3\n8\n2025\n\n\n8\nADOLESCENTE INFRACTOR COIP\nACCION PENAL PUBLICA\nART. 088 AGRESION\nAZUAY\nCUENCA\nUJ FMNA DE CUENCA\nUNIDADES JUDICIALES\n1/1/21\n2\n2\n3\n8\n2025\n\n\n9\nADOLESCENTE INFRACTOR COIP\nACCION PENAL PUBLICA\nART. 088 AGRESION\nAZUAY\nCUENCA\nUJ FMNA DE CUENCA\nUNIDADES JUDICIALES\n1/1/23\n2\n1\n4\n8\n2025\n\n\n10\nADOLESCENTE INFRACTOR COIP\nACCION PENAL PUBLICA\nART. 088 AGRESION\nAZUAY\nCUENCA\nUJ FMNA DE CUENCA\nUNIDADES JUDICIALES\n1/1/24\n1\n0\n5\n8\n2025\n\n\n11\nADOLESCENTE INFRACTOR COIP\nACCION PENAL PUBLICA\nART. 088 AGRESION\nAZUAY\nGUALACEO\nUJ FMNA DE GUALACEO\nUNIDADES JUDICIALES\n1/1/17\n1\n1\n0\n8\n2025\n\n\n12\nADOLESCENTE INFRACTOR COIP\nACCION PENAL PUBLICA\nART. 088 AGRESION\nAZUAY\nGUALACEO\nUJ FMNA DE GUALACEO\nUNIDADES JUDICIALES\n1/1/18\n1\n1\n0\n8\n2025\n\n\n13\nADOLESCENTE INFRACTOR COIP\nACCION PENAL PUBLICA\nART. 088 AGRESION\nCARCHI\nTULCAN\nUJ FMNA DE TULCAN\nUNIDADES JUDICIALES\n1/1/16\n1\n1\n0\n8\n2025\n\n\n14\nADOLESCENTE INFRACTOR COIP\nACCION PENAL PUBLICA\nART. 088 AGRESION\nCHIMBORAZO\nRIOBAMBA\nUJ FMNA DE RIOBAMBA\nUNIDADES JUDICIALES\n1/1/16\n2\n1\n1\n8\n2025\n\n\n15\nADOLESCENTE INFRACTOR COIP\nACCION PENAL PUBLICA\nART. 088 AGRESION\nCHIMBORAZO\nRIOBAMBA\nUJ FMNA DE RIOBAMBA\nUNIDADES JUDICIALES\n1/1/17\n1\n1\n1\n8\n2025\n\n\n16\nADOLESCENTE INFRACTOR COIP\nACCION PENAL PUBLICA\nART. 088 AGRESION\nCHIMBORAZO\nRIOBAMBA\nUJ FMNA DE RIOBAMBA\nUNIDADES JUDICIALES\n1/1/19\n4\n1\n4\n8\n2025\n\n\n17\nADOLESCENTE INFRACTOR COIP\nACCION PENAL PUBLICA\nART. 088 AGRESION\nCHIMBORAZO\nRIOBAMBA\nUJ FMNA DE RIOBAMBA\nUNIDADES JUDICIALES\n1/1/24\n2\n0\n6\n8\n2025\n\n\n18\nADOLESCENTE INFRACTOR COIP\nACCION PENAL PUBLICA\nART. 088 AGRESION\nCHIMBORAZO\nRIOBAMBA\nUJ FMNA DE RIOBAMBA\nUNIDADES JUDICIALES\n1/1/25\n1\n0\n7\n8\n2025\n\n\n19\nADOLESCENTE INFRACTOR COIP\nACCION PENAL PUBLICA\nART. 088 AGRESION\nCOTOPAXI\nLATACUNGA\nUJ FMNA DE LATACUNGA\nUNIDADES JUDICIALES\n1/1/17\n1\n1\n0\n8\n2025\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# --- Agregacion y Top 10 ---\ntop_delitos = (df.groupby('Delito', as_index=False)['Ingreso']\n                 .sum()\n                 .sort_values('Ingreso', ascending=False)\n                 .head(10))\n\n# (Opcional) ver tabla en consola\nprint(top_delitos.to_string(index=False))\n\n# --- Grafica ---\nbar = alt.Chart(top_delitos).mark_bar().encode(\n    y=alt.Y('Delito:N', sort='-x', title='Delito'),\n    x=alt.X('Ingreso:Q', title='Causas (Ingreso)'),\n    tooltip=['Delito', 'Ingreso']\n).properties(\n    title='Top 10 de causas mas comunes por Delito (segun Ingreso)',\n    width=700,\n    height=350\n).interactive()\n\nlabels = bar.mark_text(align='left', dx=3).encode(text='Ingreso:Q')\n\n(bar + labels)\n\n\n                                                                              Delito  Ingreso\n                                ART. 389 CONTRAVENCIONES DE TRANSITO DE CUARTA CLASE  1116571\n                                                          COBRO DE PAGARE A LA ORDEN   503535\nART. 159 CONTRAVENCIONES DE VIOLENCIA CONTRA LA MUJER O MIEMBROS DEL NUCLEO FAMILIAR   459978\n                                                                           ALIMENTOS   414144\n                               ART. 386 CONTRAVENCIONES DE TRANSITO DE PRIMERA CLASE   227334\n                                                                 DIVORCIO POR CAUSAL   174142\n                         BENEFICIOS Y PETICIONES DE PERSONAS PRIVADAS DE LA LIBERTAD   161231\n                                                                ACCION DE PROTECCION   144420\n                             ART. 385 CONDUCCION DE VEHICULO EN ESTADO DE EMBRIAGUEZ   128014\n          ART. 220 TRAFICO ILICITO DE SUSTANCIAS CATALOGADAS SUJETAS A FISCALIZACION   110272\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Asegurar limpieza de texto\ndf['Delito'] = df['Delito'].fillna('SIN ESPECIFICAR').astype(str).str.strip()\ndf['Provincia'] = df['Provincia'].astype(str).str.strip()\ndf['Ingreso'] = pd.to_numeric(df['Ingreso'], errors='coerce').fillna(0)\n\n# Filtrar solo el delito 140 ASESINATO\ndf_asesinato = df[df['Delito'].str.upper().str.contains(\"140 ASESINATO\")]\n\n# Agrupar por provincia y ordenar\ntop5_provincias = (df_asesinato.groupby('Provincia', as_index=False)['Ingreso']\n                   .sum()\n                   .sort_values('Ingreso', ascending=False)\n                   .head(5))\n\nprint(top5_provincias.to_string(index=False))  # tabla en consola\n\n# Gráfica\nchart = alt.Chart(top5_provincias).mark_bar().encode(\n    x=alt.X('Ingreso:Q', title='Causas (Ingreso)'),\n    y=alt.Y('Provincia:N', sort='-x', title='Provincia'),\n    tooltip=['Provincia','Ingreso']\n).properties(\n    title='Top 5 provincias con mas causas por delito: 140 ASESINATO',\n    width=600,\n    height=300\n).interactive()\n\nlabels = chart.mark_text(align='left', dx=3).encode(text='Ingreso:Q')\n\n(chart + labels)\n\n\n Provincia  Ingreso\n    GUAYAS     5399\n PICHINCHA     1699\nESMERALDAS     1581\n    MANABI     1448\n  LOS RIOS     1226\n\n\n\n\n\n\n\n\n\n\nCode\n# Asegurar limpieza de texto\ndf['Delito'] = df['Delito'].fillna('SIN ESPECIFICAR').astype(str).str.strip()\ndf['Provincia'] = df['Provincia'].astype(str).str.strip()\ndf['Ingreso'] = pd.to_numeric(df['Ingreso'], errors='coerce').fillna(0)\n\n# Filtrar solo el delito 140 ASESINATO\ndf_asesinato = df[df['Delito'].str.upper().str.contains(\"140 ASESINATO\")]\n\n# Identificar las top 5 provincias con más casos acumulados\ntop5_provincias = (df_asesinato.groupby('Provincia', as_index=False)['Ingreso']\n                   .sum()\n                   .sort_values('Ingreso', ascending=False)\n                   .head(5)['Provincia']\n                   .tolist())\n\n# Filtrar solo esas provincias\ndf_top5 = df_asesinato[df_asesinato['Provincia'].isin(top5_provincias)]\n\n# Agrupar por año y provincia\nevolucion = (df_top5.groupby(['Anio','Provincia'], as_index=False)['Ingreso']\n             .sum()\n             .sort_values(['Provincia','Anio']))\n\nprint(evolucion.to_string(index=False))  # Tabla en consola\n\n# Gráfica de evolución\nchart = alt.Chart(evolucion).mark_line(point=True).encode(\n    x=alt.X('Anio:O', title='Año'),\n    y=alt.Y('Ingreso:Q', title='Causas (Ingreso)'),\n    color=alt.Color('Provincia:N', title='Provincia'),\n    tooltip=['Anio','Provincia','Ingreso']\n).properties(\n    title='Evolución por año — Delito 140 ASESINATO (Top 5 provincias)',\n    width=500,\n    height=400\n).interactive()\n\nchart\n\n\n Anio  Provincia  Ingreso\n 2025 ESMERALDAS     1581\n 2025     GUAYAS     5399\n 2025   LOS RIOS     1226\n 2025     MANABI     1448\n 2025  PICHINCHA     1699\n\n\n\n\n\n\n\n\n\n\nCode\ndf = df.copy()\nfor c in ['Ingreso','Resuelta','Tramite']:\n    df[c] = pd.to_numeric(df[c], errors='coerce').fillna(0)\ndf['Provincia'] = df['Provincia'].astype(str).str.strip()\n\n# Provincias con mayor Resuelta (para ordenar/filtrar)\ntopN = 5\norden_prov = (df.groupby('Provincia', as_index=False)['Resuelta']\n                .sum()\n                .sort_values('Resuelta', ascending=False)\n                .head(topN)['Provincia']\n                .tolist())\n\n# Subset y pasar a formato largo\nsubset = (df[df['Provincia'].isin(orden_prov)]\n          .groupby('Provincia', as_index=False)[['Ingreso','Resuelta','Tramite']]\n          .sum())\n\ndatos_long = subset.melt('Provincia', var_name='Estado', value_name='Causas')\n\n# --- barras agrupadas (xOffset) ---\nchart = alt.Chart(datos_long).mark_line(point=True).encode(\n    x=alt.X('Provincia:N', sort=orden_prov, title='Provincia'),\n    y=alt.Y('Causas:Q', title='Número de causas'),\n    color=alt.Color('Estado:N', title='Estado'),\n    xOffset='Estado:N',  # &lt;--- Agrupa barras por Estado dentro de cada provincia\n    tooltip=['Provincia','Estado','Causas']\n).properties(\n    title=f'Provincias vs número de causas (Ingreso / Resuelta / Tramite) — Top {topN}',\n    width=600,\n    height=450\n)\n\nchart"
  },
  {
    "objectID": "cnj.html#dataset",
    "href": "cnj.html#dataset",
    "title": "Visualización de Data",
    "section": "",
    "text": "We will use the movies dataset from vega-datasets, which includes information about thousands of films such as their ratings, genres, duration, and box office revenue.\nLet’s load and preview the dataset:\n\n\nCode\nimport pandas as pd\nimport altair as alt\nfrom vega_datasets import data\n\n# Load dataset\nmovies = data.movies()\n\n# Show first rows\nmovies.head()\n\n\n\n\n\n\n\n\n\nTitle\nUS_Gross\nWorldwide_Gross\nUS_DVD_Sales\nProduction_Budget\nRelease_Date\nMPAA_Rating\nRunning_Time_min\nDistributor\nSource\nMajor_Genre\nCreative_Type\nDirector\nRotten_Tomatoes_Rating\nIMDB_Rating\nIMDB_Votes\n\n\n\n\n0\nThe Land Girls\n146083.0\n146083.0\nNaN\n8000000.0\nJun 12 1998\nR\nNaN\nGramercy\nNone\nNone\nNone\nNone\nNaN\n6.1\n1071.0\n\n\n1\nFirst Love, Last Rites\n10876.0\n10876.0\nNaN\n300000.0\nAug 07 1998\nR\nNaN\nStrand\nNone\nDrama\nNone\nNone\nNaN\n6.9\n207.0\n\n\n2\nI Married a Strange Person\n203134.0\n203134.0\nNaN\n250000.0\nAug 28 1998\nNone\nNaN\nLionsgate\nNone\nComedy\nNone\nNone\nNaN\n6.8\n865.0\n\n\n3\nLet's Talk About Sex\n373615.0\n373615.0\nNaN\n300000.0\nSep 11 1998\nNone\nNaN\nFine Line\nNone\nComedy\nNone\nNone\n13.0\nNaN\nNaN\n\n\n4\nSlam\n1009819.0\n1087521.0\nNaN\n1000000.0\nOct 09 1998\nR\nNaN\nTrimark\nOriginal Screenplay\nDrama\nContemporary Fiction\nNone\n62.0\n3.4\n165.0\n\n\n\n\n\n\n\nNow, let’s examine the shape (number of rows and columns) of the dataset:\n\n\nCode\nmovies.shape\n\n\n(3201, 16)\n\n\nThis tells us how many entries (rows) and features (columns) are present in the dataset."
  },
  {
    "objectID": "cnj.html#first-steps",
    "href": "cnj.html#first-steps",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "Before diving deeper into the data, it’s useful to explore some key metadata:\n\n✅ The column names and their data types\n⚠️ The presence of missing values\n📊 Summary statistics for numeric columns\n\n\n\nUnderstanding the structure of the dataset helps us know what type of data we’re dealing with.\n\n\nCode\nmovies.dtypes\n\n\nTitle                      object\nUS_Gross                  float64\nWorldwide_Gross           float64\nUS_DVD_Sales              float64\nProduction_Budget         float64\nRelease_Date               object\nMPAA_Rating                object\nRunning_Time_min          float64\nDistributor                object\nSource                     object\nMajor_Genre                object\nCreative_Type              object\nDirector                   object\nRotten_Tomatoes_Rating    float64\nIMDB_Rating               float64\nIMDB_Votes                float64\ndtype: object\n\n\nWe can also use .info() for a more complete summary, including non-null counts:\n\n\nCode\n# Overview of the dataset\nmovies.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3201 entries, 0 to 3200\nData columns (total 16 columns):\n #   Column                  Non-Null Count  Dtype  \n---  ------                  --------------  -----  \n 0   Title                   3200 non-null   object \n 1   US_Gross                3194 non-null   float64\n 2   Worldwide_Gross         3194 non-null   float64\n 3   US_DVD_Sales            564 non-null    float64\n 4   Production_Budget       3200 non-null   float64\n 5   Release_Date            3201 non-null   object \n 6   MPAA_Rating             2596 non-null   object \n 7   Running_Time_min        1209 non-null   float64\n 8   Distributor             2969 non-null   object \n 9   Source                  2836 non-null   object \n 10  Major_Genre             2926 non-null   object \n 11  Creative_Type           2755 non-null   object \n 12  Director                1870 non-null   object \n 13  Rotten_Tomatoes_Rating  2321 non-null   float64\n 14  IMDB_Rating             2988 non-null   float64\n 15  IMDB_Votes              2988 non-null   float64\ndtypes: float64(8), object(8)\nmemory usage: 400.3+ KB"
  },
  {
    "objectID": "cnj.html#missing-values",
    "href": "cnj.html#missing-values",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "Detecting and handling missing values is a critical step in any EDA process. Missing data can bias analysis or break downstream models if not handled properly.\n\nDetect patterns in missingness\nIdentify if some columns are almost entirely null\nDecide whether to drop or impute certain variables\n\n\n\nLet’s start by computing the percentage of missing values in each column:\n\n\nCode\nnan_percent = movies.isna().mean() * 100\nnan_percent_sorted = nan_percent.sort_values(ascending=False).round(2)\nnan_percent_sorted\n\n\nUS_DVD_Sales              82.38\nRunning_Time_min          62.23\nDirector                  41.58\nRotten_Tomatoes_Rating    27.49\nMPAA_Rating               18.90\nCreative_Type             13.93\nSource                    11.40\nMajor_Genre                8.59\nDistributor                7.25\nIMDB_Rating                6.65\nIMDB_Votes                 6.65\nUS_Gross                   0.22\nWorldwide_Gross            0.22\nTitle                      0.03\nProduction_Budget          0.03\nRelease_Date               0.00\ndtype: float64\n\n\n\n\n\nTo visualize missing values with Altair, we need to reshape the data into a long format where each missing value is a row:\n\n\nCode\nmovies_nans = movies.isna().reset_index().melt(\n    id_vars='index',\n    var_name='column',\n    value_name=\"NaN\"\n)\nmovies_nans\n\n\n\n\n\n\n\n\n\nindex\ncolumn\nNaN\n\n\n\n\n0\n0\nTitle\nFalse\n\n\n1\n1\nTitle\nFalse\n\n\n2\n2\nTitle\nFalse\n\n\n3\n3\nTitle\nFalse\n\n\n4\n4\nTitle\nFalse\n\n\n...\n...\n...\n...\n\n\n51211\n3196\nIMDB_Votes\nFalse\n\n\n51212\n3197\nIMDB_Votes\nTrue\n\n\n51213\n3198\nIMDB_Votes\nFalse\n\n\n51214\n3199\nIMDB_Votes\nFalse\n\n\n51215\n3200\nIMDB_Votes\nFalse\n\n\n\n\n51216 rows × 3 columns\n\n\n\n\n\n\nThis heatmap shows where missing values occur across rows and columns. Patterns may indicate:\n\nColumns with consistently missing values\nEntire rows with large gaps\nCorrelated missingness between variables\n\nTo avoid limitations in the number of rows rendered by Altair, we disable the max rows warning:\n\n\nCode\nalt.data_transformers.disable_max_rows()\n\n\nDataTransformerRegistry.enable('default')\n\n\nNow we can create the heatmap:\n\n\nCode\nalt.Chart(movies_nans).mark_rect().encode(\n    alt.X('index:O'),\n    alt.Y('column'),\n    alt.Color('NaN')\n).properties(\n    width=1000\n)\n\n\n\n\n\n\n\n\nThis plot can help identify columns or rows with critical data issues.\n\n\n\nIn many real-world cases, we may decide to remove columns that have too many missing values. Let’s set a threshold of 70%:\n\n\nCode\nthreshold_nan = 70 # in percent\ncols_to_drop = nan_percent[nan_percent&gt;threshold_nan].index\ncols_to_drop\n\n\nIndex(['US_DVD_Sales'], dtype='object')\n\n\nThese columns have more than 70% missing values and may not be useful for analysis."
  },
  {
    "objectID": "cnj.html#cleaned-dataset",
    "href": "cnj.html#cleaned-dataset",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "Finally, we drop the selected columns and inspect the updated dataset:\n\n\nCode\nmovies_cleaned = movies.drop(columns=cols_to_drop)\nmovies_cleaned\n\n\n\n\n\n\n\n\n\nTitle\nUS_Gross\nWorldwide_Gross\nProduction_Budget\nRelease_Date\nMPAA_Rating\nRunning_Time_min\nDistributor\nSource\nMajor_Genre\nCreative_Type\nDirector\nRotten_Tomatoes_Rating\nIMDB_Rating\nIMDB_Votes\n\n\n\n\n0\nThe Land Girls\n146083.0\n146083.0\n8000000.0\nJun 12 1998\nR\nNaN\nGramercy\nNone\nNone\nNone\nNone\nNaN\n6.1\n1071.0\n\n\n1\nFirst Love, Last Rites\n10876.0\n10876.0\n300000.0\nAug 07 1998\nR\nNaN\nStrand\nNone\nDrama\nNone\nNone\nNaN\n6.9\n207.0\n\n\n2\nI Married a Strange Person\n203134.0\n203134.0\n250000.0\nAug 28 1998\nNone\nNaN\nLionsgate\nNone\nComedy\nNone\nNone\nNaN\n6.8\n865.0\n\n\n3\nLet's Talk About Sex\n373615.0\n373615.0\n300000.0\nSep 11 1998\nNone\nNaN\nFine Line\nNone\nComedy\nNone\nNone\n13.0\nNaN\nNaN\n\n\n4\nSlam\n1009819.0\n1087521.0\n1000000.0\nOct 09 1998\nR\nNaN\nTrimark\nOriginal Screenplay\nDrama\nContemporary Fiction\nNone\n62.0\n3.4\n165.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3196\nZack and Miri Make a Porno\n31452765.0\n36851125.0\n24000000.0\nOct 31 2008\nR\n101.0\nWeinstein Co.\nOriginal Screenplay\nComedy\nContemporary Fiction\nKevin Smith\n65.0\n7.0\n55687.0\n\n\n3197\nZodiac\n33080084.0\n83080084.0\n85000000.0\nMar 02 2007\nR\n157.0\nParamount Pictures\nBased on Book/Short Story\nThriller/Suspense\nDramatization\nDavid Fincher\n89.0\nNaN\nNaN\n\n\n3198\nZoom\n11989328.0\n12506188.0\n35000000.0\nAug 11 2006\nPG\nNaN\nSony Pictures\nBased on Comic/Graphic Novel\nAdventure\nSuper Hero\nPeter Hewitt\n3.0\n3.4\n7424.0\n\n\n3199\nThe Legend of Zorro\n45575336.0\n141475336.0\n80000000.0\nOct 28 2005\nPG\n129.0\nSony Pictures\nRemake\nAdventure\nHistorical Fiction\nMartin Campbell\n26.0\n5.7\n21161.0\n\n\n3200\nThe Mask of Zorro\n93828745.0\n233700000.0\n65000000.0\nJul 17 1998\nPG-13\n136.0\nSony Pictures\nRemake\nAdventure\nHistorical Fiction\nMartin Campbell\n82.0\n6.7\n4789.0\n\n\n\n\n3201 rows × 15 columns"
  },
  {
    "objectID": "cnj.html#univariate-analysis-quantitative",
    "href": "cnj.html#univariate-analysis-quantitative",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "2.1 Univariate Analysis: Quantitative",
    "text": "2.1 Univariate Analysis: Quantitative\nA univariate analysis focuses on examining a single numeric variable to understand its distribution, shape, central tendency, and spread. One of the most common tools for this is the histogram.\nIn this case, we’ll explore the distribution of the movie runtime (Running_Time_min).\n\n2.1.1 Basic Histogram\nWe start by creating a histogram to visualize the distribution of running times:\n\n\nCode\nalt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('Running_Time_min',bin=alt.Bin(maxbins=30)),\n    alt.Y('count()')\n).properties(\n    title='Histogram of Movie Runtimes (30 bins)'\n)\n\n\n\n\n\n\n\n\nThis chart shows how many movies fall into each time interval (bin). However, histograms can look quite different depending on the number and size of bins used.\n\n\n2.1.2 Effect of Bin Size\nLet’s compare how the histogram shape changes with different bin sizes:\n\n\nCode\nhistogram_1 = alt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('Running_Time_min',bin=alt.Bin(maxbins=8)),\n    alt.Y('count()')\n)\n\nhistogram_2 = alt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('Running_Time_min',bin=alt.Bin(maxbins=10)),\n    alt.Y('count()')\n)\n\nhistogram_1 | histogram_2\n\n\n\n\n\n\n\n\nEven though both plots use the same data, the choice of bin size changes the visual interpretation. A small number of bins may hide details, while too many bins can make it harder to spot trends.\n\n\n2.1.3 Density plots, or Kernel Density Estimate (KDE)\nDensity plots offer a smoothed alternative to histograms. Instead of using rectangular bins to count data points, they estimate the probability density function by placing bell-shaped curves (kernels) at each observation and summing them.\nThis approach helps reduce the visual noise and jaggedness that can occur in histograms and gives a clearer picture of the underlying distribution.\n\n\nCode\nalt.Chart(movies_cleaned).transform_density(\n    'Running_Time_min',\n    as_=['Running_Time_min','density'],\n).mark_area().encode(\n    alt.X('Running_Time_min'),\n    alt.Y('density:Q')\n).properties(\n    title=\"Movies runtime\"\n)\n\n\n\n\n\n\n\n\n\n\n2.1.4 Grouped Density plot\nWe can also compare distributions across groups by splitting the KDE by a categorical variable using the groupby parameter. This helps us see how the distribution differs between categories, such as genres.\n\n\nCode\nselection = alt.selection_point(fields=['Major_Genre'], bind='legend')\n\nalt.Chart(movies_cleaned).transform_density(\n    'Running_Time_min',\n    groupby=['Major_Genre'],\n    as_=['Running_Time_min', 'density'],\n).mark_area(opacity=0.5).encode(\n    alt.X('Running_Time_min'),\n    alt.Y('density:Q', stack=None),\n    alt.Color('Major_Genre'),\n    opacity=alt.condition(selection, \n        alt.value(1), \n        alt.value(0.05)\n    )\n).add_params(\n    selection\n).properties(\n    title=\"Movies Runtime by Genre (Interactive Filter)\"\n).interactive()\n\n\n\n\n\n\n\n\nThe transparency (opacity=0.5) allows us to observe overlapping distributions and ensures that small density areas are not completely hidden behind larger ones.\nFrom this plot, we can observe, for example, that Drama movies have runtimes nearly as long as the longest Adventure movies, even though their overall distributions differ."
  },
  {
    "objectID": "cnj.html#bivariate-analysis-categorical-vs-quantitative",
    "href": "cnj.html#bivariate-analysis-categorical-vs-quantitative",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "2.2 Bivariate Analysis: Categorical vs Quantitative",
    "text": "2.2 Bivariate Analysis: Categorical vs Quantitative\nBivariate analysis examines the relationship between two variables. In this case, we focus on one categorical variable (e.g., genre) and one quantitative variable (e.g., revenue), which is a very common scenario in exploratory data analysis.\nThis type of analysis is useful to: - Compare average or median values across categories. - Detect outliers or high-variance groups. - Understand distributional differences across categories.\nBelow are several effective visualizations for this analysis.\n\n2.2.1 Basic Barchart\nBar charts are effective for comparing aggregated values (like the mean) across different groups. However, they hide the distribution and variation within each group.\n\n\nCode\nalt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('mean(Worldwide_Gross)'),\n    alt.Y(\"Major_Genre\")\n).properties(\n    title=\"Average Worldwide Gross by Genre\"\n)\n\n\n\n\n\n\n\n\nThis bar chart shows the mean Worldwide Gross per genre. It is useful for identifying which genres are more profitable on average, but does not show how spread out the data is.\n\n\n2.2.2 Tick Plot\nTo visualize individual data points, we use a tick plot. This helps uncover variability within genres and detect outliers.\n\n\nCode\nalt.Chart(movies_cleaned).mark_tick().encode(\n    alt.X('Worldwide_Gross'),\n    alt.Y(\"Major_Genre\"),\n    alt.Tooltip('Title:N')\n).properties(\n    title=\"Individual Gross per Movie by Genre\"\n)\n\n\n\n\n\n\n\n\n\n\n2.2.3 Heatmaps\nHeatmaps can summarize the frequency of data points across both axes (quantitative and categorical) using color intensity. It’s particularly useful for spotting patterns without getting overwhelmed by individual points.\n\n\nCode\nalt.Chart(movies_cleaned).mark_rect().encode(\n    alt.X('Worldwide_Gross',bin=alt.Bin(maxbins=100)),\n    alt.Y(\"Major_Genre\"),\n    alt.Color('count()'),\n    alt.Tooltip('count()')\n).properties(\n    title=\"Heatmap of Movie Counts by Gross and Genre\"\n)\n\n\n\n\n\n\n\n\nThis heatmap shows how frequently movies from each genre fall into different revenue ranges.\n\n\n2.2.4 Boxplot\nBoxplots are useful for comparing distributions across categories and identifying outliers. Boxplots summarize a distribution using five statistics:\n\nMedian (Q2)\nFirst Quartile (Q1)\nThird Quartile (Q3)\nLower Whisker (Q1 - 1.5 × IQR)\nUpper Whisker (Q3 + 1.5 × IQR)\n\n\n\nCode\nalt.Chart(movies_cleaned).mark_boxplot().encode(\n    alt.X('Worldwide_Gross'),\n    alt.Y(\"Major_Genre\")\n).properties(\n    title=\"Boxplot of Worldwide Gross by Genre\"\n)\n\n\n\n\n\n\n\n\n\n\n2.2.5 Side-by-side: Boxplot and Bar Chart\nTo contrast aggregated values (bar chart) with the full distribution (boxplot), we can display them together:\n\n\nCode\nbar = alt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('mean(Worldwide_Gross)'),\n    alt.Y(\"Major_Genre\")\n)\n\nbox = alt.Chart(movies_cleaned).mark_boxplot().encode(\n    alt.X('mean(Worldwide_Gross)'),\n    alt.Y(\"Major_Genre\")\n)\n\nbox | bar\n\n\n\n\n\n\n\n\nThis comparison reveals whether the mean is a good representative of the genre, or whether the data is skewed or contains outliers that affect the average"
  },
  {
    "objectID": "cnj.html#bivariate-analysis-quantitative-vs-quantitative",
    "href": "cnj.html#bivariate-analysis-quantitative-vs-quantitative",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "2.3 Bivariate Analysis: Quantitative vs Quantitative",
    "text": "2.3 Bivariate Analysis: Quantitative vs Quantitative\nWhen analyzing two quantitative (numerical) variables simultaneously, we aim to discover possible relationships, trends, or correlations. This type of bivariate analysis can reveal whether increases in one variable are associated with increases or decreases in another (positive or negative correlation), or if there’s no relationship at all. The most common and intuitive visualization for this is the scatterplot.\n\n2.3.1 Scatterplots\nScatter plots are effective visualizations for exploring two-dimensional distributions, allowing us to identify patterns, trends, clusters, or outliers.\nLet’s start by visualizing how movies are rated across two popular online platforms:\n\nIMDb\n\nRotten Tomatoes\n\nAre movies rated similarly on different platforms?\n\n\nCode\nalt.Chart(movies_cleaned).mark_point().encode(\n    alt.X('IMDB_Rating'),\n    alt.Y('Rotten_Tomatoes_Rating')\n).properties(\n    title=\"IMDB vs Rotten Tomatoes Ratings\"\n)\n\n\n\n\n\n\n\n\n\n\n2.3.2 Scatterplot Saturation\nScatterplots can become saturated when too many points overlap in a small area of the chart, making it difficult to distinguish dense regions from sparse ones. For example, when plotting financial variables like production budget versus worldwide gross:\n\n\nCode\nsaturated = alt.Chart(movies_cleaned).mark_point().encode(\n    alt.X('Production_Budget'),\n    alt.Y('Worldwide_Gross')\n).properties(\n    title=\"Saturated Scatterplot: Budget vs Gross\"\n)\nsaturated\n\n\n\n\n\n\n\n\n\n\n2.3.3 Using Binned Heatmap to Reduce Saturation\nTo address saturation, we can bin both variables and use a heatmap where the color intensity represents the number of movies that fall into each rectangular region of the grid. This makes dense areas more interpretable\n\n\nCode\nheatmap_scatter = alt.Chart(movies_cleaned).mark_rect().encode(\n    alt.X('Production_Budget', bin=alt.Bin(maxbins=60)),\n    alt.Y('Worldwide_Gross', bin=alt.Bin(maxbins=60)),\n    alt.Color('count()'),\n    alt.Tooltip('count()')\n).properties(\n    title=\"Binned Heatmap: Budget vs Gross\"\n)\nheatmap_scatter\n\n\n\n\n\n\n\n\n\n\n2.3.4 Side-by-side Comparison\nCompare the raw scatterplot with the heatmap representation:\n\n\nCode\nsaturated | heatmap_scatter"
  },
  {
    "objectID": "cnj.html#bivariate-analysis-categorical-vs-categorical",
    "href": "cnj.html#bivariate-analysis-categorical-vs-categorical",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "2.4 Bivariate Analysis: Categorical vs Categorical",
    "text": "2.4 Bivariate Analysis: Categorical vs Categorical\nWhen working with two categorical variables, bivariate analysis helps us understand how categories from one variable relate or are distributed across the other. For example, we might want to know how different movie genres are rated according to the MPAA rating system. Visualization techniques like grouped bar charts and faceted plots can reveal patterns, associations, or class imbalances.\n\n2.4.1 Basic Faceted Bar Chart\nWe begin by exploring how movies are rated (MPAA_Rating) across different genres (Major_Genre). A faceted bar chart allows us to visualize this relationship by plotting a bar chart per genre, helping to identify genre-specific rating distributions.\n\n\nCode\nalt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('count()'),\n    alt.Y('MPAA_Rating'),\n    alt.Color('MPAA_Rating')\n).facet(\n    'Major_Genre'\n)\n\n\n\n\n\n\n\n\n\n\n2.4.2 Vertical Faceting for Alignment\nFaceting horizontally can make comparisons across genres harder when the x-axis is misaligned. By specifying columns=1, we lay out the facets vertically, making it easier to compare counts across genres.\n\n\nCode\nalt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('count()'),\n    alt.Y('MPAA_Rating'),\n    alt.Color('MPAA_Rating')\n).facet(\n    'Major_Genre',\n    columns=1\n)\n\n\n\n\n\n\n\n\n\n\n2.4.3 Dependent vs Independent Axis Scaling\nBy default, facet plots share the same x-axis scale (dependent scale), which allows for easier comparison across panels. However, when the number of observations varies greatly between genres, this shared scale can compress some charts.\nWe can instead use independent x-axis scaling for each facet. This highlights the relative distribution within each genre.\n\n\nCode\nshared_scale = alt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('count()'),\n    alt.Y('MPAA_Rating'),\n    alt.Color('MPAA_Rating')\n).facet(\n    'Major_Genre',\n    columns=4\n)\n\nindependent_scale = alt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('count()'),\n    alt.Y('MPAA_Rating'),\n    alt.Color('MPAA_Rating')\n).facet(\n    'Major_Genre',\n    columns=4\n).resolve_scale(x='independent')\n\nshared_scale | independent_scale\n\n\n\n\n\n\n\n\nThe left panel (shared scale) makes absolute comparisons between genres, while the right panel (independent scale) makes within-genre comparisons more readable.\n\n\n2.4.4 Heatmaps\nHeatmaps are effective for visualizing the relationship between two categorical variables when the goal is to display counts or frequency of occurrences. They map the number of observations to color, providing an intuitive view of which category pairs are most or least common.\nWe can enhance this basic representation by also using marker size, combining both color intensity and circle area to represent counts more effectively. This dual encoding can improve interpretation, especially when printed in grayscale or when there are subtle color differences.\n\n\nCode\nheatmap_color = alt.Chart(movies_cleaned).mark_rect().encode(\n    alt.X('MPAA_Rating'),\n    alt.Y('Major_Genre', sort='color'),\n    alt.Color('count()')\n).properties(\n    title=\"Heatmap with Color (Count of Movies)\"\n)\n\nheatmap_size = alt.Chart(movies_cleaned).mark_circle().encode(\n    alt.X('MPAA_Rating'),\n    alt.Y('Major_Genre', sort='color'),\n    alt.Color('count()'),\n    alt.Size('count()')\n).properties(\n    title=\"Heatmap with Color + Size (Count of Movies)\"\n)\n\nheatmap_color | heatmap_size"
  },
  {
    "objectID": "cnj.html#multivariate-analysis",
    "href": "cnj.html#multivariate-analysis",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "2.5 Multivariate Analysis",
    "text": "2.5 Multivariate Analysis\nMultivariate analysis helps us understand the interactions and relationships among multiple variables simultaneously. In the context of numerical features, it is useful to explore pairwise distributions, correlations, and detect potential clusters or anomalies.\nWhen the number of variables is large, repeated charts such as histograms or scatter plot matrices help us summarize patterns efficiently and consistently across all numerical dimensions.\n\n2.5.1 Repeated Histograms for Numerical Columns\nWe first identify and isolate all numerical columns from the dataset. Then we repeat a histogram for each of these columns to understand the individual distributions. This overview is helpful to detect skewness, outliers, or binning decisions that affect how data is grouped visually.\n\n\nCode\n# Select only numerical columns\nnumerical_columns = movies_cleaned.select_dtypes('number').columns.tolist()\n\n\n\n\nCode\nalt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X(alt.repeat(),type='quantitative',bin=alt.Bin(maxbins=30)),\n    alt.Y('count()')\n).properties(\n    width=150,\n    height=150\n).repeat(\n    numerical_columns,\n    columns=4\n)\n\n\n\n\n\n\n\n\n\n\n2.5.2 Scatter Plot Matrix (Pairplot)\nA scatter plot matrix shows the pairwise relationships between all numerical variables. This is a common exploratory tool to detect:\n\nCorrelations between variables\nOutliers or clusters\nRelationships useful for prediction models (e.g., to predict rating or budget)\n\nWe focus especially on the plots below the diagonal, as they are not duplicated.\n\n\nCode\nalt.Chart(movies_cleaned).mark_point().encode(\n    alt.X(alt.repeat('column'),type='quantitative'),\n    alt.Y(alt.repeat('row'),type='quantitative'),\n    alt.Tooltip('Title:N')\n).properties(\n    width=100,\n    height=100\n).repeat(\n    column=numerical_columns,\n    row=numerical_columns\n)\n\n\n\n\n\n\n\n\n\n\n2.5.3 Heatmap Matrix\nWhen scatter plots become too saturated (many overlapping points), heatmaps offer a better alternative by binning the numeric values and encoding the count in color intensity.\n\n\nCode\nalt.Chart(movies_cleaned).mark_rect().encode(\n    alt.X(alt.repeat('column'),type='quantitative',bin=alt.Bin(maxbins=30)),\n    alt.Y(alt.repeat('row'),type='quantitative',bin=alt.Bin(maxbins=30)),\n    alt.Color('count()'),\n    alt.Tooltip('count()')\n).properties(\n    width=100,\n    height=100\n).repeat(\n    column=numerical_columns,\n    row=numerical_columns\n).resolve_scale(\n    color='independent'\n)\n\n\n\n\n\n\n\n\nTo gain deeper insights into the dataset, it’s important to analyze how numerical variables behave across different categories. This type of multivariate analysis allows us to:\n\nCompare distributions across categories\nDetect outliers within categories\nObserve central tendency (median, quartiles) and spread (range, IQR)\n\nBoxplots are particularly effective for this purpose. In the following visualizations, we explore these relationships by repeating plots across combinations of categorical and numerical features.\n\n\n2.5.4 Filter Categorical Columns\nFirst, we select the relevant categorical columns, excluding identifiers and text-heavy variables like movie titles or director names.\n\n\nCode\ncategorical_columns =  movies_cleaned.select_dtypes('object').columns.to_list()\n\ncategorical_columns_remove = ['Title','Release_Date','Distributor','Director']\n\ncategorical_filtered = [col for col in categorical_columns if col not in categorical_columns_remove]\n\n\n\n\n2.5.5 Repeated Boxplots: Categorical vs Numerical\nWe repeat boxplots using combinations of categorical (rows) and numerical (columns) features. This matrix layout gives a clear visual overview of how numerical values are distributed within each category.\n\n\nCode\nalt.Chart(movies_cleaned).mark_boxplot().encode(\n    alt.X(alt.repeat('column'),type='quantitative'),\n    alt.Y(alt.repeat('row'),type='nominal'),\n    alt.Size('count()')\n).properties(\n    width=200,\n    height=200\n).repeat(\n    column=numerical_columns,\n    row=categorical_filtered\n)\n\n\n\n\n\n\n\n\n\n\n2.5.6 Faceted Boxplots\nFor more focused analysis, we can facet the boxplots using a specific categorical variable like MPAA_Rating, and repeat the chart by different categorical rows. This lets us keep the numerical axis fixed while comparing how categories vary across different classes (e.g., movie ratings).\n\n\nCode\nalt.Chart(movies_cleaned).mark_boxplot().encode(\n    alt.X('Running_Time_min', type='quantitative'),\n    alt.Y(alt.repeat('row'),type='nominal'),\n    alt.Size('count()'),\n    alt.Tooltip('Title:N')\n).properties(\n    width=100,\n    height=100\n).facet(\n    column='MPAA_Rating'\n).repeat(\n    row=categorical_filtered\n)"
  },
  {
    "objectID": "cnj.html#titulo-y-proposito",
    "href": "cnj.html#titulo-y-proposito",
    "title": "Visualización de Data",
    "section": "",
    "text": "Nombre del dataset: Numero de causas ingresadas, resueltas y en tramite a nivel nacional (ambitos COIP, COGEP y Constitucional).\n\nObjetivo: Mostrar estadisticas judiciales de Ecuador sobre cuantas causas se han ingresado, resuelto o se encuentran en tramite, desglosadas por materia, tipo de accion, delito, provincia, canton, judicatura e instancia."
  },
  {
    "objectID": "cnj.html#cobertura-temporal-y-actualizacion",
    "href": "cnj.html#cobertura-temporal-y-actualizacion",
    "title": "Visualización de Data",
    "section": "",
    "text": "Periodo: Desde 2017 hasta la fecha de corte.\n\nUltima actualizacion: 17 de septiembre de 2025.\n\nPeriodicidad: Mensual (se actualiza cada mes)."
  },
  {
    "objectID": "cnj.html#fuente-y-autoria",
    "href": "cnj.html#fuente-y-autoria",
    "title": "Visualización de Data",
    "section": "",
    "text": "Organismo responsable: Consejo de la Judicatura de Ecuador.\n\nDepartamento: Direccion Nacional de Estudios Jurimetricos y Estadistica Judicial.\n\nLicencia: Creative Commons Attribution (CC-BY)."
  },
  {
    "objectID": "cnj.html#variables-destacadas",
    "href": "cnj.html#variables-destacadas",
    "title": "Visualización de Data",
    "section": "",
    "text": "El dataset incluye información desagregada en los siguientes campos:\n\nAño y Mes\n\nMateria (penal, constitucional, administrativo, etc.)\n\nTipo de acción / clase de causa\n\nDelito (cuando aplica)\n\nProvincia y Cantón\n\nJudicatura e Instancia\n\nNúmero de causas:\n\nIngresadas\n\nResueltas\n\nEn trámite"
  },
  {
    "objectID": "cnj.html#título-y-propósito",
    "href": "cnj.html#título-y-propósito",
    "title": "Visualización de Data",
    "section": "",
    "text": "Nombre del dataset: Número de causas ingresadas, resueltas y en trámite a nivel nacional (ámbitos COIP, COGEP y Constitucional).\n\nObjetivo: Mostrar estadísticas judiciales de Ecuador sobre cuántas causas se han ingresado, resuelto o se encuentran en trámite, desglosadas por materia, tipo de acción, delito, provincia, cantón, judicatura e instancia."
  },
  {
    "objectID": "cnj.html#cobertura-temporal-y-actualización",
    "href": "cnj.html#cobertura-temporal-y-actualización",
    "title": "Visualización de Data",
    "section": "",
    "text": "Período: Desde 2017 hasta la fecha de corte.\n\nÚltima actualización: 17 de septiembre de 2025.\n\nPeriodicidad: Mensual (se actualiza cada mes)."
  },
  {
    "objectID": "cnj.html#fuente-y-autoría",
    "href": "cnj.html#fuente-y-autoría",
    "title": "Visualización de Data",
    "section": "",
    "text": "Organismo responsable: Consejo de la Judicatura de Ecuador.\n\nDepartamento: Dirección Nacional de Estudios Jurimétricos y Estadística Judicial.\n\nLicencia: Creative Commons Attribution (CC-BY)."
  },
  {
    "objectID": "cnj.html#carga-del-dataset",
    "href": "cnj.html#carga-del-dataset",
    "title": "Visualización de Data",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport altair as alt\n\n# Ruta local del archivo .ods\nfile = \"/Users/usr-s3c/Documents/Maestria_YTech/Machine-Learning/cnj.csv\"\n# Leer la primera hoja del archivo\ndf = pd.read_csv(file, sep=\";\", low_memory=False)\n\n# Mostrar las primeras filas\nprint(df.head(20))\n\n\n                       Materia            TipoAccion  \\\n0   ADOLESCENTE INFRACTOR COIP  ACCION PENAL PUBLICA   \n1   ADOLESCENTE INFRACTOR COIP  ACCION PENAL PUBLICA   \n2   ADOLESCENTE INFRACTOR COIP  ACCION PENAL PUBLICA   \n3   ADOLESCENTE INFRACTOR COIP  ACCION PENAL PUBLICA   \n4   ADOLESCENTE INFRACTOR COIP  ACCION PENAL PUBLICA   \n5   ADOLESCENTE INFRACTOR COIP  ACCION PENAL PUBLICA   \n6   ADOLESCENTE INFRACTOR COIP  ACCION PENAL PUBLICA   \n7   ADOLESCENTE INFRACTOR COIP  ACCION PENAL PUBLICA   \n8   ADOLESCENTE INFRACTOR COIP  ACCION PENAL PUBLICA   \n9   ADOLESCENTE INFRACTOR COIP  ACCION PENAL PUBLICA   \n10  ADOLESCENTE INFRACTOR COIP  ACCION PENAL PUBLICA   \n11  ADOLESCENTE INFRACTOR COIP  ACCION PENAL PUBLICA   \n12  ADOLESCENTE INFRACTOR COIP  ACCION PENAL PUBLICA   \n13  ADOLESCENTE INFRACTOR COIP  ACCION PENAL PUBLICA   \n14  ADOLESCENTE INFRACTOR COIP  ACCION PENAL PUBLICA   \n15  ADOLESCENTE INFRACTOR COIP  ACCION PENAL PUBLICA   \n16  ADOLESCENTE INFRACTOR COIP  ACCION PENAL PUBLICA   \n17  ADOLESCENTE INFRACTOR COIP  ACCION PENAL PUBLICA   \n18  ADOLESCENTE INFRACTOR COIP  ACCION PENAL PUBLICA   \n19  ADOLESCENTE INFRACTOR COIP  ACCION PENAL PUBLICA   \n\n                           Delito   Provincia           Canton  \\\n0   ART. 084 DESAPARICION FORZADA       AZUAY           CUENCA   \n1   ART. 084 DESAPARICION FORZADA       AZUAY           CUENCA   \n2   ART. 084 DESAPARICION FORZADA   SUCUMBIOS  GONZALO PIZARRO   \n3               ART. 088 AGRESION       AZUAY           CUENCA   \n4               ART. 088 AGRESION       AZUAY           CUENCA   \n5               ART. 088 AGRESION       AZUAY           CUENCA   \n6               ART. 088 AGRESION       AZUAY           CUENCA   \n7               ART. 088 AGRESION       AZUAY           CUENCA   \n8               ART. 088 AGRESION       AZUAY           CUENCA   \n9               ART. 088 AGRESION       AZUAY           CUENCA   \n10              ART. 088 AGRESION       AZUAY           CUENCA   \n11              ART. 088 AGRESION       AZUAY         GUALACEO   \n12              ART. 088 AGRESION       AZUAY         GUALACEO   \n13              ART. 088 AGRESION      CARCHI           TULCAN   \n14              ART. 088 AGRESION  CHIMBORAZO         RIOBAMBA   \n15              ART. 088 AGRESION  CHIMBORAZO         RIOBAMBA   \n16              ART. 088 AGRESION  CHIMBORAZO         RIOBAMBA   \n17              ART. 088 AGRESION  CHIMBORAZO         RIOBAMBA   \n18              ART. 088 AGRESION  CHIMBORAZO         RIOBAMBA   \n19              ART. 088 AGRESION    COTOPAXI        LATACUNGA   \n\n                               Judicatura            Instancia Periodo  \\\n0                       UJ FMNA DE CUENCA  UNIDADES JUDICIALES  1/1/17   \n1                       UJ FMNA DE CUENCA  UNIDADES JUDICIALES  1/1/18   \n2   UJ MULTICOMPETENTE DE GONZALO PIZARRO  UNIDADES JUDICIALES  1/1/17   \n3                       UJ FMNA DE CUENCA  UNIDADES JUDICIALES  1/1/16   \n4                       UJ FMNA DE CUENCA  UNIDADES JUDICIALES  1/1/17   \n5                       UJ FMNA DE CUENCA  UNIDADES JUDICIALES  1/1/18   \n6                       UJ FMNA DE CUENCA  UNIDADES JUDICIALES  1/1/19   \n7                       UJ FMNA DE CUENCA  UNIDADES JUDICIALES  1/1/20   \n8                       UJ FMNA DE CUENCA  UNIDADES JUDICIALES  1/1/21   \n9                       UJ FMNA DE CUENCA  UNIDADES JUDICIALES  1/1/23   \n10                      UJ FMNA DE CUENCA  UNIDADES JUDICIALES  1/1/24   \n11                    UJ FMNA DE GUALACEO  UNIDADES JUDICIALES  1/1/17   \n12                    UJ FMNA DE GUALACEO  UNIDADES JUDICIALES  1/1/18   \n13                      UJ FMNA DE TULCAN  UNIDADES JUDICIALES  1/1/16   \n14                    UJ FMNA DE RIOBAMBA  UNIDADES JUDICIALES  1/1/16   \n15                    UJ FMNA DE RIOBAMBA  UNIDADES JUDICIALES  1/1/17   \n16                    UJ FMNA DE RIOBAMBA  UNIDADES JUDICIALES  1/1/19   \n17                    UJ FMNA DE RIOBAMBA  UNIDADES JUDICIALES  1/1/24   \n18                    UJ FMNA DE RIOBAMBA  UNIDADES JUDICIALES  1/1/25   \n19                   UJ FMNA DE LATACUNGA  UNIDADES JUDICIALES  1/1/17   \n\n    Ingreso  Resuelta  Tramite  Mes  Anio  \n0         3         2        1    8  2025  \n1        57        49        9    8  2025  \n2         2         2        0    8  2025  \n3         2         1        1    8  2025  \n4         1         0        2    8  2025  \n5         1         1        2    8  2025  \n6         2         1        3    8  2025  \n7         3         3        3    8  2025  \n8         2         2        3    8  2025  \n9         2         1        4    8  2025  \n10        1         0        5    8  2025  \n11        1         1        0    8  2025  \n12        1         1        0    8  2025  \n13        1         1        0    8  2025  \n14        2         1        1    8  2025  \n15        1         1        1    8  2025  \n16        4         1        4    8  2025  \n17        2         0        6    8  2025  \n18        1         0        7    8  2025  \n19        1         1        0    8  2025  \n\n\n\n\nCode\n# Mostrar nombres de todas las columnas\nprint(df.columns.tolist())\n\n\n['Materia', 'TipoAccion', 'Delito', 'Provincia', 'Canton', 'Judicatura', 'Instancia', 'Periodo', 'Ingreso', 'Resuelta', 'Tramite', 'Mes', 'Anio']\n\n\n\n\nCode\nprint(\"Número de columnas:\", len(df.columns))\n\n\nNúmero de columnas: 13\n\n\n\n\nCode\nprint(df.info())\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 235644 entries, 0 to 235643\nData columns (total 13 columns):\n #   Column      Non-Null Count   Dtype \n---  ------      --------------   ----- \n 0   Materia     235644 non-null  object\n 1   TipoAccion  235644 non-null  object\n 2   Delito      235644 non-null  object\n 3   Provincia   235644 non-null  object\n 4   Canton      235644 non-null  object\n 5   Judicatura  235644 non-null  object\n 6   Instancia   235644 non-null  object\n 7   Periodo     235644 non-null  object\n 8   Ingreso     235644 non-null  int64 \n 9   Resuelta    235644 non-null  int64 \n 10  Tramite     235644 non-null  int64 \n 11  Mes         235644 non-null  int64 \n 12  Anio        235644 non-null  int64 \ndtypes: int64(5), object(8)\nmemory usage: 23.4+ MB\nNone\n\n\n\n\nCode\ndf.head(20)\n\n\n\n\n\n\n\n\n\nMateria\nTipoAccion\nDelito\nProvincia\nCanton\nJudicatura\nInstancia\nPeriodo\nIngreso\nResuelta\nTramite\nMes\nAnio\n\n\n\n\n0\nADOLESCENTE INFRACTOR COIP\nACCION PENAL PUBLICA\nART. 084 DESAPARICION FORZADA\nAZUAY\nCUENCA\nUJ FMNA DE CUENCA\nUNIDADES JUDICIALES\n1/1/17\n3\n2\n1\n8\n2025\n\n\n1\nADOLESCENTE INFRACTOR COIP\nACCION PENAL PUBLICA\nART. 084 DESAPARICION FORZADA\nAZUAY\nCUENCA\nUJ FMNA DE CUENCA\nUNIDADES JUDICIALES\n1/1/18\n57\n49\n9\n8\n2025\n\n\n2\nADOLESCENTE INFRACTOR COIP\nACCION PENAL PUBLICA\nART. 084 DESAPARICION FORZADA\nSUCUMBIOS\nGONZALO PIZARRO\nUJ MULTICOMPETENTE DE GONZALO PIZARRO\nUNIDADES JUDICIALES\n1/1/17\n2\n2\n0\n8\n2025\n\n\n3\nADOLESCENTE INFRACTOR COIP\nACCION PENAL PUBLICA\nART. 088 AGRESION\nAZUAY\nCUENCA\nUJ FMNA DE CUENCA\nUNIDADES JUDICIALES\n1/1/16\n2\n1\n1\n8\n2025\n\n\n4\nADOLESCENTE INFRACTOR COIP\nACCION PENAL PUBLICA\nART. 088 AGRESION\nAZUAY\nCUENCA\nUJ FMNA DE CUENCA\nUNIDADES JUDICIALES\n1/1/17\n1\n0\n2\n8\n2025\n\n\n5\nADOLESCENTE INFRACTOR COIP\nACCION PENAL PUBLICA\nART. 088 AGRESION\nAZUAY\nCUENCA\nUJ FMNA DE CUENCA\nUNIDADES JUDICIALES\n1/1/18\n1\n1\n2\n8\n2025\n\n\n6\nADOLESCENTE INFRACTOR COIP\nACCION PENAL PUBLICA\nART. 088 AGRESION\nAZUAY\nCUENCA\nUJ FMNA DE CUENCA\nUNIDADES JUDICIALES\n1/1/19\n2\n1\n3\n8\n2025\n\n\n7\nADOLESCENTE INFRACTOR COIP\nACCION PENAL PUBLICA\nART. 088 AGRESION\nAZUAY\nCUENCA\nUJ FMNA DE CUENCA\nUNIDADES JUDICIALES\n1/1/20\n3\n3\n3\n8\n2025\n\n\n8\nADOLESCENTE INFRACTOR COIP\nACCION PENAL PUBLICA\nART. 088 AGRESION\nAZUAY\nCUENCA\nUJ FMNA DE CUENCA\nUNIDADES JUDICIALES\n1/1/21\n2\n2\n3\n8\n2025\n\n\n9\nADOLESCENTE INFRACTOR COIP\nACCION PENAL PUBLICA\nART. 088 AGRESION\nAZUAY\nCUENCA\nUJ FMNA DE CUENCA\nUNIDADES JUDICIALES\n1/1/23\n2\n1\n4\n8\n2025\n\n\n10\nADOLESCENTE INFRACTOR COIP\nACCION PENAL PUBLICA\nART. 088 AGRESION\nAZUAY\nCUENCA\nUJ FMNA DE CUENCA\nUNIDADES JUDICIALES\n1/1/24\n1\n0\n5\n8\n2025\n\n\n11\nADOLESCENTE INFRACTOR COIP\nACCION PENAL PUBLICA\nART. 088 AGRESION\nAZUAY\nGUALACEO\nUJ FMNA DE GUALACEO\nUNIDADES JUDICIALES\n1/1/17\n1\n1\n0\n8\n2025\n\n\n12\nADOLESCENTE INFRACTOR COIP\nACCION PENAL PUBLICA\nART. 088 AGRESION\nAZUAY\nGUALACEO\nUJ FMNA DE GUALACEO\nUNIDADES JUDICIALES\n1/1/18\n1\n1\n0\n8\n2025\n\n\n13\nADOLESCENTE INFRACTOR COIP\nACCION PENAL PUBLICA\nART. 088 AGRESION\nCARCHI\nTULCAN\nUJ FMNA DE TULCAN\nUNIDADES JUDICIALES\n1/1/16\n1\n1\n0\n8\n2025\n\n\n14\nADOLESCENTE INFRACTOR COIP\nACCION PENAL PUBLICA\nART. 088 AGRESION\nCHIMBORAZO\nRIOBAMBA\nUJ FMNA DE RIOBAMBA\nUNIDADES JUDICIALES\n1/1/16\n2\n1\n1\n8\n2025\n\n\n15\nADOLESCENTE INFRACTOR COIP\nACCION PENAL PUBLICA\nART. 088 AGRESION\nCHIMBORAZO\nRIOBAMBA\nUJ FMNA DE RIOBAMBA\nUNIDADES JUDICIALES\n1/1/17\n1\n1\n1\n8\n2025\n\n\n16\nADOLESCENTE INFRACTOR COIP\nACCION PENAL PUBLICA\nART. 088 AGRESION\nCHIMBORAZO\nRIOBAMBA\nUJ FMNA DE RIOBAMBA\nUNIDADES JUDICIALES\n1/1/19\n4\n1\n4\n8\n2025\n\n\n17\nADOLESCENTE INFRACTOR COIP\nACCION PENAL PUBLICA\nART. 088 AGRESION\nCHIMBORAZO\nRIOBAMBA\nUJ FMNA DE RIOBAMBA\nUNIDADES JUDICIALES\n1/1/24\n2\n0\n6\n8\n2025\n\n\n18\nADOLESCENTE INFRACTOR COIP\nACCION PENAL PUBLICA\nART. 088 AGRESION\nCHIMBORAZO\nRIOBAMBA\nUJ FMNA DE RIOBAMBA\nUNIDADES JUDICIALES\n1/1/25\n1\n0\n7\n8\n2025\n\n\n19\nADOLESCENTE INFRACTOR COIP\nACCION PENAL PUBLICA\nART. 088 AGRESION\nCOTOPAXI\nLATACUNGA\nUJ FMNA DE LATACUNGA\nUNIDADES JUDICIALES\n1/1/17\n1\n1\n0\n8\n2025"
  },
  {
    "objectID": "cnj.html#top-de-delitos-en-el-pais",
    "href": "cnj.html#top-de-delitos-en-el-pais",
    "title": "Visualización de Data",
    "section": "",
    "text": "Code\n# --- Agregacion y Top 10 ---\ntop_delitos = (df.groupby('Delito', as_index=False)['Ingreso']\n                 .sum()\n                 .sort_values('Ingreso', ascending=False)\n                 .head(10))\n\n# (Opcional) ver tabla en consola\nprint(top_delitos.to_string(index=False))\n\n# --- Grafica ---\nbar = alt.Chart(top_delitos).mark_bar().encode(\n    y=alt.Y('Delito:N', sort='-x', title='Delito'),\n    x=alt.X('Ingreso:Q', title='Causas (Ingreso)'),\n    tooltip=['Delito', 'Ingreso']\n).properties(\n    title='Top 10 de causas mas comunes por Delito (segun Ingreso)',\n    width=700,\n    height=350\n).interactive()\n\nlabels = bar.mark_text(align='left', dx=3).encode(text='Ingreso:Q')\n\n(bar + labels)\n\n\n                                                                              Delito  Ingreso\n                                ART. 389 CONTRAVENCIONES DE TRANSITO DE CUARTA CLASE  1116571\n                                                          COBRO DE PAGARE A LA ORDEN   503535\nART. 159 CONTRAVENCIONES DE VIOLENCIA CONTRA LA MUJER O MIEMBROS DEL NUCLEO FAMILIAR   459978\n                                                                           ALIMENTOS   414144\n                               ART. 386 CONTRAVENCIONES DE TRANSITO DE PRIMERA CLASE   227334\n                                                                 DIVORCIO POR CAUSAL   174142\n                         BENEFICIOS Y PETICIONES DE PERSONAS PRIVADAS DE LA LIBERTAD   161231\n                                                                ACCION DE PROTECCION   144420\n                             ART. 385 CONDUCCION DE VEHICULO EN ESTADO DE EMBRIAGUEZ   128014\n          ART. 220 TRAFICO ILICITO DE SUSTANCIAS CATALOGADAS SUJETAS A FISCALIZACION   110272"
  },
  {
    "objectID": "cnj.html#top-10-de-delitos-comunes-en-el-pais",
    "href": "cnj.html#top-10-de-delitos-comunes-en-el-pais",
    "title": "Visualización de Data",
    "section": "",
    "text": "Code\n# --- Agregacion y Top 10 ---\ntop_delitos = (df.groupby('Delito', as_index=False)['Ingreso']\n                 .sum()\n                 .sort_values('Ingreso', ascending=False)\n                 .head(10))\n\n# (Opcional) ver tabla en consola\nprint(top_delitos.to_string(index=False))\n\n# --- Grafica ---\nbar = alt.Chart(top_delitos).mark_bar().encode(\n    y=alt.Y('Delito:N', sort='-x', title='Delito'),\n    x=alt.X('Ingreso:Q', title='Causas (Ingreso)'),\n    tooltip=['Delito', 'Ingreso']\n).properties(\n    title='Top 10 de causas mas comunes por Delito (segun Ingreso)',\n    width=700,\n    height=350\n).interactive()\n\nlabels = bar.mark_text(align='left', dx=3).encode(text='Ingreso:Q')\n\n(bar + labels)\n\n\n                                                                              Delito  Ingreso\n                                ART. 389 CONTRAVENCIONES DE TRANSITO DE CUARTA CLASE  1116571\n                                                          COBRO DE PAGARE A LA ORDEN   503535\nART. 159 CONTRAVENCIONES DE VIOLENCIA CONTRA LA MUJER O MIEMBROS DEL NUCLEO FAMILIAR   459978\n                                                                           ALIMENTOS   414144\n                               ART. 386 CONTRAVENCIONES DE TRANSITO DE PRIMERA CLASE   227334\n                                                                 DIVORCIO POR CAUSAL   174142\n                         BENEFICIOS Y PETICIONES DE PERSONAS PRIVADAS DE LA LIBERTAD   161231\n                                                                ACCION DE PROTECCION   144420\n                             ART. 385 CONDUCCION DE VEHICULO EN ESTADO DE EMBRIAGUEZ   128014\n          ART. 220 TRAFICO ILICITO DE SUSTANCIAS CATALOGADAS SUJETAS A FISCALIZACION   110272"
  },
  {
    "objectID": "cnj.html#top-10-de-delitos-comunes-en-el-pais-1",
    "href": "cnj.html#top-10-de-delitos-comunes-en-el-pais-1",
    "title": "Visualización de Data",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport altair as alt\n\n# Asegurar que Delito este limpio y Ingreso sea numerico\ndf['Delito'] = df['Delito'].fillna('SIN ESPECIFICAR').astype(str).str.strip()\ndf['Ingreso'] = pd.to_numeric(df['Ingreso'], errors='coerce').fillna(0)\n\n# Agrupamos por Delito y sacamos el top 10 con menos casos\nbottom_delitos = (df.groupby('Delito', as_index=False)['Ingreso']\n                    .sum()\n                    .sort_values('Ingreso', ascending=True)\n                    .head(10))\n\n# (Opcional) tabla en consola\nprint(bottom_delitos.to_string(index=True))\n\n# Grafica de barras horizontales\nbar = alt.Chart(bottom_delitos).mark_bar().encode(\n    y=alt.Y('Delito:N', sort='x', title='Delito'),\n    x=alt.X('Ingreso:Q', title='Causas (Ingreso)'),\n    tooltip=['Delito','Ingreso']\n).properties(\n    title='Top 10 de causas menos comunes por Delito (segun Ingreso)',\n    width=700,\n    height=350\n)\n\nlabels = bar.mark_text(align='left', dx=3).encode(text='Ingreso:Q')\n\n(bar + labels)\n\n\n                                                                                      Delito  Ingreso\n84                                                       ART. 164 INSEMINACION NO CONSENTIDA        1\n40                     ART. 098 REALIZACION DE PROCEDIMIENTOS DE TRASPLANTE SIN AUTORIZACION        1\n39                                                               ART. 096 TRAFICO DE ORGANOS        1\n344  CONTRA REGISTRADOR DE LA PROPIEDAD ANTE NEGATIVA DE INSCRIPCION POR RAZONES TRIBUTARIAS        1\n56                                       ART. 125 PRIVACION DE LIBERTAD DE PERSONA PROTEGIDA        1\n34                                                             ART. 072 EXTINCION DE LA PENA        1\n293                                                                      ART. 86 PERSECUCION        1\n58                           ART. 134 OMISION DE MEDIDAS DE SOCORRO Y ASISTENCIA HUMANITARIA        1\n373                                                                     DESPIDO INTEMPESTIVO        1\n341                                              CONTRA NEGATIVA DE PETICION DE COMPENSACION        1"
  },
  {
    "objectID": "cnj.html#top-10-de-delitos-menos-comunes-en-el-pais",
    "href": "cnj.html#top-10-de-delitos-menos-comunes-en-el-pais",
    "title": "Visualización de Data",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport altair as alt\n\n# Asegurar que Delito este limpio y Ingreso sea numerico\ndf['Delito'] = df['Delito'].fillna('SIN ESPECIFICAR').astype(str).str.strip()\ndf['Ingreso'] = pd.to_numeric(df['Ingreso'], errors='coerce').fillna(0)\n\n# Agrupamos por Delito y sacamos el top 10 con menos casos\nbottom_delitos = (df.groupby('Delito', as_index=False)['Ingreso']\n                    .sum()\n                    .sort_values('Ingreso', ascending=True)\n                    .head(10))\n\n# (Opcional) tabla en consola\nprint(bottom_delitos.to_string(index=True))\n\n# Grafica de barras horizontales\nbar = alt.Chart(bottom_delitos).mark_bar().encode(\n    y=alt.Y('Delito:N', sort='x', title='Delito'),\n    x=alt.X('Ingreso:Q', title='Causas (Ingreso)'),\n    tooltip=['Delito','Ingreso']\n).properties(\n    title='Top 10 de causas menos comunes por Delito (segun Ingreso)',\n    width=700,\n    height=350\n)\n\nlabels = bar.mark_text(align='left', dx=3).encode(text='Ingreso:Q')\n\n(bar + labels)\n\n\n                                                                                      Delito  Ingreso\n84                                                       ART. 164 INSEMINACION NO CONSENTIDA        1\n40                     ART. 098 REALIZACION DE PROCEDIMIENTOS DE TRASPLANTE SIN AUTORIZACION        1\n39                                                               ART. 096 TRAFICO DE ORGANOS        1\n344  CONTRA REGISTRADOR DE LA PROPIEDAD ANTE NEGATIVA DE INSCRIPCION POR RAZONES TRIBUTARIAS        1\n56                                       ART. 125 PRIVACION DE LIBERTAD DE PERSONA PROTEGIDA        1\n34                                                             ART. 072 EXTINCION DE LA PENA        1\n293                                                                      ART. 86 PERSECUCION        1\n58                           ART. 134 OMISION DE MEDIDAS DE SOCORRO Y ASISTENCIA HUMANITARIA        1\n373                                                                     DESPIDO INTEMPESTIVO        1\n341                                              CONTRA NEGATIVA DE PETICION DE COMPENSACION        1"
  },
  {
    "objectID": "cnj.html#top-de-delitos-comunes-en-el-pais",
    "href": "cnj.html#top-de-delitos-comunes-en-el-pais",
    "title": "Visualización de Data",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport altair as alt\n\n# Asegurar que Delito este limpio y Ingreso sea numerico\ndf['Delito'] = df['Delito'].fillna('SIN ESPECIFICAR').astype(str).str.strip()\ndf['Ingreso'] = pd.to_numeric(df['Ingreso'], errors='coerce').fillna(0)\n\n# Agrupamos por Delito y sacamos el top 10 con menos casos\nbottom_delitos = (df.groupby('Delito', as_index=False)['Ingreso']\n                    .sum()\n                    .sort_values('Ingreso', ascending=False)\n                    .head(10))\n\n# (Opcional) tabla en consola\nprint(bottom_delitos.to_string(index=True))\n\n# Grafica de barras horizontales\nbar = alt.Chart(bottom_delitos).mark_bar().encode(\n    y=alt.Y('Delito:N', sort='x', title='Delito'),\n    x=alt.X('Ingreso:Q', title='Causas (Ingreso)'),\n    tooltip=['Delito','Ingreso']\n).properties(\n    title='Top 10 de causas menos comunes por Delito (segun Ingreso)',\n    width=700,\n    height=350\n)\n\nlabels = bar.mark_text(align='left', dx=3).encode(text='Ingreso:Q')\n\n(bar + labels)\n\n\n                                                                                   Delito  Ingreso\n284                                  ART. 389 CONTRAVENCIONES DE TRANSITO DE CUARTA CLASE  1116571\n327                                                            COBRO DE PAGARE A LA ORDEN   503535\n79   ART. 159 CONTRAVENCIONES DE VIOLENCIA CONTRA LA MUJER O MIEMBROS DEL NUCLEO FAMILIAR   459978\n20                                                                              ALIMENTOS   414144\n281                                 ART. 386 CONTRAVENCIONES DE TRANSITO DE PRIMERA CLASE   227334\n383                                                                   DIVORCIO POR CAUSAL   174142\n310                           BENEFICIOS Y PETICIONES DE PERSONAS PRIVADAS DE LA LIBERTAD   161231\n4                                                                    ACCION DE PROTECCION   144420\n280                               ART. 385 CONDUCCION DE VEHICULO EN ESTADO DE EMBRIAGUEZ   128014\n140            ART. 220 TRAFICO ILICITO DE SUSTANCIAS CATALOGADAS SUJETAS A FISCALIZACION   110272"
  },
  {
    "objectID": "cnj.html#top-10-de-las-provincias-mas-violentas",
    "href": "cnj.html#top-10-de-las-provincias-mas-violentas",
    "title": "Visualización de Data",
    "section": "",
    "text": "Code\n# Asegurar limpieza de texto\ndf['Delito'] = df['Delito'].fillna('SIN ESPECIFICAR').astype(str).str.strip()\ndf['Provincia'] = df['Provincia'].astype(str).str.strip()\ndf['Ingreso'] = pd.to_numeric(df['Ingreso'], errors='coerce').fillna(0)\n\n# Filtrar solo el delito 140 ASESINATO\ndf_asesinato = df[df['Delito'].str.upper().str.contains(\"140 ASESINATO\")]\n\n# Agrupar por provincia y ordenar\ntop5_provincias = (df_asesinato.groupby('Provincia', as_index=False)['Ingreso']\n                   .sum()\n                   .sort_values('Ingreso', ascending=False)\n                   .head(5))\n\nprint(top5_provincias.to_string(index=False))  # tabla en consola\n\n# Gráfica\nchart = alt.Chart(top5_provincias).mark_bar().encode(\n    x=alt.X('Ingreso:Q', title='Causas (Ingreso)'),\n    y=alt.Y('Provincia:N', sort='-x', title='Provincia'),\n    tooltip=['Provincia','Ingreso']\n).properties(\n    title='Top 5 provincias con mas causas por delito: 140 ASESINATO',\n    width=600,\n    height=300\n).interactive()\n\nlabels = chart.mark_text(align='left', dx=3).encode(text='Ingreso:Q')\n\n(chart + labels)\n\n\n Provincia  Ingreso\n    GUAYAS     5399\n PICHINCHA     1699\nESMERALDAS     1581\n    MANABI     1448\n  LOS RIOS     1226\n\n\n\n\n\n\n\n\n\n\nCode\n# Asegurar limpieza de texto\ndf['Delito'] = df['Delito'].fillna('SIN ESPECIFICAR').astype(str).str.strip()\ndf['Provincia'] = df['Provincia'].astype(str).str.strip()\ndf['Ingreso'] = pd.to_numeric(df['Ingreso'], errors='coerce').fillna(0)\n\n# Filtrar solo el delito 140 ASESINATO\ndf_asesinato = df[df['Delito'].str.upper().str.contains(\"140 ASESINATO\")]\n\n# Identificar las top 5 provincias con más casos acumulados\ntop5_provincias = (df_asesinato.groupby('Provincia', as_index=False)['Ingreso']\n                   .sum()\n                   .sort_values('Ingreso', ascending=False)\n                   .head(5)['Provincia']\n                   .tolist())\n\n# Filtrar solo esas provincias\ndf_top5 = df_asesinato[df_asesinato['Provincia'].isin(top5_provincias)]\n\n# Agrupar por año y provincia\nevolucion = (df_top5.groupby(['Anio','Provincia'], as_index=False)['Ingreso']\n             .sum()\n             .sort_values(['Provincia','Anio']))\n\nprint(evolucion.to_string(index=False))  # Tabla en consola\n\n# Gráfica de evolución\nchart = alt.Chart(evolucion).mark_line(point=True).encode(\n    x=alt.X('Anio:O', title='Año'),\n    y=alt.Y('Ingreso:Q', title='Causas (Ingreso)'),\n    color=alt.Color('Provincia:N', title='Provincia'),\n    tooltip=['Anio','Provincia','Ingreso']\n).properties(\n    title='Evolución por año — Delito 140 ASESINATO (Top 5 provincias)',\n    width=500,\n    height=400\n).interactive()\n\nchart\n\n\n Anio  Provincia  Ingreso\n 2025 ESMERALDAS     1581\n 2025     GUAYAS     5399\n 2025   LOS RIOS     1226\n 2025     MANABI     1448\n 2025  PICHINCHA     1699\n\n\n\n\n\n\n\n\n\n\nCode\ndf = df.copy()\nfor c in ['Ingreso','Resuelta','Tramite']:\n    df[c] = pd.to_numeric(df[c], errors='coerce').fillna(0)\ndf['Provincia'] = df['Provincia'].astype(str).str.strip()\n\n# Provincias con mayor Resuelta (para ordenar/filtrar)\ntopN = 5\norden_prov = (df.groupby('Provincia', as_index=False)['Resuelta']\n                .sum()\n                .sort_values('Resuelta', ascending=False)\n                .head(topN)['Provincia']\n                .tolist())\n\n# Subset y pasar a formato largo\nsubset = (df[df['Provincia'].isin(orden_prov)]\n          .groupby('Provincia', as_index=False)[['Ingreso','Resuelta','Tramite']]\n          .sum())\n\ndatos_long = subset.melt('Provincia', var_name='Estado', value_name='Causas')\n\n# --- barras agrupadas (xOffset) ---\nchart = alt.Chart(datos_long).mark_line(point=True).encode(\n    x=alt.X('Provincia:N', sort=orden_prov, title='Provincia'),\n    y=alt.Y('Causas:Q', title='Número de causas'),\n    color=alt.Color('Estado:N', title='Estado'),\n    xOffset='Estado:N',  # &lt;--- Agrupa barras por Estado dentro de cada provincia\n    tooltip=['Provincia','Estado','Causas']\n).properties(\n    title=f'Provincias vs número de causas (Ingreso / Resuelta / Tramite) — Top {topN}',\n    width=600,\n    height=450\n)\n\nchart"
  },
  {
    "objectID": "classificaction.html",
    "href": "classificaction.html",
    "title": "Classification",
    "section": "",
    "text": "1. Importar Librerias\n\n\nCode\n# 1. Import libraries\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split as tts\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay\n\n\n\n\n2. Cargar el Dataset\n\n\nCode\ndf = load_breast_cancer()\nX= df.data\ny= df.target\n\nX\ny\n\n\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n       0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n       1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n       0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n       1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1])\n\n\n\n\n3. Split de la data\n\n\nCode\nX_train, X_test, y_train, y_test = tts(\n    X,y,test_size=0.2, random_state=42,stratify=y\n)\nprint(\"X_train\")\nprint(X_train)\nprint(\"X_test\")\nprint(X_test)\n\n\nprint(\"Y_train\")\nprint(y_train)\nprint(\"y_test\")\nprint(y_test)\n\n\nX_train\n[[1.032e+01 1.635e+01 6.531e+01 ... 2.381e-02 2.681e-01 7.399e-02]\n [2.018e+01 1.954e+01 1.338e+02 ... 2.173e-01 3.032e-01 8.075e-02]\n [1.066e+01 1.515e+01 6.749e+01 ... 0.000e+00 2.710e-01 6.164e-02]\n ...\n [1.546e+01 2.395e+01 1.038e+02 ... 2.163e-01 3.013e-01 1.067e-01]\n [1.705e+01 1.908e+01 1.134e+02 ... 2.543e-01 3.109e-01 9.061e-02]\n [1.088e+01 1.562e+01 7.041e+01 ... 7.966e-02 2.581e-01 1.080e-01]]\nX_test\n[[1.955e+01 2.877e+01 1.336e+02 ... 1.941e-01 2.818e-01 1.005e-01]\n [1.113e+01 1.662e+01 7.047e+01 ... 4.044e-02 2.383e-01 7.083e-02]\n [1.382e+01 2.449e+01 9.233e+01 ... 1.521e-01 3.651e-01 1.183e-01]\n ...\n [1.532e+01 1.727e+01 1.032e+02 ... 2.229e-01 3.258e-01 1.191e-01]\n [1.262e+01 2.397e+01 8.135e+01 ... 1.180e-01 2.826e-01 9.585e-02]\n [1.168e+01 1.617e+01 7.549e+01 ... 9.815e-02 2.804e-01 8.024e-02]]\nY_train\n[1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 0 1 1 1 0 1 1 1 0 1 1 0 0 0\n 1 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 0\n 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 1 1 0 1 1 1\n 0 0 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 1 0\n 1 0 1 1 1 0 0 1 1 0 1 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1\n 0 1 1 0 1 0 1 1 1 0 1 1 0 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1\n 1 1 1 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0 0 1 1 1 0\n 0 0 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 0 1 1 1 0\n 0 0 0 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0\n 1 0 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 0 0 1 1 1\n 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1\n 1 1 0 1 1 1 0 1 0 0 0 1 1 1 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 1 1 1 0 0 0 1 1\n 1 1 0 1 0 0 0 0 0 0 1]\ny_test\n[0 1 0 1 0 1 1 0 0 0 1 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1\n 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0\n 0 0 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1\n 0 1 1]\n\n\n\n\n4. Entrenar el modelo\n\n\nCode\nmodel = LogisticRegression(max_iter=1000000)\nmodel.fit(X_train,y_train)\n\n\nLogisticRegression(max_iter=1000000)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegression?Documentation for LogisticRegressioniFitted\n        \n            \n                Parameters\n                \n\n\n\n\npenalty \n'l2'\n\n\n\ndual \nFalse\n\n\n\ntol \n0.0001\n\n\n\nC \n1.0\n\n\n\nfit_intercept \nTrue\n\n\n\nintercept_scaling \n1\n\n\n\nclass_weight \nNone\n\n\n\nrandom_state \nNone\n\n\n\nsolver \n'lbfgs'\n\n\n\nmax_iter \n1000000\n\n\n\nmulti_class \n'deprecated'\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nl1_ratio \nNone\n\n\n\n\n            \n        \n    \n\n\n\n\n5. Predicción\n\n\nCode\ny_pred= model.predict(X_test)\n\n\n\n\n6. Evaluación\n\n\nCode\naccuracy= accuracy_score(y_test,y_pred)\nprecision=precision_score(y_test,y_pred)\nrecall=recall_score(y_test,y_pred)\nf1=f1_score(y_test,y_pred)\n\nprint(f\"Accuracy : \",accuracy)\nprint(\"precision :\",precision)\nprint(\"recall :\",recall)\nprint(\"f1 :\", f1)\n\n\nAccuracy :  0.9649122807017544\nprecision : 0.9594594594594594\nrecall : 0.9861111111111112\nf1 : 0.9726027397260274"
  },
  {
    "objectID": "classification.html",
    "href": "classification.html",
    "title": "Classification",
    "section": "",
    "text": "Code\n# 1. Import libraries\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split as tts\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay\n\n\n\n\n\n\n\nCode\ndf = load_breast_cancer()\nX= df.data\ny= df.target\n\nX\ny\n\n\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n       0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n       1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n       0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n       1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1])\n\n\n\n\n\n\n\nCode\nX_train, X_test, y_train, y_test = tts(\n    X,y,test_size=0.2, random_state=42,stratify=y\n)\nprint(\"X_train\")\nprint(X_train)\nprint(\"X_test\")\nprint(X_test)\n\n\nprint(\"Y_train\")\nprint(y_train)\nprint(\"y_test\")\nprint(y_test)\n\n\nX_train\n[[1.032e+01 1.635e+01 6.531e+01 ... 2.381e-02 2.681e-01 7.399e-02]\n [2.018e+01 1.954e+01 1.338e+02 ... 2.173e-01 3.032e-01 8.075e-02]\n [1.066e+01 1.515e+01 6.749e+01 ... 0.000e+00 2.710e-01 6.164e-02]\n ...\n [1.546e+01 2.395e+01 1.038e+02 ... 2.163e-01 3.013e-01 1.067e-01]\n [1.705e+01 1.908e+01 1.134e+02 ... 2.543e-01 3.109e-01 9.061e-02]\n [1.088e+01 1.562e+01 7.041e+01 ... 7.966e-02 2.581e-01 1.080e-01]]\nX_test\n[[1.955e+01 2.877e+01 1.336e+02 ... 1.941e-01 2.818e-01 1.005e-01]\n [1.113e+01 1.662e+01 7.047e+01 ... 4.044e-02 2.383e-01 7.083e-02]\n [1.382e+01 2.449e+01 9.233e+01 ... 1.521e-01 3.651e-01 1.183e-01]\n ...\n [1.532e+01 1.727e+01 1.032e+02 ... 2.229e-01 3.258e-01 1.191e-01]\n [1.262e+01 2.397e+01 8.135e+01 ... 1.180e-01 2.826e-01 9.585e-02]\n [1.168e+01 1.617e+01 7.549e+01 ... 9.815e-02 2.804e-01 8.024e-02]]\nY_train\n[1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 0 1 1 1 0 1 1 1 0 1 1 0 0 0\n 1 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 0\n 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 1 1 0 1 1 1\n 0 0 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 1 0\n 1 0 1 1 1 0 0 1 1 0 1 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1\n 0 1 1 0 1 0 1 1 1 0 1 1 0 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1\n 1 1 1 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0 0 1 1 1 0\n 0 0 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 0 1 1 1 0\n 0 0 0 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0\n 1 0 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 0 0 1 1 1\n 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1\n 1 1 0 1 1 1 0 1 0 0 0 1 1 1 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 1 1 1 0 0 0 1 1\n 1 1 0 1 0 0 0 0 0 0 1]\ny_test\n[0 1 0 1 0 1 1 0 0 0 1 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1\n 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0\n 0 0 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1\n 0 1 1]\n\n\n\n\n\n\n\nCode\nmodel = LogisticRegression(max_iter=1000000)\nmodel.fit(X_train,y_train)\n\n\nLogisticRegression(max_iter=1000000)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegression?Documentation for LogisticRegressioniFitted\n        \n            \n                Parameters\n                \n\n\n\n\npenalty \n'l2'\n\n\n\ndual \nFalse\n\n\n\ntol \n0.0001\n\n\n\nC \n1.0\n\n\n\nfit_intercept \nTrue\n\n\n\nintercept_scaling \n1\n\n\n\nclass_weight \nNone\n\n\n\nrandom_state \nNone\n\n\n\nsolver \n'lbfgs'\n\n\n\nmax_iter \n1000000\n\n\n\nmulti_class \n'deprecated'\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nl1_ratio \nNone\n\n\n\n\n            \n        \n    \n\n\n\n\n\n\n\nCode\ny_pred= model.predict(X_test)\n\n\n\n\n\n\n\nCode\naccuracy= accuracy_score(y_test,y_pred)\nprecision=precision_score(y_test,y_pred)\nrecall=recall_score(y_test,y_pred)\nf1=f1_score(y_test,y_pred)\n\nprint(f\"Accuracy : \",accuracy)\nprint(\"precision :\",precision)\nprint(\"recall :\",recall)\nprint(\"f1 :\", f1)\n\n\nAccuracy :  0.9649122807017544\nprecision : 0.9594594594594594\nrecall : 0.9861111111111112\nf1 : 0.9726027397260274\n\n\n\n\n\n\n\nCode\nConfusionMatrixDisplay.from_predictions(y_test,y_pred)",
    "crumbs": [
      "Home",
      "Classification",
      "Overview"
    ]
  },
  {
    "objectID": "classification.html#importar-librerias",
    "href": "classification.html#importar-librerias",
    "title": "Classification",
    "section": "",
    "text": "Code\n# 1. Import libraries\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split as tts\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay",
    "crumbs": [
      "Home",
      "Classification",
      "Overview"
    ]
  },
  {
    "objectID": "classification.html#cargar-el-dataset",
    "href": "classification.html#cargar-el-dataset",
    "title": "Classification",
    "section": "",
    "text": "Code\ndf = load_breast_cancer()\nX= df.data\ny= df.target\n\nX\ny\n\n\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n       0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n       1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n       0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n       1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1])",
    "crumbs": [
      "Home",
      "Classification",
      "Overview"
    ]
  },
  {
    "objectID": "classification.html#split-de-la-data",
    "href": "classification.html#split-de-la-data",
    "title": "Classification",
    "section": "",
    "text": "Code\nX_train, X_test, y_train, y_test = tts(\n    X,y,test_size=0.2, random_state=42,stratify=y\n)\nprint(\"X_train\")\nprint(X_train)\nprint(\"X_test\")\nprint(X_test)\n\n\nprint(\"Y_train\")\nprint(y_train)\nprint(\"y_test\")\nprint(y_test)\n\n\nX_train\n[[1.032e+01 1.635e+01 6.531e+01 ... 2.381e-02 2.681e-01 7.399e-02]\n [2.018e+01 1.954e+01 1.338e+02 ... 2.173e-01 3.032e-01 8.075e-02]\n [1.066e+01 1.515e+01 6.749e+01 ... 0.000e+00 2.710e-01 6.164e-02]\n ...\n [1.546e+01 2.395e+01 1.038e+02 ... 2.163e-01 3.013e-01 1.067e-01]\n [1.705e+01 1.908e+01 1.134e+02 ... 2.543e-01 3.109e-01 9.061e-02]\n [1.088e+01 1.562e+01 7.041e+01 ... 7.966e-02 2.581e-01 1.080e-01]]\nX_test\n[[1.955e+01 2.877e+01 1.336e+02 ... 1.941e-01 2.818e-01 1.005e-01]\n [1.113e+01 1.662e+01 7.047e+01 ... 4.044e-02 2.383e-01 7.083e-02]\n [1.382e+01 2.449e+01 9.233e+01 ... 1.521e-01 3.651e-01 1.183e-01]\n ...\n [1.532e+01 1.727e+01 1.032e+02 ... 2.229e-01 3.258e-01 1.191e-01]\n [1.262e+01 2.397e+01 8.135e+01 ... 1.180e-01 2.826e-01 9.585e-02]\n [1.168e+01 1.617e+01 7.549e+01 ... 9.815e-02 2.804e-01 8.024e-02]]\nY_train\n[1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 0 1 1 1 0 1 1 1 0 1 1 0 0 0\n 1 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 0\n 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 1 1 0 1 1 1\n 0 0 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 1 0\n 1 0 1 1 1 0 0 1 1 0 1 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1\n 0 1 1 0 1 0 1 1 1 0 1 1 0 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1\n 1 1 1 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0 0 1 1 1 0\n 0 0 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 0 1 1 1 0\n 0 0 0 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0\n 1 0 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 0 0 1 1 1\n 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1\n 1 1 0 1 1 1 0 1 0 0 0 1 1 1 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 1 1 1 0 0 0 1 1\n 1 1 0 1 0 0 0 0 0 0 1]\ny_test\n[0 1 0 1 0 1 1 0 0 0 1 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1\n 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0\n 0 0 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1\n 0 1 1]",
    "crumbs": [
      "Home",
      "Classification",
      "Overview"
    ]
  },
  {
    "objectID": "classification.html#entrenar-el-modelo",
    "href": "classification.html#entrenar-el-modelo",
    "title": "Classification",
    "section": "",
    "text": "Code\nmodel = LogisticRegression(max_iter=1000000)\nmodel.fit(X_train,y_train)\n\n\nLogisticRegression(max_iter=1000000)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegression?Documentation for LogisticRegressioniFitted\n        \n            \n                Parameters\n                \n\n\n\n\npenalty \n'l2'\n\n\n\ndual \nFalse\n\n\n\ntol \n0.0001\n\n\n\nC \n1.0\n\n\n\nfit_intercept \nTrue\n\n\n\nintercept_scaling \n1\n\n\n\nclass_weight \nNone\n\n\n\nrandom_state \nNone\n\n\n\nsolver \n'lbfgs'\n\n\n\nmax_iter \n1000000\n\n\n\nmulti_class \n'deprecated'\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nl1_ratio \nNone",
    "crumbs": [
      "Home",
      "Classification",
      "Overview"
    ]
  },
  {
    "objectID": "classification.html#predicción",
    "href": "classification.html#predicción",
    "title": "Classification",
    "section": "",
    "text": "Code\ny_pred= model.predict(X_test)",
    "crumbs": [
      "Home",
      "Classification",
      "Overview"
    ]
  },
  {
    "objectID": "classification.html#evaluación",
    "href": "classification.html#evaluación",
    "title": "Classification",
    "section": "",
    "text": "Code\naccuracy= accuracy_score(y_test,y_pred)\nprecision=precision_score(y_test,y_pred)\nrecall=recall_score(y_test,y_pred)\nf1=f1_score(y_test,y_pred)\n\nprint(f\"Accuracy : \",accuracy)\nprint(\"precision :\",precision)\nprint(\"recall :\",recall)\nprint(\"f1 :\", f1)\n\n\nAccuracy :  0.9649122807017544\nprecision : 0.9594594594594594\nrecall : 0.9861111111111112\nf1 : 0.9726027397260274",
    "crumbs": [
      "Home",
      "Classification",
      "Overview"
    ]
  },
  {
    "objectID": "classification.html#matriz-de-confusion",
    "href": "classification.html#matriz-de-confusion",
    "title": "Classification",
    "section": "",
    "text": "Code\nConfusionMatrixDisplay.from_predictions(y_test,y_pred)",
    "crumbs": [
      "Home",
      "Classification",
      "Overview"
    ]
  },
  {
    "objectID": "classification.html#cargar-el-dataset-1",
    "href": "classification.html#cargar-el-dataset-1",
    "title": "Classification",
    "section": "2.2 Cargar el Dataset",
    "text": "2.2 Cargar el Dataset\n\n\nCode\ndf = load_breast_cancer()\nX= df.data\ny= df.target\n\nX\ny\n\n\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n       0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n       1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n       0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n       1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1])",
    "crumbs": [
      "Home",
      "Classification",
      "Overview"
    ]
  },
  {
    "objectID": "classification.html#split-de-la-data-1",
    "href": "classification.html#split-de-la-data-1",
    "title": "Classification",
    "section": "2.3 Split de la data",
    "text": "2.3 Split de la data\n\n\nCode\nX_train, X_test, y_train, y_test = tts(\n    X,y,test_size=0.2, random_state=42,stratify=y\n)\nprint(\"X_train\")\nprint(X_train)\nprint(\"X_test\")\nprint(X_test)\n\n\nprint(\"Y_train\")\nprint(y_train)\nprint(\"y_test\")\nprint(y_test)\n\n\nX_train\n[[1.032e+01 1.635e+01 6.531e+01 ... 2.381e-02 2.681e-01 7.399e-02]\n [2.018e+01 1.954e+01 1.338e+02 ... 2.173e-01 3.032e-01 8.075e-02]\n [1.066e+01 1.515e+01 6.749e+01 ... 0.000e+00 2.710e-01 6.164e-02]\n ...\n [1.546e+01 2.395e+01 1.038e+02 ... 2.163e-01 3.013e-01 1.067e-01]\n [1.705e+01 1.908e+01 1.134e+02 ... 2.543e-01 3.109e-01 9.061e-02]\n [1.088e+01 1.562e+01 7.041e+01 ... 7.966e-02 2.581e-01 1.080e-01]]\nX_test\n[[1.955e+01 2.877e+01 1.336e+02 ... 1.941e-01 2.818e-01 1.005e-01]\n [1.113e+01 1.662e+01 7.047e+01 ... 4.044e-02 2.383e-01 7.083e-02]\n [1.382e+01 2.449e+01 9.233e+01 ... 1.521e-01 3.651e-01 1.183e-01]\n ...\n [1.532e+01 1.727e+01 1.032e+02 ... 2.229e-01 3.258e-01 1.191e-01]\n [1.262e+01 2.397e+01 8.135e+01 ... 1.180e-01 2.826e-01 9.585e-02]\n [1.168e+01 1.617e+01 7.549e+01 ... 9.815e-02 2.804e-01 8.024e-02]]\nY_train\n[1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 0 1 1 1 0 1 1 1 0 1 1 0 0 0\n 1 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 0\n 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 1 1 0 1 1 1\n 0 0 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 1 0\n 1 0 1 1 1 0 0 1 1 0 1 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1\n 0 1 1 0 1 0 1 1 1 0 1 1 0 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1\n 1 1 1 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0 0 1 1 1 0\n 0 0 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 0 1 1 1 0\n 0 0 0 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0\n 1 0 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 0 0 1 1 1\n 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1\n 1 1 0 1 1 1 0 1 0 0 0 1 1 1 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 1 1 1 0 0 0 1 1\n 1 1 0 1 0 0 0 0 0 0 1]\ny_test\n[0 1 0 1 0 1 1 0 0 0 1 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1\n 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0\n 0 0 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1\n 0 1 1]",
    "crumbs": [
      "Home",
      "Classification",
      "Overview"
    ]
  },
  {
    "objectID": "classification.html#entrenar-el-modelo-1",
    "href": "classification.html#entrenar-el-modelo-1",
    "title": "Classification",
    "section": "2.5 Entrenar el modelo",
    "text": "2.5 Entrenar el modelo\n\n\nCode\npipe.fit(X_train,y_train)\n\n\nPipeline(steps=[('escalado', StandardScaler()),\n                ('logreg', LogisticRegression(max_iter=1000000))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps \n[('escalado', ...), ('logreg', ...)]\n\n\n\ntransform_input \nNone\n\n\n\nmemory \nNone\n\n\n\nverbose \nFalse\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    LogisticRegression?Documentation for LogisticRegression\n        \n            \n                Parameters\n                \n\n\n\n\npenalty \n'l2'\n\n\n\ndual \nFalse\n\n\n\ntol \n0.0001\n\n\n\nC \n1.0\n\n\n\nfit_intercept \nTrue\n\n\n\nintercept_scaling \n1\n\n\n\nclass_weight \nNone\n\n\n\nrandom_state \nNone\n\n\n\nsolver \n'lbfgs'\n\n\n\nmax_iter \n1000000\n\n\n\nmulti_class \n'deprecated'\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nl1_ratio \nNone",
    "crumbs": [
      "Home",
      "Classification",
      "Overview"
    ]
  },
  {
    "objectID": "classification.html#predicción-1",
    "href": "classification.html#predicción-1",
    "title": "Classification",
    "section": "2.6 Predicción",
    "text": "2.6 Predicción\n\n\nCode\ny_pred_pipe= pipe.predict(X_test)",
    "crumbs": [
      "Home",
      "Classification",
      "Overview"
    ]
  },
  {
    "objectID": "classification.html#evaluación-1",
    "href": "classification.html#evaluación-1",
    "title": "Classification",
    "section": "2.7 Evaluación",
    "text": "2.7 Evaluación\n\n\nCode\naccuracy= accuracy_score(y_test,y_pred_pipe)\nprecision=precision_score(y_test,y_pred_pipe)\nrecall=recall_score(y_test,y_pred_pipe)\nf1=f1_score(y_test,y_pred_pipe)\n\nprint(f\"Accuracy : \",accuracy)\nprint(\"precision :\",precision)\nprint(\"recall :\",recall)\nprint(\"f1 :\", f1)\n\n\nAccuracy :  0.9824561403508771\nprecision : 0.9861111111111112\nrecall : 0.9861111111111112\nf1 : 0.9861111111111112",
    "crumbs": [
      "Home",
      "Classification",
      "Overview"
    ]
  },
  {
    "objectID": "classification.html#matriz-de-confusion-1",
    "href": "classification.html#matriz-de-confusion-1",
    "title": "Classification",
    "section": "2.8 Matriz de confusion",
    "text": "2.8 Matriz de confusion\n\n\nCode\nConfusionMatrixDisplay.from_predictions(y_test,y_pred_pipe)",
    "crumbs": [
      "Home",
      "Classification",
      "Overview"
    ]
  },
  {
    "objectID": "classification.html#importar-librerias-1",
    "href": "classification.html#importar-librerias-1",
    "title": "Classification",
    "section": "2.1 1. Importar Librerias",
    "text": "2.1 1. Importar Librerias\n\n\nCode\n# 1. Import libraries\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler",
    "crumbs": [
      "Home",
      "Classification",
      "Overview"
    ]
  },
  {
    "objectID": "classification.html#pipeline",
    "href": "classification.html#pipeline",
    "title": "Classification",
    "section": "2.4 Pipeline",
    "text": "2.4 Pipeline\n\n\nCode\npipe = Pipeline([\n    (\"escalado\", StandardScaler()),\n    (\"logreg\",LogisticRegression(max_iter=1000000))\n])",
    "crumbs": [
      "Home",
      "Classification",
      "Overview"
    ]
  },
  {
    "objectID": "midterm.html",
    "href": "midterm.html",
    "title": "Classification",
    "section": "",
    "text": "1 📌 Introducción\nEl dataset “Datos de recaudaciones de tributos de comercio exterior” proviene del Servicio Nacional de Aduana del Ecuador (SENAE) y está disponible en el portal oficial de Datos Abiertos Ecuador.\nEste conjunto de datos recoge información detallada sobre los tributos generados por operaciones de comercio exterior en el Ecuador, con valores de recaudación asociados a diferentes tributos, distritos aduaneros y canales de pago.\n\n\n\n2 📊 Variables principales\nAlgunas columnas clave del dataset son:\n\nNOM_DISTRITO: Nombre del distrito aduanero donde se registró la operación.\n\nNOM_TRIBUTO: Tipo de tributo aplicado (ej. IVA, Arancel Advalorem, Salvaguardias).\n\nCANAL_PAGO: Medio por el cual se realizó el pago (internet, ventanilla, tarjeta, etc.).\n\nVALOR_TOTAL_TRIBUTO_EN_NC: Valor pagado con notas de crédito.\n\nVALOR_TOTAL_TRIBUTO_EN_EFECTIVO: Valor pagado en efectivo.\n\nVALOR_TOTAL_TRIBUTO_PAGADO: Valor total del tributo (suma de los anteriores).\n\n\n\n\n3 🎯 Propósito del análisis\nPara el ejercicio de regresión, nos planteamos el siguiente problema:\n\nPredecir el valor total de tributos pagados (VALOR_TOTAL_TRIBUTO_PAGADO) en función del canal de pago, el distrito, el tipo de tributo y los valores parciales (nota de crédito, efectivo).\n\nEste análisis nos permitirá:\n\nIdentificar qué factores influyen más en la recaudación.\n\nEvaluar la capacidad predictiva de un modelo de regresión en este contexto.\n\nDemostrar un flujo completo de Machine Learning supervisado con Scikit-learn Pipeline (preprocesamiento, entrenamiento, métricas y visualizaciones)."
  },
  {
    "objectID": "midterm.html#importar-librerias",
    "href": "midterm.html#importar-librerias",
    "title": "Classification",
    "section": "",
    "text": "Code\n# 1. Import libraries\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split as tts\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay"
  },
  {
    "objectID": "midterm.html#cargar-el-dataset",
    "href": "midterm.html#cargar-el-dataset",
    "title": "Classification",
    "section": "",
    "text": "Code\ndf = load_breast_cancer()\nX= df.data\ny= df.target\n\nX\ny\n\n\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n       0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n       1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n       0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n       1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1])"
  },
  {
    "objectID": "midterm.html#split-de-la-data",
    "href": "midterm.html#split-de-la-data",
    "title": "Classification",
    "section": "",
    "text": "Code\nX_train, X_test, y_train, y_test = tts(\n    X,y,test_size=0.2, random_state=42,stratify=y\n)\nprint(\"X_train\")\nprint(X_train)\nprint(\"X_test\")\nprint(X_test)\n\n\nprint(\"Y_train\")\nprint(y_train)\nprint(\"y_test\")\nprint(y_test)\n\n\nX_train\n[[1.032e+01 1.635e+01 6.531e+01 ... 2.381e-02 2.681e-01 7.399e-02]\n [2.018e+01 1.954e+01 1.338e+02 ... 2.173e-01 3.032e-01 8.075e-02]\n [1.066e+01 1.515e+01 6.749e+01 ... 0.000e+00 2.710e-01 6.164e-02]\n ...\n [1.546e+01 2.395e+01 1.038e+02 ... 2.163e-01 3.013e-01 1.067e-01]\n [1.705e+01 1.908e+01 1.134e+02 ... 2.543e-01 3.109e-01 9.061e-02]\n [1.088e+01 1.562e+01 7.041e+01 ... 7.966e-02 2.581e-01 1.080e-01]]\nX_test\n[[1.955e+01 2.877e+01 1.336e+02 ... 1.941e-01 2.818e-01 1.005e-01]\n [1.113e+01 1.662e+01 7.047e+01 ... 4.044e-02 2.383e-01 7.083e-02]\n [1.382e+01 2.449e+01 9.233e+01 ... 1.521e-01 3.651e-01 1.183e-01]\n ...\n [1.532e+01 1.727e+01 1.032e+02 ... 2.229e-01 3.258e-01 1.191e-01]\n [1.262e+01 2.397e+01 8.135e+01 ... 1.180e-01 2.826e-01 9.585e-02]\n [1.168e+01 1.617e+01 7.549e+01 ... 9.815e-02 2.804e-01 8.024e-02]]\nY_train\n[1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 0 1 1 1 0 1 1 1 0 1 1 0 0 0\n 1 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 0\n 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 1 1 0 1 1 1\n 0 0 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 1 0\n 1 0 1 1 1 0 0 1 1 0 1 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1\n 0 1 1 0 1 0 1 1 1 0 1 1 0 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1\n 1 1 1 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0 0 1 1 1 0\n 0 0 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 0 1 1 1 0\n 0 0 0 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0\n 1 0 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 0 0 1 1 1\n 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1\n 1 1 0 1 1 1 0 1 0 0 0 1 1 1 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 1 1 1 0 0 0 1 1\n 1 1 0 1 0 0 0 0 0 0 1]\ny_test\n[0 1 0 1 0 1 1 0 0 0 1 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1\n 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0\n 0 0 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1\n 0 1 1]"
  },
  {
    "objectID": "midterm.html#entrenar-el-modelo",
    "href": "midterm.html#entrenar-el-modelo",
    "title": "Classification",
    "section": "",
    "text": "Code\nmodel = LogisticRegression(max_iter=1000000)\nmodel.fit(X_train,y_train)\n\n\nLogisticRegression(max_iter=1000000)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegression?Documentation for LogisticRegressioniFitted\n        \n            \n                Parameters\n                \n\n\n\n\npenalty \n'l2'\n\n\n\ndual \nFalse\n\n\n\ntol \n0.0001\n\n\n\nC \n1.0\n\n\n\nfit_intercept \nTrue\n\n\n\nintercept_scaling \n1\n\n\n\nclass_weight \nNone\n\n\n\nrandom_state \nNone\n\n\n\nsolver \n'lbfgs'\n\n\n\nmax_iter \n1000000\n\n\n\nmulti_class \n'deprecated'\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nl1_ratio \nNone"
  },
  {
    "objectID": "midterm.html#predicción",
    "href": "midterm.html#predicción",
    "title": "Classification",
    "section": "",
    "text": "Code\ny_pred= model.predict(X_test)"
  },
  {
    "objectID": "midterm.html#evaluación",
    "href": "midterm.html#evaluación",
    "title": "Classification",
    "section": "",
    "text": "Code\naccuracy= accuracy_score(y_test,y_pred)\nprecision=precision_score(y_test,y_pred)\nrecall=recall_score(y_test,y_pred)\nf1=f1_score(y_test,y_pred)\n\nprint(f\"Accuracy : \",accuracy)\nprint(\"precision :\",precision)\nprint(\"recall :\",recall)\nprint(\"f1 :\", f1)\n\n\nAccuracy :  0.9649122807017544\nprecision : 0.9594594594594594\nrecall : 0.9861111111111112\nf1 : 0.9726027397260274"
  },
  {
    "objectID": "midterm.html#matriz-de-confusion",
    "href": "midterm.html#matriz-de-confusion",
    "title": "Classification",
    "section": "",
    "text": "Code\nConfusionMatrixDisplay.from_predictions(y_test,y_pred)"
  },
  {
    "objectID": "midterm.html#importar-librerias-1",
    "href": "midterm.html#importar-librerias-1",
    "title": "Classification",
    "section": "2.1 1. Importar Librerias",
    "text": "2.1 1. Importar Librerias\n\n\nCode\n# 1. Import libraries\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler"
  },
  {
    "objectID": "midterm.html#cargar-el-dataset-1",
    "href": "midterm.html#cargar-el-dataset-1",
    "title": "Classification",
    "section": "2.2 Cargar el Dataset",
    "text": "2.2 Cargar el Dataset\n\n\nCode\ndf = load_breast_cancer()\nX= df.data\ny= df.target\n\nX\ny\n\n\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n       0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n       1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n       0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n       1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1])"
  },
  {
    "objectID": "midterm.html#split-de-la-data-1",
    "href": "midterm.html#split-de-la-data-1",
    "title": "Classification",
    "section": "2.3 Split de la data",
    "text": "2.3 Split de la data\n\n\nCode\nX_train, X_test, y_train, y_test = tts(\n    X,y,test_size=0.2, random_state=42,stratify=y\n)\nprint(\"X_train\")\nprint(X_train)\nprint(\"X_test\")\nprint(X_test)\n\n\nprint(\"Y_train\")\nprint(y_train)\nprint(\"y_test\")\nprint(y_test)\n\n\nX_train\n[[1.032e+01 1.635e+01 6.531e+01 ... 2.381e-02 2.681e-01 7.399e-02]\n [2.018e+01 1.954e+01 1.338e+02 ... 2.173e-01 3.032e-01 8.075e-02]\n [1.066e+01 1.515e+01 6.749e+01 ... 0.000e+00 2.710e-01 6.164e-02]\n ...\n [1.546e+01 2.395e+01 1.038e+02 ... 2.163e-01 3.013e-01 1.067e-01]\n [1.705e+01 1.908e+01 1.134e+02 ... 2.543e-01 3.109e-01 9.061e-02]\n [1.088e+01 1.562e+01 7.041e+01 ... 7.966e-02 2.581e-01 1.080e-01]]\nX_test\n[[1.955e+01 2.877e+01 1.336e+02 ... 1.941e-01 2.818e-01 1.005e-01]\n [1.113e+01 1.662e+01 7.047e+01 ... 4.044e-02 2.383e-01 7.083e-02]\n [1.382e+01 2.449e+01 9.233e+01 ... 1.521e-01 3.651e-01 1.183e-01]\n ...\n [1.532e+01 1.727e+01 1.032e+02 ... 2.229e-01 3.258e-01 1.191e-01]\n [1.262e+01 2.397e+01 8.135e+01 ... 1.180e-01 2.826e-01 9.585e-02]\n [1.168e+01 1.617e+01 7.549e+01 ... 9.815e-02 2.804e-01 8.024e-02]]\nY_train\n[1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 0 1 1 1 0 1 1 1 0 1 1 0 0 0\n 1 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 0\n 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 1 1 0 1 1 1\n 0 0 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 1 0\n 1 0 1 1 1 0 0 1 1 0 1 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1\n 0 1 1 0 1 0 1 1 1 0 1 1 0 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1\n 1 1 1 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0 0 1 1 1 0\n 0 0 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 0 1 1 1 0\n 0 0 0 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0\n 1 0 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 0 0 1 1 1\n 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1\n 1 1 0 1 1 1 0 1 0 0 0 1 1 1 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 1 1 1 0 0 0 1 1\n 1 1 0 1 0 0 0 0 0 0 1]\ny_test\n[0 1 0 1 0 1 1 0 0 0 1 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1\n 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0\n 0 0 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1\n 0 1 1]"
  },
  {
    "objectID": "midterm.html#pipeline",
    "href": "midterm.html#pipeline",
    "title": "Classification",
    "section": "2.4 Pipeline",
    "text": "2.4 Pipeline\n\n\nCode\npipe = Pipeline([\n    (\"escalado\", StandardScaler()),\n    (\"logreg\",LogisticRegression(max_iter=1000000))\n])"
  },
  {
    "objectID": "midterm.html#entrenar-el-modelo-1",
    "href": "midterm.html#entrenar-el-modelo-1",
    "title": "Classification",
    "section": "2.5 Entrenar el modelo",
    "text": "2.5 Entrenar el modelo\n\n\nCode\npipe.fit(X_train,y_train)\n\n\nPipeline(steps=[('escalado', StandardScaler()),\n                ('logreg', LogisticRegression(max_iter=1000000))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps \n[('escalado', ...), ('logreg', ...)]\n\n\n\ntransform_input \nNone\n\n\n\nmemory \nNone\n\n\n\nverbose \nFalse\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    LogisticRegression?Documentation for LogisticRegression\n        \n            \n                Parameters\n                \n\n\n\n\npenalty \n'l2'\n\n\n\ndual \nFalse\n\n\n\ntol \n0.0001\n\n\n\nC \n1.0\n\n\n\nfit_intercept \nTrue\n\n\n\nintercept_scaling \n1\n\n\n\nclass_weight \nNone\n\n\n\nrandom_state \nNone\n\n\n\nsolver \n'lbfgs'\n\n\n\nmax_iter \n1000000\n\n\n\nmulti_class \n'deprecated'\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nl1_ratio \nNone"
  },
  {
    "objectID": "midterm.html#predicción-1",
    "href": "midterm.html#predicción-1",
    "title": "Classification",
    "section": "2.6 Predicción",
    "text": "2.6 Predicción\n\n\nCode\ny_pred_pipe= pipe.predict(X_test)"
  },
  {
    "objectID": "midterm.html#evaluación-1",
    "href": "midterm.html#evaluación-1",
    "title": "Classification",
    "section": "2.7 Evaluación",
    "text": "2.7 Evaluación\n\n\nCode\naccuracy= accuracy_score(y_test,y_pred_pipe)\nprecision=precision_score(y_test,y_pred_pipe)\nrecall=recall_score(y_test,y_pred_pipe)\nf1=f1_score(y_test,y_pred_pipe)\n\nprint(f\"Accuracy : \",accuracy)\nprint(\"precision :\",precision)\nprint(\"recall :\",recall)\nprint(\"f1 :\", f1)\n\n\nAccuracy :  0.9824561403508771\nprecision : 0.9861111111111112\nrecall : 0.9861111111111112\nf1 : 0.9861111111111112"
  },
  {
    "objectID": "midterm.html#matriz-de-confusion-1",
    "href": "midterm.html#matriz-de-confusion-1",
    "title": "Classification",
    "section": "2.8 Matriz de confusion",
    "text": "2.8 Matriz de confusion\n\n\nCode\nConfusionMatrixDisplay.from_predictions(y_test,y_pred_pipe)"
  },
  {
    "objectID": "midterm_classification.qmd.html",
    "href": "midterm_classification.qmd.html",
    "title": "Classification",
    "section": "",
    "text": "1 📌 Introducción\nEl dataset “Datos de recaudaciones de tributos de comercio exterior” proviene del Servicio Nacional de Aduana del Ecuador (SENAE) y está disponible en el portal oficial de Datos Abiertos Ecuador.\nEste conjunto de datos recoge información detallada sobre los tributos generados por operaciones de comercio exterior en el Ecuador, con valores de recaudación asociados a diferentes tributos, distritos aduaneros y canales de pago.\n\n\n\n2 📊 Variables principales\nAlgunas columnas clave del dataset son:\n\nNOM_DISTRITO: Nombre del distrito aduanero donde se registró la operación.\n\nNOM_TRIBUTO: Tipo de tributo aplicado (ej. IVA, Arancel Advalorem, Salvaguardias).\n\nCANAL_PAGO: Medio por el cual se realizó el pago (internet, ventanilla, tarjeta, etc.).\n\nVALOR_TOTAL_TRIBUTO_EN_NC: Valor pagado con notas de crédito.\n\nVALOR_TOTAL_TRIBUTO_EN_EFECTIVO: Valor pagado en efectivo.\n\nVALOR_TOTAL_TRIBUTO_PAGADO: Valor total del tributo (suma de los anteriores).\n\n\n\n\n3 🎯 Propósito del análisis\nPara el ejercicio de regresión, nos planteamos el siguiente problema:\n\nPredecir el valor total de tributos pagados (VALOR_TOTAL_TRIBUTO_PAGADO) en función del canal de pago, el distrito, el tipo de tributo y los valores parciales (nota de crédito, efectivo).\n\nEste análisis nos permitirá:\n\nIdentificar qué factores influyen más en la recaudación.\n\nEvaluar la capacidad predictiva de un modelo de regresión en este contexto.\n\nDemostrar un flujo completo de Machine Learning supervisado con Scikit-learn Pipeline (preprocesamiento, entrenamiento, métricas y visualizaciones)."
  },
  {
    "objectID": "midterm_classification.html",
    "href": "midterm_classification.html",
    "title": "Clasificación",
    "section": "",
    "text": "El conjunto de datos “Casos COVID-19”, publicado por el Ministerio de Salud Pública del Ecuador (MSP), recopila los registros oficiales de casos de SARS-CoV-2 notificados entre febrero de 2020 y marzo de 2022.\nCada fila representa un caso epidemiológico individual, con datos sobre edad, sexo, provincia, fechas de atención y resultado final del paciente.\n\n💡 Importancia del dataset\n\n📊 Analiza la evolución de la pandemia en Ecuador.\n\n🧍‍♂️ Permite estudiar factores asociados a hospitalización o fallecimiento.\n\n🤖 Ideal para aplicar modelos supervisados (clasificación / regresión) en salud pública.\n\n\n\n\n\n\n\n\n🗂️ Aspecto\nDetalle\n\n\n\n\nFuente\nDatos Abiertos Ecuador — MSP\n\n\nEntidad responsable\nDirección Nacional de Vigilancia Epidemiológica, MSP\n\n\nFormato\nCSV (valores separados por comas)\n\n\nCobertura temporal\nFebrero 2020 – Marzo 2022\n\n\nLicencia\nCreative Commons Atribución (CC-BY)\n\n\nÚltima actualización\n13 de diciembre de 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n🏷️ Campo\n📖 Descripción\n⚙️ Uso\n\n\n\n\nanio_notificacion, mes_notificacion, dia_notificacion\nFecha en la que se notificó el caso.\nVariables numéricas predictoras.\n\n\nanio_atencion, mes_atencion, dia_atencion\nFecha de atención médica.\nPredictores temporales.\n\n\nedad_paciente\nEdad del paciente al momento de la atención.\nVariable clave numérica.\n\n\nsexo_paciente\nSexo del paciente (HOMBRE/MUJER).\nPuede binarizarse si se usa.\n\n\ncondicion_final\nEstado final: VIVO / MUERTO.\n🎯 Variable objetivo (y).\n\n\nae_se_notificacion\nSemana epidemiológica del año.\nControl temporal opcional.\n\n\n\n\n\n\n\nPredictores (X): - anio_notificacion, mes_notificacion, dia_notificacion\n- anio_atencion, mes_atencion, dia_atencion\n- edad_paciente\n- ae_se_notificacion\nVariable objetivo (y): - condicion_final → 0 = VIVO, 1 = MUERTO\n\n\n\n\nEl modelo busca predecir la condición final del paciente (supervivencia o fallecimiento) a partir de variables numéricas relacionadas con edad y tiempo de atención.\nSe excluyen variables categóricas para cumplir la rúbrica del proyecto y mantener un flujo de clasificación binaria limpia y reproducible."
  },
  {
    "objectID": "midterm_regression.html",
    "href": "midterm_regression.html",
    "title": "Regresión",
    "section": "",
    "text": "El dataset “Datos de recaudaciones de tributos de comercio exterior” proviene del Servicio Nacional de Aduana del Ecuador (SENAE) y está disponible en el portal oficial de Datos Abiertos Ecuador.\nEste conjunto de datos recoge información detallada sobre los tributos generados por operaciones de comercio exterior en el Ecuador, con valores de recaudación asociados a diferentes tributos, distritos aduaneros y canales de pago.\n\n\n\nAlgunas columnas clave del dataset son:\n\nNOM_DISTRITO: Nombre del distrito aduanero donde se registró la operación.\n\nNOM_TRIBUTO: Tipo de tributo aplicado (ej. IVA, Arancel Advalorem, Salvaguardias).\n\nCANAL_PAGO: Medio por el cual se realizó el pago (internet, ventanilla, tarjeta, etc.).\n\nVALOR_TOTAL_TRIBUTO_EN_NC: Valor pagado con notas de crédito.\n\nVALOR_TOTAL_TRIBUTO_EN_EFECTIVO: Valor pagado en efectivo.\n\nVALOR_TOTAL_TRIBUTO_PAGADO: Valor total del tributo (suma de los anteriores).\n\n\n\n\n\nPara el ejercicio de regresión, nos planteamos el siguiente problema:\n\nPredecir el valor total de tributos pagados (VALOR_TOTAL_TRIBUTO_PAGADO) en función del distrito, el tipo de tributo y los valores parciales (nota de crédito, efectivo).\n\nEste análisis nos permitirá:\n\nIdentificar qué factores influyen más en la recaudación.\n\nEvaluar la capacidad predictiva de un modelo de regresión en este contexto.\n\nDemostrar un flujo completo de Machine Learning supervisado con Scikit-learn Pipeline (preprocesamiento, entrenamiento, métricas y visualizaciones)."
  },
  {
    "objectID": "midterm_regression.html#variables-principales",
    "href": "midterm_regression.html#variables-principales",
    "title": "Regresión",
    "section": "",
    "text": "Algunas columnas clave del dataset son:\n\nNOM_DISTRITO: Nombre del distrito aduanero donde se registró la operación.\n\nNOM_TRIBUTO: Tipo de tributo aplicado (ej. IVA, Arancel Advalorem, Salvaguardias).\n\nCANAL_PAGO: Medio por el cual se realizó el pago (internet, ventanilla, tarjeta, etc.).\n\nVALOR_TOTAL_TRIBUTO_EN_NC: Valor pagado con notas de crédito.\n\nVALOR_TOTAL_TRIBUTO_EN_EFECTIVO: Valor pagado en efectivo.\n\nVALOR_TOTAL_TRIBUTO_PAGADO: Valor total del tributo (suma de los anteriores)."
  },
  {
    "objectID": "midterm_regression.html#propósito-del-análisis",
    "href": "midterm_regression.html#propósito-del-análisis",
    "title": "Regresión",
    "section": "",
    "text": "Para el ejercicio de regresión, nos planteamos el siguiente problema:\n\nPredecir el valor total de tributos pagados (VALOR_TOTAL_TRIBUTO_PAGADO) en función del distrito, el tipo de tributo y los valores parciales (nota de crédito, efectivo).\n\nEste análisis nos permitirá:\n\nIdentificar qué factores influyen más en la recaudación.\n\nEvaluar la capacidad predictiva de un modelo de regresión en este contexto.\n\nDemostrar un flujo completo de Machine Learning supervisado con Scikit-learn Pipeline (preprocesamiento, entrenamiento, métricas y visualizaciones)."
  },
  {
    "objectID": "midterm_regression.html#librerías",
    "href": "midterm_regression.html#librerías",
    "title": "Regresión",
    "section": "2.1 Librerías",
    "text": "2.1 Librerías\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split as tts\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay\nimport altair as alt"
  },
  {
    "objectID": "midterm_regression.html#carga-del-dataset",
    "href": "midterm_regression.html#carga-del-dataset",
    "title": "Regresión",
    "section": "2.2 Carga del dataset",
    "text": "2.2 Carga del dataset\n\n\nCode\ndf = pd.read_csv(\"/Users/usr-s3c/Documents/Maestria_YTech/Machine-Learning/db/senae.csv\", sep=\"|\", encoding=\"utf-8\")\ndf.info()\ndf.head()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 205188 entries, 0 to 205187\nData columns (total 12 columns):\n #   Column                           Non-Null Count   Dtype  \n---  ------                           --------------   -----  \n 0   NOM_DISTRITO                     205188 non-null  object \n 1   NOM_TRIBUTO                      205188 non-null  object \n 2   FEC_PAGO                         205188 non-null  object \n 3   NOM_TIPO_LIQUIDACION             205188 non-null  object \n 4   NOM_ESTADO_LIQUIDACION           205188 non-null  object \n 5   TIPOS_DOC_REFERENCIA             205188 non-null  object \n 6   NOM_INFRACCION                   205188 non-null  object \n 7   NOM_TIPO_SANCION                 205188 non-null  object \n 8   CANAL_PAGO                       205188 non-null  object \n 9   VALOR_TOTAL_TRIBUTO_EN_NC        205188 non-null  float64\n 10  VALOR_TOTAL_TRIBUTO_EN_EFECTIVO  205188 non-null  float64\n 11  VALOR_TOTAL_TRIBUTO_PAGADO       205188 non-null  float64\ndtypes: float64(3), object(9)\nmemory usage: 18.8+ MB\n\n\n\n\n\n\n\n\n\nNOM_DISTRITO\nNOM_TRIBUTO\nFEC_PAGO\nNOM_TIPO_LIQUIDACION\nNOM_ESTADO_LIQUIDACION\nTIPOS_DOC_REFERENCIA\nNOM_INFRACCION\nNOM_TIPO_SANCION\nCANAL_PAGO\nVALOR_TOTAL_TRIBUTO_EN_NC\nVALOR_TOTAL_TRIBUTO_EN_EFECTIVO\nVALOR_TOTAL_TRIBUTO_PAGADO\n\n\n\n\n0\n028-GUAYAQUIL - MARITIMO\n09-SALVAGUARDIA ESPECIFICA\n2024-01-01\n01-LIQUIDACION DE LA DECLARACION DE IMPORTACION\nC-PAGADO\n001-NUMERO DE IMPORTACION\nNO APLICA\nNO APLICA\n3 -INTERNET\n0.0\n0.00\n0.00\n\n\n1\n019-GUAYAQUIL - AEREO\n07-IVA\n2024-01-01\n01-LIQUIDACION DE LA DECLARACION DE IMPORTACION\nC-PAGADO\n011-NUMERO DE IMPORTACION SIMPLIFICADA\nNO APLICA\nNO APLICA\n11-SIN CANAL\n0.0\n0.00\n0.00\n\n\n2\n028-GUAYAQUIL - MARITIMO\n04-FONDINFA\n2024-01-02\n01-LIQUIDACION DE LA DECLARACION DE IMPORTACION\nC-PAGADO\n001-NUMERO DE IMPORTACION\nNO APLICA\nNO APLICA\n2 -OTROS DISPOSITIVOS\n0.0\n389630.34\n389630.34\n\n\n3\n028-GUAYAQUIL - MARITIMO\n11-INCREMENTO ICE\n2024-01-02\n01-LIQUIDACION DE LA DECLARACION DE IMPORTACION\nC-PAGADO\n001-NUMERO DE IMPORTACION\nNO APLICA\nNO APLICA\n11-SIN CANAL\n0.0\n0.00\n0.00\n\n\n4\n055-QUITO\n04-FONDINFA\n2024-01-02\n01-LIQUIDACION DE LA DECLARACION DE IMPORTACION\nC-PAGADO\n011-NUMERO DE IMPORTACION SIMPLIFICADA\nNO APLICA\nNO APLICA\n1 -VENTANILLA\n0.0\n214.72\n214.72\n\n\n\n\n\n\n\n\n2.2.1 Revisión de valores nulos y estadísticos\n\n\nCode\ndf.isna().sum()\n\n\nNOM_DISTRITO                       0\nNOM_TRIBUTO                        0\nFEC_PAGO                           0\nNOM_TIPO_LIQUIDACION               0\nNOM_ESTADO_LIQUIDACION             0\nTIPOS_DOC_REFERENCIA               0\nNOM_INFRACCION                     0\nNOM_TIPO_SANCION                   0\nCANAL_PAGO                         0\nVALOR_TOTAL_TRIBUTO_EN_NC          0\nVALOR_TOTAL_TRIBUTO_EN_EFECTIVO    0\nVALOR_TOTAL_TRIBUTO_PAGADO         0\ndtype: int64\n\n\n\n\nCode\ndf.describe()\n\n\n\n\n\n\n\n\n\nVALOR_TOTAL_TRIBUTO_EN_NC\nVALOR_TOTAL_TRIBUTO_EN_EFECTIVO\nVALOR_TOTAL_TRIBUTO_PAGADO\n\n\n\n\ncount\n2.051880e+05\n2.051880e+05\n2.051880e+05\n\n\nmean\n3.640965e+02\n1.840297e+04\n1.876707e+04\n\n\nstd\n1.234270e+04\n1.674045e+05\n1.678412e+05\n\n\nmin\n-1.000000e-02\n0.000000e+00\n-1.000000e-02\n\n\n25%\n0.000000e+00\n0.000000e+00\n0.000000e+00\n\n\n50%\n0.000000e+00\n0.000000e+00\n0.000000e+00\n\n\n75%\n0.000000e+00\n2.300000e+02\n2.709600e+02\n\n\nmax\n2.050702e+06\n7.518819e+06\n7.518819e+06\n\n\n\n\n\n\n\n\n\n2.2.2 Preprocesamiento\n\n\nCode\n# Selección de columnas (simplificado)\nnum_features = [\"VALOR_TOTAL_TRIBUTO_EN_NC\", \"VALOR_TOTAL_TRIBUTO_EN_EFECTIVO\"]\ncat_features = [\"NOM_DISTRITO\", \"CANAL_PAGO\"]\n\n\n\n\nCode\n# Mapeo manual de distritos\nmap_distrito = {\n    \"028-GUAYAQUIL - MARITIMO\": 1,\n    \"019-GUAYAQUIL - AEREO\": 2,\n    \"055-QUITO\": 3,\n    \"073-TULCAN\": 4,\n    \"082-HUAQUILLAS\": 5,\n    \"091-CUENCA\": 6,\n    \"037-MANTA\": 7,\n    \"064-PUERTO BOLIVAR\": 8,\n    \"136-GERENCIA GENERAL\": 9,\n    \"109-LOJA - MACARA\": 10,\n    \"145-CEBAF SAN MIGUEL\": 11,\n    \"046-ESMERALDAS\": 12,\n    \"127-LATACUNGA\": 13\n}\n\n# Mapeo manual de canales de pago\nmap_canal = {\n    \"3 -INTERNET\": 1,\n    \" 11-SIN CANAL\": 2,\n    \"2 -OTROS DISPOSITIVOS\": 3,\n    \"1 -VENTANILLA\": 4,\n    \"9 -TARJETA CREDITO\": 5,\n    \"7 -PAGO EN LINEA\": 6,\n    \"8 -DEBITO AUTOMATICO\": 7,\n    \" 12-OTROS\": 8\n}\n\n\n\n\nCode\n# Crear nuevas columnas numéricas\ndf[\"DISTRITO_ID\"] = df[\"NOM_DISTRITO\"].map(map_distrito)\ndf[\"CANAL_ID\"] = df[\"CANAL_PAGO\"].map(map_canal)\n\n# Revisar los resultados\ndf[[\"NOM_DISTRITO\", \"DISTRITO_ID\", \"CANAL_PAGO\", \"CANAL_ID\"]].head(20)\n\n\n\n\n\n\n\n\n\nNOM_DISTRITO\nDISTRITO_ID\nCANAL_PAGO\nCANAL_ID\n\n\n\n\n0\n028-GUAYAQUIL - MARITIMO\n1\n3 -INTERNET\n1\n\n\n1\n019-GUAYAQUIL - AEREO\n2\n11-SIN CANAL\n2\n\n\n2\n028-GUAYAQUIL - MARITIMO\n1\n2 -OTROS DISPOSITIVOS\n3\n\n\n3\n028-GUAYAQUIL - MARITIMO\n1\n11-SIN CANAL\n2\n\n\n4\n055-QUITO\n3\n1 -VENTANILLA\n4\n\n\n5\n055-QUITO\n3\n3 -INTERNET\n1\n\n\n6\n055-QUITO\n3\n11-SIN CANAL\n2\n\n\n7\n019-GUAYAQUIL - AEREO\n2\n11-SIN CANAL\n2\n\n\n8\n073-TULCAN\n4\n3 -INTERNET\n1\n\n\n9\n055-QUITO\n3\n3 -INTERNET\n1\n\n\n10\n055-QUITO\n3\n3 -INTERNET\n1\n\n\n11\n055-QUITO\n3\n1 -VENTANILLA\n4\n\n\n12\n055-QUITO\n3\n2 -OTROS DISPOSITIVOS\n3\n\n\n13\n073-TULCAN\n4\n9 -TARJETA CREDITO\n5\n\n\n14\n028-GUAYAQUIL - MARITIMO\n1\n3 -INTERNET\n1\n\n\n15\n028-GUAYAQUIL - MARITIMO\n1\n2 -OTROS DISPOSITIVOS\n3\n\n\n16\n019-GUAYAQUIL - AEREO\n2\n3 -INTERNET\n1\n\n\n17\n019-GUAYAQUIL - AEREO\n2\n3 -INTERNET\n1\n\n\n18\n028-GUAYAQUIL - MARITIMO\n1\n1 -VENTANILLA\n4\n\n\n19\n028-GUAYAQUIL - MARITIMO\n1\n1 -VENTANILLA\n4\n\n\n\n\n\n\n\n\n\nCode\n# Variables numéricas y categóricas codificadas\nX = df[[\"DISTRITO_ID\", \"CANAL_ID\",\"VALOR_TOTAL_TRIBUTO_EN_NC\", \"VALOR_TOTAL_TRIBUTO_EN_EFECTIVO\"]]\ny = df[\"VALOR_TOTAL_TRIBUTO_PAGADO\"]\n\ndf.head(10)\n\n\n\n\n\n\n\n\n\nNOM_DISTRITO\nNOM_TRIBUTO\nFEC_PAGO\nNOM_TIPO_LIQUIDACION\nNOM_ESTADO_LIQUIDACION\nTIPOS_DOC_REFERENCIA\nNOM_INFRACCION\nNOM_TIPO_SANCION\nCANAL_PAGO\nVALOR_TOTAL_TRIBUTO_EN_NC\nVALOR_TOTAL_TRIBUTO_EN_EFECTIVO\nVALOR_TOTAL_TRIBUTO_PAGADO\nDISTRITO_ID\nCANAL_ID\n\n\n\n\n0\n028-GUAYAQUIL - MARITIMO\n09-SALVAGUARDIA ESPECIFICA\n2024-01-01\n01-LIQUIDACION DE LA DECLARACION DE IMPORTACION\nC-PAGADO\n001-NUMERO DE IMPORTACION\nNO APLICA\nNO APLICA\n3 -INTERNET\n0.00\n0.00\n0.00\n1\n1\n\n\n1\n019-GUAYAQUIL - AEREO\n07-IVA\n2024-01-01\n01-LIQUIDACION DE LA DECLARACION DE IMPORTACION\nC-PAGADO\n011-NUMERO DE IMPORTACION SIMPLIFICADA\nNO APLICA\nNO APLICA\n11-SIN CANAL\n0.00\n0.00\n0.00\n2\n2\n\n\n2\n028-GUAYAQUIL - MARITIMO\n04-FONDINFA\n2024-01-02\n01-LIQUIDACION DE LA DECLARACION DE IMPORTACION\nC-PAGADO\n001-NUMERO DE IMPORTACION\nNO APLICA\nNO APLICA\n2 -OTROS DISPOSITIVOS\n0.00\n389630.34\n389630.34\n1\n3\n\n\n3\n028-GUAYAQUIL - MARITIMO\n11-INCREMENTO ICE\n2024-01-02\n01-LIQUIDACION DE LA DECLARACION DE IMPORTACION\nC-PAGADO\n001-NUMERO DE IMPORTACION\nNO APLICA\nNO APLICA\n11-SIN CANAL\n0.00\n0.00\n0.00\n1\n2\n\n\n4\n055-QUITO\n04-FONDINFA\n2024-01-02\n01-LIQUIDACION DE LA DECLARACION DE IMPORTACION\nC-PAGADO\n011-NUMERO DE IMPORTACION SIMPLIFICADA\nNO APLICA\nNO APLICA\n1 -VENTANILLA\n0.00\n214.72\n214.72\n3\n4\n\n\n5\n055-QUITO\n08-SALVAGUARDIA\n2024-01-02\n01-LIQUIDACION DE LA DECLARACION DE IMPORTACION\nC-PAGADO\n001-NUMERO DE IMPORTACION\nNO APLICA\nNO APLICA\n3 -INTERNET\n0.00\n0.00\n0.00\n3\n1\n\n\n6\n055-QUITO\n03-ANTIDUMPING\n2024-01-02\n01-LIQUIDACION DE LA DECLARACION DE IMPORTACION\nC-PAGADO\n011-NUMERO DE IMPORTACION SIMPLIFICADA\nNO APLICA\nNO APLICA\n11-SIN CANAL\n0.00\n0.00\n0.00\n3\n2\n\n\n7\n019-GUAYAQUIL - AEREO\n07-IVA\n2024-01-02\n01-LIQUIDACION DE LA DECLARACION DE IMPORTACION\nC-PAGADO\n011-NUMERO DE IMPORTACION SIMPLIFICADA\nNO APLICA\nNO APLICA\n11-SIN CANAL\n2097.51\n0.00\n2097.51\n2\n2\n\n\n8\n073-TULCAN\n04-FONDINFA\n2024-01-03\n01-LIQUIDACION DE LA DECLARACION DE IMPORTACION\nC-PAGADO\n001-NUMERO DE IMPORTACION\nNO APLICA\nNO APLICA\n3 -INTERNET\n0.00\n29524.03\n29524.03\n4\n1\n\n\n9\n055-QUITO\n02-ARANCEL ESPECIFICO\n2024-01-03\n01-LIQUIDACION DE LA DECLARACION DE IMPORTACION\nC-PAGADO\n001-NUMERO DE IMPORTACION\nNO APLICA\nNO APLICA\n3 -INTERNET\n0.00\n199174.39\n199174.39\n3\n1\n\n\n\n\n\n\n\n\n\nCode\nX_train, X_test, y_train, y_test = tts(\n    X, y, test_size=0.2, random_state=42\n)\n\n\n\n\nCode\npipe = Pipeline([\n    (\"escalado\", StandardScaler()),\n    (\"logreg\",LinearRegression( n_jobs=-1))\n])"
  },
  {
    "objectID": "midterm_regression.html#entrenar-el-modelo",
    "href": "midterm_regression.html#entrenar-el-modelo",
    "title": "Regresión",
    "section": "2.3 Entrenar el modelo",
    "text": "2.3 Entrenar el modelo\n\n\nCode\npipe.fit(X_train, y_train)\n\n\nPipeline(steps=[('escalado', StandardScaler()),\n                ('logreg', LinearRegression(n_jobs=-1))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps \n[('escalado', ...), ('logreg', ...)]\n\n\n\ntransform_input \nNone\n\n\n\nmemory \nNone\n\n\n\nverbose \nFalse\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    LinearRegression?Documentation for LinearRegression\n        \n            \n                Parameters\n                \n\n\n\n\nfit_intercept \nTrue\n\n\n\ncopy_X \nTrue\n\n\n\ntol \n1e-06\n\n\n\nn_jobs \n-1\n\n\n\npositive \nFalse\n\n\n\n\n            \n        \n    \n\n\n\n\nCode\ny_pred = pipe.predict(X_test)\n\nmse = mean_squared_error(y_test, y_pred)\nr2  = r2_score(y_test, y_pred)\n\nprint(f\"MSE: {mse:,.2f}\")\nprint(f\"R² : {r2:.4f}\")\n\n\nMSE: 0.00\nR² : 1.0000\n\n\n\n2.3.1 🟦 1️⃣ Gráfico: Reales vs. Predichos\nEn esta gráfica, los puntos azules se alinean exactamente sobre la línea diagonal roja.\nEsto refleja que las predicciones del modelo son idénticas a los valores reales de recaudación.\n\nMSE = 0.00 → significa que no existe diferencia entre los valores reales y los predichos; el error cuadrático medio es nulo.\n\nR² = 1.00 → indica que el modelo explica el 100 % de la variabilidad del valor total del tributo.\n\n\n\nCode\nalt.data_transformers.disable_max_rows()\n\ndf_pred = pd.DataFrame({\n    \"Real\": pd.Series(y_test).astype(float).values,\n    \"Predicho\": pd.Series(y_pred).astype(float).values\n})\n\nminv = float(min(df_pred[\"Real\"].min(), df_pred[\"Predicho\"].min()))\nmaxv = float(max(df_pred[\"Real\"].max(), df_pred[\"Predicho\"].max()))\n\nscatter = (\n    alt.Chart(df_pred)\n    .mark_circle(size=60, opacity=0.5, color=\"steelblue\")\n    .encode(\n        x=alt.X(\"Real:Q\", title=\"Valor Real\"),\n        y=alt.Y(\"Predicho:Q\", title=\"Predicción\"),\n        tooltip=[\"Real:Q\",\"Predicho:Q\"]\n    )\n    .properties(width=400, height=400, title=\"Reales vs. Predichos (Regresión SENAE)\")\n)\n\nref_df = pd.DataFrame({\"Real\":[minv,maxv], \"Predicho\":[minv,maxv]})\nlinea = alt.Chart(ref_df).mark_line(color=\"red\", strokeDash=[5,5]).encode(\n    x=\"Real:Q\", y=\"Predicho:Q\"\n)\n\nscatter + linea\n\n\n\n\n\n\n\n\n\n\n2.3.2 🟧 2️⃣ Gráfico: Curva de Aprendizaje (R²)\nEn esta gráfica, las curvas de entrenamiento (R²_train) y validación (R²_val)\nse mantienen planas y constantes en 1.0 a lo largo de todos los tamaños de muestra.\n\nMSE = 0.00 en cada subconjunto → el modelo comete cero error sin importar cuántos datos use.\n\nR² = 1.00 en todas las etapas → el desempeño es perfecto y estable desde el inicio del entrenamiento.\n\nEn la curva de aprendizaje se observa solo una línea (color naranja), porque los valores de R² en entrenamiento y validación son idénticos (1.0).\nEsto ocurre debido a que el modelo reproduce una relación exacta entre las variables, por lo que ambas curvas se superponen completamente.\n\n\nCode\n## 7) Visualización 2 — Curva de aprendizaje (Altair)\n\nimport numpy as np\nfrom sklearn.model_selection import learning_curve\n\ntrain_sizes, train_scores, test_scores = learning_curve(\n    pipe, X, y, cv=5, scoring=\"r2\",\n    train_sizes=np.linspace(0.1, 1.0, 8), n_jobs=-1\n)\n\ndf_lc = pd.DataFrame({\n    \"train_size\": train_sizes,\n    \"R2_train\": train_scores.mean(axis=1),\n    \"R2_val\":   test_scores.mean(axis=1)\n})\ndf_long = df_lc.melt(id_vars=\"train_size\", value_vars=[\"R2_train\",\"R2_val\"],\n                     var_name=\"Conjunto\", value_name=\"R2\")\n\nalt.Chart(df_long).mark_line(point=True).encode(\n    x=alt.X(\"train_size:Q\", title=\"Tamaño de entrenamiento\"),\n    y=alt.Y(\"R2:Q\", title=\"R²\"),\n    color=alt.Color(\"Conjunto:N\", title=\"\"),\n    tooltip=[\"train_size:Q\",\"Conjunto:N\",\"R2:Q\"]\n).properties(width=400, height=400, title=\"Curva de aprendizaje (R²)\").interactive()\n\n\n\n\n\n\n\n\n\n\n2.3.3 🧠 Interpretación del modelo\nEl modelo de Regresión Lineal logró un desempeño perfecto\n(R² = 1.0, MSE = 0) porque la estructura de los datos del SENAE presenta una relación completamente lineal y determinística entre las variables.\nEn el dataset, el campo VALOR_TOTAL_TRIBUTO_PAGADO se obtiene directamente como la suma exacta de los valores parciales:\n\nVALOR_TOTAL_TRIBUTO_PAGADO = VALOR_TOTAL_TRIBUTO_EN_NC + VALOR_TOTAL_TRIBUTO_EN_EFECTIVO\n\nPor esta razón, el modelo ajusta los datos sin error, generando predicciones idénticas a los valores reales."
  },
  {
    "objectID": "midterm_regression.html#interpretación-breve",
    "href": "midterm_regression.html#interpretación-breve",
    "title": "Regresión",
    "section": "2.4 8) Interpretación breve",
    "text": "2.4 8) Interpretación breve"
  },
  {
    "objectID": "midterm_classification.html#carga-del-dataset",
    "href": "midterm_classification.html#carga-del-dataset",
    "title": "Clasificación",
    "section": "2.1 Carga del dataset",
    "text": "2.1 Carga del dataset\n\n\nCode\ndf = pd.read_csv(\"/Users/usr-s3c/Documents/Maestria_YTech/Machine-Learning/db/covid2.csv\", encoding=\"utf-8\")\ndf.info()\ndf.head()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 298140 entries, 0 to 298139\nData columns (total 26 columns):\n #   Column                    Non-Null Count   Dtype  \n---  ------                    --------------   -----  \n 0   fecha_notificacion        298140 non-null  object \n 1   anio_notificacion         298140 non-null  int64  \n 2   mes_notificacion          298140 non-null  int64  \n 3   dia_notificacion          298140 non-null  int64  \n 4   cod_provincia             298140 non-null  int64  \n 5   provincia                 298140 non-null  object \n 6   cod_canton                298140 non-null  int64  \n 7   canton                    298140 non-null  object \n 8   fecha_atencion            298140 non-null  object \n 9   anio_atencion             298140 non-null  int64  \n 10  mes_atencion              298140 non-null  int64  \n 11  dia_atencion              298140 non-null  int64  \n 12  cod_provincia_residencia  298140 non-null  int64  \n 13  provincia_residencia      298140 non-null  object \n 14  cod_canton_residencia     298140 non-null  int64  \n 15  canton_residencia         298140 non-null  object \n 16  edad_paciente             298140 non-null  int64  \n 17  tipo_edad                 298140 non-null  object \n 18  sexo_paciente             298140 non-null  object \n 19  condicion_final           298140 non-null  object \n 20  fecha_defuncion           8303 non-null    object \n 21  anio_defuncion            8303 non-null    float64\n 22  mes_defuncion             8303 non-null    float64\n 23  dia_defuncion             8303 non-null    float64\n 24  clasificacion_final       298140 non-null  object \n 25  ae_se_notificacion        298140 non-null  int64  \ndtypes: float64(3), int64(12), object(11)\nmemory usage: 59.1+ MB\n\n\n\n\n\n\n\n\n\nfecha_notificacion\nanio_notificacion\nmes_notificacion\ndia_notificacion\ncod_provincia\nprovincia\ncod_canton\ncanton\nfecha_atencion\nanio_atencion\n...\nedad_paciente\ntipo_edad\nsexo_paciente\ncondicion_final\nfecha_defuncion\nanio_defuncion\nmes_defuncion\ndia_defuncion\nclasificacion_final\nae_se_notificacion\n\n\n\n\n0\n18/9/20\n2020\n9\n18\n17\nPICHINCHA\n1701\nDISTRITO METROPOLITANO DE QUITO\n25/8/20\n2020\n...\n24\nANIOS\nHOMBRE\nVIVO\nNaN\nNaN\nNaN\nNaN\nDESCARTADO\n202038\n\n\n1\n6/11/20\n2020\n11\n6\n17\nPICHINCHA\n1701\nDISTRITO METROPOLITANO DE QUITO\n11/4/20\n2020\n...\n39\nANIOS\nHOMBRE\nVIVO\nNaN\nNaN\nNaN\nNaN\nCONFIRMADO\n202045\n\n\n2\n11/9/20\n2020\n9\n11\n21\nSUCUMBIOS\n2101\nLAGO AGRIO\n9/10/20\n2020\n...\n25\nANIOS\nHOMBRE\nVIVO\nNaN\nNaN\nNaN\nNaN\nDESCARTADO\n202037\n\n\n3\n30/6/20\n2020\n6\n30\n9\nGUAYAS\n901\nGUAYAQUIL\n29/6/20\n2020\n...\n26\nANIOS\nHOMBRE\nVIVO\nNaN\nNaN\nNaN\nNaN\nCONFIRMADO\n202027\n\n\n4\n14/5/20\n2020\n5\n14\n9\nGUAYAS\n910\nMILAGRO\n13/5/20\n2020\n...\n52\nANIOS\nHOMBRE\nVIVO\nNaN\nNaN\nNaN\nNaN\nDESCARTADO\n202020\n\n\n\n\n5 rows × 26 columns\n\n\n\n\n2.1.1 Revisión de valores nulos y estadísticos\n\n\nCode\ndf.isna().sum()\n\n\nfecha_notificacion               0\nanio_notificacion                0\nmes_notificacion                 0\ndia_notificacion                 0\ncod_provincia                    0\nprovincia                        0\ncod_canton                       0\ncanton                           0\nfecha_atencion                   0\nanio_atencion                    0\nmes_atencion                     0\ndia_atencion                     0\ncod_provincia_residencia         0\nprovincia_residencia             0\ncod_canton_residencia            0\ncanton_residencia                0\nedad_paciente                    0\ntipo_edad                        0\nsexo_paciente                    0\ncondicion_final                  0\nfecha_defuncion             289837\nanio_defuncion              289837\nmes_defuncion               289837\ndia_defuncion               289837\nclasificacion_final              0\nae_se_notificacion               0\ndtype: int64\n\n\n\n\nCode\ndf.describe()\n\n\n\n\n\n\n\n\n\nanio_notificacion\nmes_notificacion\ndia_notificacion\ncod_provincia\ncod_canton\nanio_atencion\nmes_atencion\ndia_atencion\ncod_provincia_residencia\ncod_canton_residencia\nedad_paciente\nanio_defuncion\nmes_defuncion\ndia_defuncion\nae_se_notificacion\n\n\n\n\ncount\n298140.000000\n298140.000000\n298140.000000\n298140.000000\n298140.000000\n298140.000000\n298140.000000\n298140.000000\n298140.000000\n298140.000000\n298140.000000\n8303.000000\n8303.000000\n8303.000000\n298140.000000\n\n\nmean\n2020.165050\n7.511404\n16.326028\n13.541511\n1356.837207\n2020.161434\n6.642772\n16.208231\n13.484735\n1351.334799\n39.708905\n2020.090449\n5.822112\n15.782007\n202047.859787\n\n\nstd\n0.422002\n3.056931\n8.804367\n5.393828\n538.726129\n0.418009\n3.278291\n8.379187\n5.384631\n537.721122\n17.730722\n0.293894\n2.661372\n9.041657\n35.268021\n\n\nmin\n2020.000000\n1.000000\n1.000000\n1.000000\n101.000000\n2020.000000\n1.000000\n1.000000\n1.000000\n101.000000\n0.000000\n2020.000000\n1.000000\n1.000000\n202009.000000\n\n\n25%\n2020.000000\n6.000000\n9.000000\n9.000000\n906.000000\n2020.000000\n4.000000\n8.000000\n9.000000\n907.000000\n28.000000\n2020.000000\n4.000000\n8.000000\n202029.000000\n\n\n50%\n2020.000000\n8.000000\n17.000000\n16.000000\n1601.000000\n2020.000000\n7.000000\n16.000000\n15.000000\n1501.000000\n37.000000\n2020.000000\n5.000000\n16.000000\n202039.000000\n\n\n75%\n2020.000000\n10.000000\n24.000000\n17.000000\n1701.000000\n2020.000000\n9.000000\n24.000000\n17.000000\n1701.000000\n51.000000\n2020.000000\n8.000000\n24.000000\n202046.000000\n\n\nmax\n2022.000000\n12.000000\n31.000000\n24.000000\n2403.000000\n2022.000000\n12.000000\n31.000000\n24.000000\n2403.000000\n120.000000\n2022.000000\n12.000000\n31.000000\n202214.000000\n\n\n\n\n\n\n\n\n\n2.1.2 Preprocesamiento\n\n\nCode\n# 🔍 Seleccionar solo las columnas necesarias para clasificación\ncols_relevantes = [\n    \"anio_atencion\", \"mes_atencion\",\"edad_paciente\",\"clasificacion_final\", \"condicion_final\"\n]\ndf = df[cols_relevantes].copy()\nprint(f\"Columnas seleccionadas: {list(df.columns)}\")\ndf.head()\n\n\nColumnas seleccionadas: ['anio_atencion', 'mes_atencion', 'edad_paciente', 'clasificacion_final', 'condicion_final']\n\n\n\n\n\n\n\n\n\nanio_atencion\nmes_atencion\nedad_paciente\nclasificacion_final\ncondicion_final\n\n\n\n\n0\n2020\n8\n24\nDESCARTADO\nVIVO\n\n\n1\n2020\n4\n39\nCONFIRMADO\nVIVO\n\n\n2\n2020\n10\n25\nDESCARTADO\nVIVO\n\n\n3\n2020\n6\n26\nCONFIRMADO\nVIVO\n\n\n4\n2020\n5\n52\nDESCARTADO\nVIVO\n\n\n\n\n\n\n\n\n\n2.1.3 Resumen del preprocesamiento\nEn esta etapa se realiza la limpieza inicial de los datos con el objetivo de preparar las variables para el modelo de clasificación:\n\nEstandarización de texto:\nSe convierten a minúsculas y se eliminan espacios en blanco en las columnas clasificacion_final y condicion_final, garantizando consistencia en los valores.\nFiltrado de registros válidos:\nSolo se conservan los casos con clasificación epidemiológica reconocida: confirmado, probable o descartado.\nCreación de la variable objetivo:\nSe transforma condicion_final en una variable binaria:\n0 = vivo y 1 = muerto.\nLimpieza de valores nulos:\nSe eliminan los registros sin etiqueta válida o con datos incompletos en la variable objetivo.\nConversión de tipo:\nFinalmente, la columna condicion_final se convierte al tipo entero (int), dejándola lista para el entrenamiento del modelo.\n\n\n💡 Con este proceso, el dataset queda estandarizado, limpio y con una variable objetivo binaria adecuada para aplicar un modelo de clasificación supervisada.\n\n\n\nCode\n# Normalizamos los textos a minúsculas sin espacios extra\ndf[\"clasificacion_final\"] = df[\"clasificacion_final\"].str.strip().str.lower()\ndf[\"condicion_final\"] = df[\"condicion_final\"].str.strip().str.lower()\n\n# 🔸 Filtrar solo registros válidos de clasificación (descartado, confirmado, probable)\n# Convertir valores de texto a números\ndf[\"clasificacion_final\"] = df[\"clasificacion_final\"].map({\n    \"confirmado\": 2,\n    \"probable\": 1,\n    \"descartado\": 0\n})\n\n# 🔸 Mapear la variable objetivo: 1 = muerto, 0 = vivo\ndf[\"condicion_final\"] = df[\"condicion_final\"].map({\"vivo\": 0, \"muerto\": 1})\n\n# 🔸 Eliminar filas sin etiqueta válida\ndf = df.dropna(subset=[\"condicion_final\"])\n\n# 🔸 Convertir tipo a entero\ndf[\"condicion_final\"] = df[\"condicion_final\"].astype(int)\n\nprint(\"Distribución de la variable objetivo:\")\nprint(df[\"condicion_final\"].value_counts())\n\ndf.head(10)\n\n\nDistribución de la variable objetivo:\ncondicion_final\n0    289837\n1      8303\nName: count, dtype: int64\n\n\n\n\n\n\n\n\n\nanio_atencion\nmes_atencion\nedad_paciente\nclasificacion_final\ncondicion_final\n\n\n\n\n0\n2020\n8\n24\n0.0\n0\n\n\n1\n2020\n4\n39\n2.0\n0\n\n\n2\n2020\n10\n25\n0.0\n0\n\n\n3\n2020\n6\n26\n2.0\n0\n\n\n4\n2020\n5\n52\n0.0\n0\n\n\n5\n2020\n4\n65\n2.0\n0\n\n\n6\n2020\n5\n57\n0.0\n0\n\n\n7\n2020\n8\n45\n2.0\n0\n\n\n8\n2020\n7\n32\n0.0\n0\n\n\n9\n2020\n4\n61\n2.0\n0\n\n\n\n\n\n\n\n\n\n2.1.4 Selección de campos para el Dataset\n\n\nCode\n# Variable objetivo\nX = df[[\"anio_atencion\", \"mes_atencion\", \"edad_paciente\", \"clasificacion_final\"]]\ny = df[\"condicion_final\"]\n\nX = X.dropna()\ny = y.loc[X.index]\n\n\nprint(\"Dimensiones de los conjuntos:\")\nprint(f\"X → {X.shape[0]} filas × {X.shape[1]} columnas\")\nprint(f\"y → {y.shape[0]} etiquetas\")\nX.head()\ny\n\n\nDimensiones de los conjuntos:\nX → 297636 filas × 4 columnas\ny → 297636 etiquetas\n\n\n0         0\n1         0\n2         0\n3         0\n4         0\n         ..\n298135    0\n298136    0\n298137    0\n298138    0\n298139    0\nName: condicion_final, Length: 297636, dtype: int64"
  },
  {
    "objectID": "midterm_classification.html#introducción-al-dataset-casos-covid-19-del-ministerio-de-salud-pública-msp",
    "href": "midterm_classification.html#introducción-al-dataset-casos-covid-19-del-ministerio-de-salud-pública-msp",
    "title": "Clasificación",
    "section": "",
    "text": "El conjunto de datos “Casos COVID-19”, publicado por el Ministerio de Salud Pública del Ecuador (MSP), contiene los registros epidemiológicos oficiales de los casos de infección por SARS-CoV-2 reportados en el país entre febrero de 2020 y el 4 de marzo de 2022.\nEste dataset forma parte del sistema de vigilancia epidemiológica (SIVE) administrado por el MSP, y cada fila representa un caso epidemiológico individual, con información sobre la edad, sexo, provincia, resultado de la prueba diagnóstica, estado clínico y otras variables relacionadas con el seguimiento de la pandemia.\n\nImportancia del dataset\n\nPermite analizar la evolución de la pandemia en Ecuador y su distribución geográfica.\n\nFacilita el estudio de factores asociados a desenlaces como recuperación, hospitalización o fallecimiento.\n\nConstituye una fuente de datos real y pública para aplicar técnicas de aprendizaje supervisado (clasificación o regresión) en el ámbito de la salud pública.\n\n\n\n\n\nFuente: Datos Abiertos Ecuador — MSP\n\nEntidad responsable: Ministerio de Salud Pública del Ecuador, Dirección Nacional de Vigilancia Epidemiológica.\n\nFormato: CSV (valores separados por comas).\n\nCobertura temporal: Febrero de 2020 – Marzo de 2022.\n\nLicencia: Creative Commons Atribución (CC-BY).\n\nÚltima actualización: 13 de diciembre de 2022.\n\n\n\n\n\nVariables categóricas: el dataset incluye columnas con valores textuales (como resultado de prueba o estado clínico). Para este proyecto se utilizarán únicamente variables numéricas, cumpliendo el criterio del trabajo práctico.\n\nDesbalance de clases: algunas categorías, como “fallecido”, pueden tener menor representación, lo cual debe considerarse al momento de entrenar el modelo de clasificación.\n\nConsistencia y limpieza: al tratarse de datos masivos, pueden existir valores faltantes, inconsistentes o duplicados; será necesario aplicar procesos de limpieza previos al modelado.\n\nUnidad de análisis: cada fila corresponde a un caso notificado, por lo que una misma persona podría tener más de un registro si fue atendida en diferentes fechas o establecimientos.\n\nCon esta base, el dataset servirá para desarrollar un modelo de clasificación binaria que permita predecir resultados clínicos (por ejemplo, recuperación o fallecimiento), aplicando los conceptos de aprendizaje supervisado estudiados en clase.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split as tts\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay\nimport altair as alt"
  },
  {
    "objectID": "midterm_classification.html#descripción-de-los-campos-principales-del-dataset",
    "href": "midterm_classification.html#descripción-de-los-campos-principales-del-dataset",
    "title": "Clasificación",
    "section": "",
    "text": "El conjunto de datos del Ministerio de Salud Pública del Ecuador (MSP) contiene información detallada sobre los casos confirmados, descartados y probables de COVID-19 registrados en todo el país.\nA continuación se describen los campos más relevantes utilizados para el desarrollo del modelo de clasificación binaria, conforme a los requisitos del trabajo práctico.\n\n\n\n\n\n\n\n\nCampo\nDescripción\nTipo de dato / Uso en el modelo\n\n\n\n\nfecha_notificacion\nFecha en la que se notifica oficialmente el caso al sistema nacional de vigilancia.\nSe puede transformar en componentes numéricos (año, mes, día) para análisis temporal.\n\n\nanio_notificacion, mes_notificacion, dia_notificacion\nDescomposición de la fecha de notificación. Permiten identificar patrones estacionales o picos de contagios.\nNuméricos — candidatos a variables predictoras.\n\n\ncod_provincia, provincia\nCódigo y nombre de la provincia donde se notificó el caso.\nCategóricos — se pueden omitir o codificar (no se usarán en este trabajo según la rúbrica).\n\n\ncod_canton, canton\nCódigo y nombre del cantón de notificación.\nCategóricos — no se usarán directamente.\n\n\nfecha_atencion\nFecha en la que el paciente fue atendido en un establecimiento de salud.\nSe puede calcular el intervalo entre atención y notificación.\n\n\nanio_atencion, mes_atencion, dia_atencion\nComponentes de la fecha de atención.\nNuméricos — útiles para análisis de tiempos de respuesta.\n\n\ncod_provincia_residencia, provincia_residencia\nProvincia de residencia del paciente.\nCategóricos — se omiten para evitar variables no numéricas.\n\n\ncod_canton_residencia, canton_residencia\nCantón de residencia del paciente.\nCategóricos — no incluidos en el modelo.\n\n\nedad_paciente\nEdad reportada del paciente al momento de la atención.\nNumérico — variable fundamental para el modelado predictivo.\n\n\ntipo_edad\nUnidad de edad (años, meses, días). En la mayoría de registros se expresa en “AÑOS”.\nCategórico — puede eliminarse si no aporta variabilidad.\n\n\nsexo_paciente\nSexo biológico del paciente (HOMBRE/MUJER).\nCategórico — se podría convertir a valores binarios si se decide usar.\n\n\ncondicion_final\nEstado del paciente al cierre del caso (VIVO o MUERTO).\nVariable objetivo (y) → binaria: 1 = MUERTO, 0 = VIVO.\n\n\nfecha_defuncion, anio_defuncion, mes_defuncion, dia_defuncion\nFecha y componentes de la defunción, si aplica.\nPueden servir para verificar coherencia con la condición final.\n\n\nclasificacion_final\nClasificación epidemiológica del caso: CONFIRMADO, PROBABLE o DESCARTADO.\nSe puede usar como filtro o como etiqueta auxiliar para validar los casos analizados.\n\n\nae_se_notificacion\nCódigo epidemiológico semanal (semana epidemiológica del año).\nNumérico — puede utilizarse para análisis temporal o como variable de control."
  },
  {
    "objectID": "midterm_classification.html#características-técnicas",
    "href": "midterm_classification.html#características-técnicas",
    "title": "Clasificación",
    "section": "",
    "text": "🗂️ Aspecto\nDetalle\n\n\n\n\nFuente\nDatos Abiertos Ecuador — MSP\n\n\nEntidad responsable\nDirección Nacional de Vigilancia Epidemiológica, MSP\n\n\nFormato\nCSV (valores separados por comas)\n\n\nCobertura temporal\nFebrero 2020 – Marzo 2022\n\n\nLicencia\nCreative Commons Atribución (CC-BY)\n\n\nÚltima actualización\n13 de diciembre de 2022"
  },
  {
    "objectID": "midterm_classification.html#consideraciones-para-el-análisis",
    "href": "midterm_classification.html#consideraciones-para-el-análisis",
    "title": "Clasificación",
    "section": "",
    "text": "Variables categóricas: el dataset incluye columnas con valores textuales (como resultado de prueba o estado clínico). Para este proyecto se utilizarán únicamente variables numéricas, cumpliendo el criterio del trabajo práctico.\n\nDesbalance de clases: algunas categorías, como “fallecido”, pueden tener menor representación, lo cual debe considerarse al momento de entrenar el modelo de clasificación.\n\nConsistencia y limpieza: al tratarse de datos masivos, pueden existir valores faltantes, inconsistentes o duplicados; será necesario aplicar procesos de limpieza previos al modelado.\n\nUnidad de análisis: cada fila corresponde a un caso notificado, por lo que una misma persona podría tener más de un registro si fue atendida en diferentes fechas o establecimientos.\n\nCon esta base, el dataset servirá para desarrollar un modelo de clasificación binaria que permita predecir resultados clínicos (por ejemplo, recuperación o fallecimiento), aplicando los conceptos de aprendizaje supervisado estudiados en clase."
  },
  {
    "objectID": "midterm_classification.html#variables-seleccionadas-para-el-modelo",
    "href": "midterm_classification.html#variables-seleccionadas-para-el-modelo",
    "title": "Clasificación",
    "section": "",
    "text": "Predictores (X): - anio_notificacion, mes_notificacion, dia_notificacion\n- anio_atencion, mes_atencion, dia_atencion\n- edad_paciente\n- ae_se_notificacion\nVariable objetivo (y): - condicion_final → 0 = VIVO, 1 = MUERTO"
  },
  {
    "objectID": "midterm_classification.html#justificación",
    "href": "midterm_classification.html#justificación",
    "title": "Clasificación",
    "section": "",
    "text": "El modelo busca predecir la condición final del paciente (supervivencia o fallecimiento) a partir de variables numéricas relacionadas con edad y tiempo de atención.\nSe excluyen variables categóricas para cumplir la rúbrica del proyecto y mantener un flujo de clasificación binaria limpia y reproducible."
  },
  {
    "objectID": "midterm_classification.html#campos-principales-del-dataset",
    "href": "midterm_classification.html#campos-principales-del-dataset",
    "title": "Clasificación",
    "section": "",
    "text": "🏷️ Campo\n📖 Descripción\n⚙️ Uso\n\n\n\n\nanio_notificacion, mes_notificacion, dia_notificacion\nFecha en la que se notificó el caso.\nVariables numéricas predictoras.\n\n\nanio_atencion, mes_atencion, dia_atencion\nFecha de atención médica.\nPredictores temporales.\n\n\nedad_paciente\nEdad del paciente al momento de la atención.\nVariable clave numérica.\n\n\nsexo_paciente\nSexo del paciente (HOMBRE/MUJER).\nPuede binarizarse si se usa.\n\n\ncondicion_final\nEstado final: VIVO / MUERTO.\n🎯 Variable objetivo (y).\n\n\nae_se_notificacion\nSemana epidemiológica del año.\nControl temporal opcional."
  },
  {
    "objectID": "midterm_classification.html#pipeline",
    "href": "midterm_classification.html#pipeline",
    "title": "Clasificación",
    "section": "2.3 Pipeline",
    "text": "2.3 Pipeline\n\n\nCode\npipe = Pipeline([\n    (\"escalado\", StandardScaler()),\n    (\"logreg\",LogisticRegression(max_iter=1000000))\n])"
  },
  {
    "objectID": "midterm_classification.html#entrenar-el-modelo",
    "href": "midterm_classification.html#entrenar-el-modelo",
    "title": "Clasificación",
    "section": "2.4 Entrenar el modelo",
    "text": "2.4 Entrenar el modelo\n\n\nCode\npipe.fit(X_train,y_train)\n\n\nPipeline(steps=[('escalado', StandardScaler()),\n                ('logreg', LogisticRegression(max_iter=1000000))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps \n[('escalado', ...), ('logreg', ...)]\n\n\n\ntransform_input \nNone\n\n\n\nmemory \nNone\n\n\n\nverbose \nFalse\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    LogisticRegression?Documentation for LogisticRegression\n        \n            \n                Parameters\n                \n\n\n\n\npenalty \n'l2'\n\n\n\ndual \nFalse\n\n\n\ntol \n0.0001\n\n\n\nC \n1.0\n\n\n\nfit_intercept \nTrue\n\n\n\nintercept_scaling \n1\n\n\n\nclass_weight \nNone\n\n\n\nrandom_state \nNone\n\n\n\nsolver \n'lbfgs'\n\n\n\nmax_iter \n1000000\n\n\n\nmulti_class \n'deprecated'\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nl1_ratio \nNone"
  },
  {
    "objectID": "midterm_classification.html#predicción",
    "href": "midterm_classification.html#predicción",
    "title": "Clasificación",
    "section": "2.5 Predicción",
    "text": "2.5 Predicción\n\n\nCode\ny_pred_pipe= pipe.predict(X_test)"
  },
  {
    "objectID": "midterm_classification.html#evaluación",
    "href": "midterm_classification.html#evaluación",
    "title": "Clasificación",
    "section": "2.6 Evaluación",
    "text": "2.6 Evaluación\n\n\nCode\naccuracy= accuracy_score(y_test,y_pred_pipe)\nprecision=precision_score(y_test,y_pred_pipe)\nrecall=recall_score(y_test,y_pred_pipe)\nf1=f1_score(y_test,y_pred_pipe)\n\nprint(f\"Accuracy : \",accuracy)\nprint(\"precision :\",precision)\nprint(\"recall :\",recall)\nprint(\"f1 :\", f1)\n\n\nAccuracy :  0.9714588092998253\nprecision : 0.38953488372093026\nrecall : 0.0403371462974112\nf1 : 0.07310420076377523"
  },
  {
    "objectID": "midterm_classification.html#matriz-de-confusion",
    "href": "midterm_classification.html#matriz-de-confusion",
    "title": "Clasificación",
    "section": "2.7 Matriz de confusion",
    "text": "2.7 Matriz de confusion\n\n2.7.1 📊 Análisis\n\nVerdaderos Negativos (57 762):\nCasos correctamente clasificados como vivos.\nRepresentan la gran mayoría de los registros, lo que muestra que el modelo identifica correctamente a los pacientes que no fallecieron.\nFalsos Positivos (105):\nCasos predichos como muertos cuando en realidad estaban vivos.\nSon pocos, lo que indica un bajo nivel de alarmas falsas.\nFalsos Negativos (1 594):\nCasos reales de muertos que el modelo clasificó como vivos.\nEste valor es relativamente alto en proporción a los positivos reales, lo que sugiere que el modelo tiende a subestimar los casos críticos (bajo recall para la clase 1).\nVerdaderos Positivos (67):\nCasos correctamente identificados como muertos.\nAunque son pocos, muestran que el modelo tiene cierta capacidad de detección de la clase minoritaria.\n\n\n\n\n2.7.2 📈 Conclusión\nEl modelo logra un excelente desempeño general (alta precisión global), pero presenta desbalance de clases: la mayoría de los pacientes son vivos, y los casos muertos son escasos.\n\n\nCode\nConfusionMatrixDisplay.from_predictions(y_test,y_pred_pipe)\n\n\n\n\n\n\n\n\n\n\n\n2.7.3 📈 Interpretación de la Curva ROC\nLa curva ROC (Receiver Operating Characteristic) muestra la relación entre la tasa de verdaderos positivos (TPR) y la tasa de falsos positivos (FPR) a diferentes umbrales de decisión.\n\nLa línea roja discontinua representa un modelo aleatorio (sin poder de discriminación).\n\nLa curva azul, correspondiente al modelo de regresión logística, se eleva muy por encima de esta línea.\n\nEl valor AUC = 0.902 indica una alta capacidad de clasificación, lo que significa que el modelo distingue correctamente entre los pacientes vivos (0) y muertos (1) en más del 90% de los casos.\n\n\n✅ En términos prácticos:\nEl modelo tiene un desempeño excelente y generaliza bien; sin embargo, se recomienda revisar el balance de clases y ajustar el umbral dependiedno la necesidad\n\n\n\nCode\n# Calcular los valores de la curva ROC\ny_proba = pipe.predict_proba(X_test)[:, 1]\nfpr, tpr, thr = roc_curve(y_test, y_proba)\nauc = roc_auc_score(y_test, y_proba)\n\n# Crear un DataFrame con los resultados\nroc_df = pd.DataFrame({\n    \"FPR\": fpr,\n    \"TPR\": tpr,\n    \"Threshold\": thr\n})\n\n# === Gráfico ROC con Altair ===\nroc_chart = (\n    alt.Chart(roc_df)\n    .mark_line(color=\"#1f77b4\", strokeWidth=3)\n    .encode(\n        x=alt.X(\"FPR\", title=\"Tasa de falsos positivos\"),\n        y=alt.Y(\"TPR\", title=\"Tasa de verdaderos positivos\"),\n        tooltip=[\"FPR\", \"TPR\", \"Threshold\"]\n    )\n    .properties(\n        title=f\"Curva ROC (AUC = {auc:.3f})\",\n        width=400,\n        height=300,\n    ).interactive()\n)\n\n# Línea diagonal de referencia (clasificador aleatorio)\nref_line = (\n    alt.Chart(pd.DataFrame({\"x\": [0, 1], \"y\": [0, 1]}))\n    .mark_line(strokeDash=[5, 5], color=\"red\")\n    .encode(x=\"x\", y=\"y\")\n)\n\nroc_chart_final = roc_chart + ref_line\nroc_chart_final"
  },
  {
    "objectID": "midterm_classification.html#división-de-datos-train-y-test",
    "href": "midterm_classification.html#división-de-datos-train-y-test",
    "title": "Clasificación",
    "section": "2.2 División de datos Train y Test",
    "text": "2.2 División de datos Train y Test\n\n\nCode\nX_train, X_test, y_train, y_test = tts(\n    X,y,test_size=0.2, random_state=42,stratify=y\n)\nprint(\"X_train\")\nprint(X_train)\nprint(\"X_test\")\nprint(X_test)\n\n\nprint(\"Y_train\")\nprint(y_train)\nprint(\"y_test\")\nprint(y_test)\n\n\nX_train\n        anio_atencion  mes_atencion  edad_paciente  clasificacion_final\n38355            2020            11             42                  0.0\n45855            2020            12             78                  0.0\n213784           2021             4             21                  0.0\n131373           2020             7             42                  2.0\n48286            2020            11             34                  2.0\n...               ...           ...            ...                  ...\n151738           2020             2             45                  2.0\n121168           2020             6             21                  0.0\n51233            2020            11             34                  0.0\n223903           2020            10             63                  0.0\n138724           2020             7             55                  0.0\n\n[238108 rows x 4 columns]\nX_test\n        anio_atencion  mes_atencion  edad_paciente  clasificacion_final\n103414           2020             7             85                  2.0\n83515            2020            11             79                  0.0\n205962           2020            11             21                  0.0\n80186            2020             8             23                  2.0\n69273            2020             9             56                  0.0\n...               ...           ...            ...                  ...\n116721           2020             5             39                  1.0\n268055           2020            12             40                  2.0\n246002           2020             5             33                  0.0\n276284           2020             5             69                  2.0\n177896           2020             7             21                  2.0\n\n[59528 rows x 4 columns]\nY_train\n38355     0\n45855     0\n213784    0\n131373    0\n48286     0\n         ..\n151738    0\n121168    0\n51233     0\n223903    0\n138724    0\nName: condicion_final, Length: 238108, dtype: int64\ny_test\n103414    1\n83515     0\n205962    0\n80186     0\n69273     0\n         ..\n116721    0\n268055    0\n246002    0\n276284    1\n177896    0\nName: condicion_final, Length: 59528, dtype: int64"
  },
  {
    "objectID": "midterm_regression.html#interpretación-del-modelo",
    "href": "midterm_regression.html#interpretación-del-modelo",
    "title": "Regresión",
    "section": "2.4 🧠 Interpretación del modelo",
    "text": "2.4 🧠 Interpretación del modelo\nEl modelo de Regresión Lineal logró un desempeño perfecto\n(R² = 1.0, MSE = 0) porque la estructura de los datos del SENAE presenta una relación completamente lineal y determinística entre las variables.\nEn el dataset, el campo VALOR_TOTAL_TRIBUTO_PAGADO se obtiene directamente como la suma exacta de los valores parciales:\n\nVALOR_TOTAL_TRIBUTO_PAGADO = VALOR_TOTAL_TRIBUTO_EN_NC + VALOR_TOTAL_TRIBUTO_EN_EFECTIVO\n\nPor esta razón, el modelo ajusta los datos sin error, generando predicciones idénticas a los valores reales."
  },
  {
    "objectID": "categorical_encoding.html",
    "href": "categorical_encoding.html",
    "title": "Categorical Encoding",
    "section": "",
    "text": "import pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler,LabelEncoder\nfrom sklearn.compose import ColumnTransformer\n\n\n\n\n\ndata = {\n    'size' : ['M', 'S', 'L'], # Categorico Ordinal porque va en orden\n    'color' : ['green','red','blue'], #  \n    'price': [10.1, 13.5, 15.6],\n    'label': ['pantalones', 'camisetas', 'camisetas']\n}\n\ndf = pd.DataFrame(data)\ndf\n\n\n\n\n\n\n\n\nsize\ncolor\nprice\nlabel\n\n\n\n\n0\nM\ngreen\n10.1\npantalones\n\n\n1\nS\nred\n13.5\ncamisetas\n\n\n2\nL\nblue\n15.6\ncamisetas\n\n\n\n\n\n\n\n\n\n\n\ndf['label_encoded']=LabelEncoder().fit_transform(df['label'])\ndf\n\n\n\n\n\n\n\n\nsize\ncolor\nprice\nlabel\nlabel_encoded\n\n\n\n\n0\nM\ngreen\n10.1\npantalones\n1\n\n\n1\nS\nred\n13.5\ncamisetas\n0\n\n\n2\nL\nblue\n15.6\ncamisetas\n0\n\n\n\n\n\n\n\n\n\n\n\nsize_order = [['S','M','L']]\ndf['size_encoded']=OrdinalEncoder(categories=size_order).fit_transform(df[['size']])\ndf\n\n\n\n\n\n\n\n\nsize\ncolor\nprice\nlabel\nlabel_encoded\nsize_encoded\n\n\n\n\n0\nM\ngreen\n10.1\npantalones\n1\n1.0\n\n\n1\nS\nred\n13.5\ncamisetas\n0\n0.0\n\n\n2\nL\nblue\n15.6\ncamisetas\n0\n2.0\n\n\n\n\n\n\n\n\n\n\n\ncolor_encoder = OneHotEncoder (sparse_output=False, handle_unknown='ignore')\ncolor_encoded = color_encoder.fit_transform(df[['color']])\ncolor_title_encoded = color_encoder.get_feature_names_out(['color'])\n\ndf[color_title_encoded] = color_encoded\ndf\n\n\n\n\n\n\n\n\nsize\ncolor\nprice\nlabel\nlabel_encoded\nsize_encoded\ncolor_blue\ncolor_green\ncolor_red\n\n\n\n\n0\nM\ngreen\n10.1\npantalones\n1\n1.0\n0.0\n1.0\n0.0\n\n\n1\nS\nred\n13.5\ncamisetas\n0\n0.0\n0.0\n0.0\n1.0\n\n\n2\nL\nblue\n15.6\ncamisetas\n0\n2.0\n1.0\n0.0\n0.0\n\n\n\n\n\n\n\n\n\n\n\n\n\ncategoricas_features = ['color']\nordinal_features = ['size']\nnumerical_features = ['price']\n\nsize_orde = [['S','M','L']]\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('color_onehot', OneHotEncoder(),categoricas_features),\n        ('size_ord',OrdinalEncoder(categories=size_orde), ordinal_features),\n        ('price_stan',StandardScaler(),numerical_features)\n    ]\n)\n\n\n\n\n\nfeature_processor = preprocessor.fit_transform(df)\nfeature_processor\n\narray([[ 0.        ,  1.        ,  0.        ,  1.        , -1.30910667],\n       [ 0.        ,  0.        ,  1.        ,  0.        ,  0.19121783],\n       [ 1.        ,  0.        ,  0.        ,  2.        ,  1.11788884]])",
    "crumbs": [
      "Home",
      "Classification",
      "Ejemplo de Categorización"
    ]
  },
  {
    "objectID": "categorical_encoding.html#importación-de-librerias",
    "href": "categorical_encoding.html#importación-de-librerias",
    "title": "Categorical Encoding",
    "section": "",
    "text": "import pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler,LabelEncoder\nfrom sklearn.compose import ColumnTransformer",
    "crumbs": [
      "Home",
      "Classification",
      "Ejemplo de Categorización"
    ]
  },
  {
    "objectID": "categorical_encoding.html#dataframe-encoder",
    "href": "categorical_encoding.html#dataframe-encoder",
    "title": "Categorical Encoding",
    "section": "",
    "text": "data = {\n    'size' : ['M', 'S', 'L'], # Categorico Ordinal porque va en orden\n    'color' : ['green','red','blue'], #  \n    'price': [10.1, 13.5, 15.6],\n    'label': ['pantalones', 'camisetas', 'camisetas']\n}\n\ndf = pd.DataFrame(data)\ndf\n\n\n\n\n\n\n\n\nsize\ncolor\nprice\nlabel\n\n\n\n\n0\nM\ngreen\n10.1\npantalones\n\n\n1\nS\nred\n13.5\ncamisetas\n\n\n2\nL\nblue\n15.6\ncamisetas",
    "crumbs": [
      "Home",
      "Classification",
      "Ejemplo de Categorización"
    ]
  },
  {
    "objectID": "categorical_encoding.html#label-encoder",
    "href": "categorical_encoding.html#label-encoder",
    "title": "Categorical Encoding",
    "section": "",
    "text": "Code\ndf['label_encoded']=LabelEncoder().fit_transform(df['label'])\ndf\n\n\n\n\n\n\n\n\n\nsize\ncolor\nprice\nlabel\nlabel_encoded\n\n\n\n\n0\nM\ngreen\n10.1\npantalones\n1\n\n\n1\nS\nred\n13.5\ncamisetas\n0\n\n\n2\nL\nblue\n15.6\ncamisetas\n0"
  },
  {
    "objectID": "categorical_encoding.html#label-encoder-usar-solo-en-output",
    "href": "categorical_encoding.html#label-encoder-usar-solo-en-output",
    "title": "Categorical Encoding",
    "section": "",
    "text": "df['label_encoded']=LabelEncoder().fit_transform(df['label'])\ndf\n\n\n\n\n\n\n\n\nsize\ncolor\nprice\nlabel\nlabel_encoded\n\n\n\n\n0\nM\ngreen\n10.1\npantalones\n1\n\n\n1\nS\nred\n13.5\ncamisetas\n0\n\n\n2\nL\nblue\n15.6\ncamisetas\n0",
    "crumbs": [
      "Home",
      "Classification",
      "Ejemplo de Categorización"
    ]
  },
  {
    "objectID": "categorical_encoding.html#ordinal-encoder",
    "href": "categorical_encoding.html#ordinal-encoder",
    "title": "Categorical Encoding",
    "section": "",
    "text": "size_order = [['S','M','L']]\ndf['size_encoded']=OrdinalEncoder(categories=size_order).fit_transform(df[['size']])\ndf\n\n\n\n\n\n\n\n\nsize\ncolor\nprice\nlabel\nlabel_encoded\nsize_encoded\n\n\n\n\n0\nM\ngreen\n10.1\npantalones\n1\n1.0\n\n\n1\nS\nred\n13.5\ncamisetas\n0\n0.0\n\n\n2\nL\nblue\n15.6\ncamisetas\n0\n2.0",
    "crumbs": [
      "Home",
      "Classification",
      "Ejemplo de Categorización"
    ]
  },
  {
    "objectID": "categorical_encoding.html#onehotencoder",
    "href": "categorical_encoding.html#onehotencoder",
    "title": "Categorical Encoding",
    "section": "",
    "text": "color_encoder = OneHotEncoder (sparse_output=False, handle_unknown='ignore')\ncolor_encoded = color_encoder.fit_transform(df[['color']])\ncolor_title_encoded = color_encoder.get_feature_names_out(['color'])\n\ndf[color_title_encoded] = color_encoded\ndf\n\n\n\n\n\n\n\n\nsize\ncolor\nprice\nlabel\nlabel_encoded\nsize_encoded\ncolor_blue\ncolor_green\ncolor_red\n\n\n\n\n0\nM\ngreen\n10.1\npantalones\n1\n1.0\n0.0\n1.0\n0.0\n\n\n1\nS\nred\n13.5\ncamisetas\n0\n0.0\n0.0\n0.0\n1.0\n\n\n2\nL\nblue\n15.6\ncamisetas\n0\n2.0\n1.0\n0.0\n0.0",
    "crumbs": [
      "Home",
      "Classification",
      "Ejemplo de Categorización"
    ]
  },
  {
    "objectID": "categorical_encoding.html#column-transformer",
    "href": "categorical_encoding.html#column-transformer",
    "title": "Categorical Encoding",
    "section": "",
    "text": "categoricas_features = ['color']\nordinal_features = ['size']\nnumerical_features = ['price']\n\nsize_orde = [['S','M','L']]\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('color_onehot', OneHotEncoder(),categoricas_features),\n        ('size_ord',OrdinalEncoder(categories=size_orde), ordinal_features),\n        ('price_stan',StandardScaler(),numerical_features)\n    ]\n)\n\n\n\n\n\nfeature_processor = preprocessor.fit_transform(df)\nfeature_processor\n\narray([[ 0.        ,  1.        ,  0.        ,  1.        , -1.30910667],\n       [ 0.        ,  0.        ,  1.        ,  0.        ,  0.19121783],\n       [ 1.        ,  0.        ,  0.        ,  2.        ,  1.11788884]])",
    "crumbs": [
      "Home",
      "Classification",
      "Ejemplo de Categorización"
    ]
  },
  {
    "objectID": "categorical_pipeline.html",
    "href": "categorical_pipeline.html",
    "title": "Categorical Encoding",
    "section": "",
    "text": "import pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler,LabelEncoder\nfrom sklearn.compose import ColumnTransformer\nimport pandas as pd\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, LabelEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import classification_report,ConfusionMatrixDisplay",
    "crumbs": [
      "Home",
      "Classification",
      "Categorización con Pipeline"
    ]
  },
  {
    "objectID": "categorical_pipeline.html#importación-de-librerias",
    "href": "categorical_pipeline.html#importación-de-librerias",
    "title": "Categorical Encoding",
    "section": "",
    "text": "import pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler,LabelEncoder\nfrom sklearn.compose import ColumnTransformer\nimport pandas as pd\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, LabelEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import classification_report,ConfusionMatrixDisplay",
    "crumbs": [
      "Home",
      "Classification",
      "Categorización con Pipeline"
    ]
  },
  {
    "objectID": "categorical_pipeline.html#eliminación-de-nulos",
    "href": "categorical_pipeline.html#eliminación-de-nulos",
    "title": "Categorical Encoding",
    "section": "Eliminación de nulos",
    "text": "Eliminación de nulos\n\ndf = df.dropna()",
    "crumbs": [
      "Home",
      "Classification",
      "Categorización con Pipeline"
    ]
  },
  {
    "objectID": "categorical_pipeline.html#separación-de-features-y-target",
    "href": "categorical_pipeline.html#separación-de-features-y-target",
    "title": "Categorical Encoding",
    "section": "Separación de Features y Target",
    "text": "Separación de Features y Target\n\nFeature\n\nX = df.drop('class', axis=1)\nX\n\n\n\n\n\n\n\n\nage\nworkclass\nfnlwgt\neducation\neducation-num\nmarital-status\noccupation\nrelationship\nrace\nsex\ncapital-gain\ncapital-loss\nhours-per-week\nnative-country\n\n\n\n\n0\n25\nPrivate\n226802\n11th\n7\nNever-married\nMachine-op-inspct\nOwn-child\nBlack\nMale\n0\n0\n40\nUnited-States\n\n\n1\n38\nPrivate\n89814\nHS-grad\n9\nMarried-civ-spouse\nFarming-fishing\nHusband\nWhite\nMale\n0\n0\n50\nUnited-States\n\n\n2\n28\nLocal-gov\n336951\nAssoc-acdm\n12\nMarried-civ-spouse\nProtective-serv\nHusband\nWhite\nMale\n0\n0\n40\nUnited-States\n\n\n3\n44\nPrivate\n160323\nSome-college\n10\nMarried-civ-spouse\nMachine-op-inspct\nHusband\nBlack\nMale\n7688\n0\n40\nUnited-States\n\n\n5\n34\nPrivate\n198693\n10th\n6\nNever-married\nOther-service\nNot-in-family\nWhite\nMale\n0\n0\n30\nUnited-States\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n48837\n27\nPrivate\n257302\nAssoc-acdm\n12\nMarried-civ-spouse\nTech-support\nWife\nWhite\nFemale\n0\n0\n38\nUnited-States\n\n\n48838\n40\nPrivate\n154374\nHS-grad\n9\nMarried-civ-spouse\nMachine-op-inspct\nHusband\nWhite\nMale\n0\n0\n40\nUnited-States\n\n\n48839\n58\nPrivate\n151910\nHS-grad\n9\nWidowed\nAdm-clerical\nUnmarried\nWhite\nFemale\n0\n0\n40\nUnited-States\n\n\n48840\n22\nPrivate\n201490\nHS-grad\n9\nNever-married\nAdm-clerical\nOwn-child\nWhite\nMale\n0\n0\n20\nUnited-States\n\n\n48841\n52\nSelf-emp-inc\n287927\nHS-grad\n9\nMarried-civ-spouse\nExec-managerial\nWife\nWhite\nFemale\n15024\n0\n40\nUnited-States\n\n\n\n\n45222 rows × 14 columns\n\n\n\n\n\nTarget\n\ny = df['class']\ny\n\n0        &lt;=50K\n1        &lt;=50K\n2         &gt;50K\n3         &gt;50K\n5        &lt;=50K\n         ...  \n48837    &lt;=50K\n48838     &gt;50K\n48839    &lt;=50K\n48840    &lt;=50K\n48841     &gt;50K\nName: class, Length: 45222, dtype: category\nCategories (2, object): ['&lt;=50K', '&gt;50K']\n\n\n\n\nAsignación de Valores y división\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42,stratify=y)",
    "crumbs": [
      "Home",
      "Classification",
      "Categorización con Pipeline"
    ]
  },
  {
    "objectID": "nltk.html",
    "href": "nltk.html",
    "title": "Natural Lenguaje ToolKit",
    "section": "",
    "text": "import pandas as pd\nimport requests\n \n# Natural Language Toolkit\nimport nltk\n# downloading some additional packages and corpora\nnltk.download('punkt_tab') # necessary for tokenization\nnltk.download('wordnet') # necessary for lemmatization\nnltk.download('stopwords') # necessary for removal of stop words\nnltk.download('averaged_perceptron_tagger_eng') # necessary for POS tagging\nnltk.download('maxent_ne_chunker' ) # necessary for entity extraction\nnltk.download('omw-1.4') # necessary for lemmatization\nnltk.download('words')\n\n[nltk_data] Downloading package punkt_tab to\n[nltk_data]     /Users/usr-s3c/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n[nltk_data] Downloading package wordnet to /Users/usr-s3c/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/usr-s3c/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n[nltk_data]     /Users/usr-s3c/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package maxent_ne_chunker to\n[nltk_data]     /Users/usr-s3c/nltk_data...\n[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /Users/usr-s3c/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n[nltk_data] Downloading package words to /Users/usr-s3c/nltk_data...\n[nltk_data]   Package words is already up-to-date!\n\n\nTrue\n\n\n\n\n\n\nurl = \"https://raw.githubusercontent.com/erickedu85/dataset/master/story.txt\"\nr = requests.get(url)\nr.encoding = \"utf-8\"\n\nstory = r.text\nstory\n\n'The seventh Sally or how Trurl\\'s own perfection led to no good\\nBy StanisÅ‚aw Lem, 1965.\\nTranslated by Michael Kandel, 1974.\\n\\nThe Universe is infinite but bounded, and therefore a beam of light, in whatever direction it may travel, will after billions of centuries return -  if powerful enough - to the point of its departure; and it is no different with rumor, that flies about from star to star and makes the rounds of every planet. One day Trurl heard distant reports of two mighty constructor-benefactors, so wise and so accomplished that they had no equal; with this news he ran to Klapaucius, who explained to him that these were not mysterious rivals, but only themselves, for their fame had circumnavigated space. Fame, however, has this fault, that it says nothing of one\\'s failures, even when those very failures are the product of a great perfection. And he who would doubt this, let him recall the last of the seven sallies of Trurl, which was undertaken without Klapaucius, whom certain urgent duties kept at home at the time.\\n\\nIn those days Trurl was exceedingly vain, receiving all marks of veneration and honor paid to him as his due and a perfectly normal thing. He was heading north in his ship, as he was the least familiar with that region, and had flown through the void for quite some time, passing spheres full of the clamor of war as well as spheres that had finally obtained the perfect peace of desolation, when suddenly a little planet came into view, really more of a stray fragment of matter than a planet.\\n\\nOn the surface of this chunk of rock someone was running back and forth, jumping and waving his arms in the strangest way. Astonished by a scene of such total loneliness and concerned by those wild gestures of despair, and perhaps of anger as well, Trurl quickly landed.\\n\\nHe was approached by a personage of tremendous hauteur, iridium and vanadium all over and with a great deal of clanging and clanking, who introduced himself as Excelsius the Tartarian, ruler of Pancreon and Cyspenderora; the inhabitants of both these kingdoms had, in a fit of regicidal madness, driven His Highness from the throne and exiled him to this barren asteroid, eternally adrift among the dark swells and currents of gravitation.\\n\\nLearning in turn the identity of his visitor, the deposed monarch began to insist that Trurl - who after all was something of a professional when it came to good deeds - immediately restore him to his former position. The thought of such a turn of events brought the flame of vengeance to the monarch\\'s eyes, and his iron fingers clutched the air, as if already closing around the throats of his beloved subjects.\\n\\nNow Trurl had no intention of complying with this request of Excelsius, as doing so would bring about untold evil and suffering, yet at the same time he wished somehow to comfort and console the humiliated king. Thinking a moment or two, he came to the conclusion that, even in this case, not all was lost, for it would be possible to satisfy the king completely - without putting his former subjects in jeopardy. And so, rolling up his sleeves and summoning up all his mastery, Trurl built the king an entirely new kingdom. There were plenty of towns, rivers, mountains, forests, and brooks, a sky with clouds, armies full of derring-do, citadels, castles, and ladies\\' chambers; and there were marketplaces, gaudy and gleaming in the sun, days of back-breaking labor, nights full of dancing and song until dawn, and the gay clatter of swordplay. Trurl also carefully set into this kingdom a fabulous capital, all in marble and alabaster, and assembled a council of hoary sages, and winter palaces and summer villas, plots, conspirators, false witnesses, nurses, informers, teams of magnificent steeds, and plumes waving crimson in the wind; and then he crisscrossed that atmosphere with silver fanfares and twenty-one gun salutes, also threw in the necessary handful of traitors, another of heroes, added a pinch of prophets and seers, and one messiah and one great poet each, after which he bent over and set the works in motion, deftly making last-minute adjustments with his microscopic tools as it ran, and he gave the women of that kingdom beauty, the men - sullen silence and surliness when drunk, the officials - arrogance and servility, the astronomers - an enthusiasm for stars, and the children - a great capacity for noise. And all of this, connected, mounted and ground to precision, fit into a box, and not a very large box, but just the size that could be carried about with ease. This Trurl presented to Excelsius, to rule and have dominion over forever; but first he showed him where the input and output of his brand-new kingdom were, and how to program wars, quell rebellions, exact tribute, collect taxes, and also instructed him in the critical points and transition states of that microminiaturized society - in other words the maxima and minima of palace coups and revolutions -  and explained everything so well that the king, an old hand in the running of tyrannies, instantly grasped the directions and, without hesitation, while the constructor watched, issued a few trial proclamations, correctly manipulating the control knobs, which were carved with imperial eagles and regal lions. These proclamations declared a state of emergency, martial law, a curfew, and a special levy. After a year had passed in the kingdom, which amounted to hardly a minute for Trurl and the king, by an act of the greatest magnanimity - that is, by a flick of the finger at the controls - the king abolished one death penalty, lightened the levy, and deigned to annul the state of emergency, whereupon a tumultuous cry of gratitude, like the squeaking of tiny mice lifted by their tails, rose up from the box, and through its curved glass cover one could see, on the dusty highways and along the banks of lazy rivers that reflected the fluffy clouds, the people rejoicing and praising the great and unsurpassed benevolence of their sovereign lord.\\n\\nAnd so, though at first he had felt insulted by Trurl\\'s gift, in that the kingdom was too small and very like a child\\'s toy, the monarch saw that the thick glass lid made everything inside seem large; perhaps too he dully understood that size was not what mattered here, for government is not measured in meters and kilograms, and emotions are somehow the same, whether experienced by giants or dwarfs - and so he thanked the constructor, if somewhat stiffly. Who knows, he might even have liked to order him thrown in chains and tortured to death, just to be safe - that would have been a sure way of nipping in the bud any gossip about how some common vagabond tinkerer presented a mighty monarch with a kingdom. Excelsius was sensible enough, however, to see that this was out of the question, owing to a very fundamental disproportion, for fleas could sooner take their host into captivity than the king\\'s army seize Trurl. So with another cold nod, he stuck his orb and scepter under his arm, lifted the box kingdom with a grunt, and took it to his humble hut of exile. And as blazing day alternated with murky night outside, according to the rhythm of the asteroid\\'s rotation, the king, who was acknowledged by his subjects as the greatest in the world, diligently reigned, bidding this, forbidding that, beheading, rewarding - in all these ways incessantly spurring his little ones on to perfect fealty and worship of the throne.\\n\\nAs for Trurl, he returned home and related to his friend Klapaucius, not without pride, how he had employed his constructor\\'s genius to indulge the autocratic aspirations of Excelsius and, at the same time, safeguard the democratic aspirations of his former subjects. But Klapaucius, surprisingly enough, had no words of praise for Trurl; in fact, there seemed to be rebuke in his expression.\\n\\n\"Have I understood you correctly?\" he said at last. \"You gave that brutal despot, that born slave master, that slavering sadist of a pain- monger, you gave him a whole civilization to rule and have dominion over forever? And you tell me, moreover, of the cries of joy brought on by the repeal of a fraction of his cruel decrees! Trurl, how could you have done such a thing?\"\\n\\n\"You must be joking!\" Trurl exclaimed. \"Really, the whole kingdom fits into a box three feet by two by two and a half ... it\\'s only a model....\"\\n\\n\"A model of what?\"\\n\\n\"What do you mean, of what? Of a civilization, obviously, except that it\\'s a hundred million times smaller.â€\\x9d\\n\\n\"And how do you know there aren\\'t civilizations a hundred million times larger than our own? And if there were, would ours then be a model? And what importance do dimensions have anyway? In that box kingdom, doesn\\'t a journey from the capital to one of the corners take months - for those inhabitants? And don\\'t they suffer, don\\'t they know the burden of labor, don\\'t they die?\"\\n\\n\"Now just a minute, you know yourself that all these processes take place only because I programmed them, and so they aren\\'t genuine....\"\\n\\n\"Aren\\'t genuine? You mean to say the box is empty, and the parades, tortures, and beheadings are merely an illusion?\"\\n\\n\"Not an illusion, no, since they have reality, though purely as certain microscopic phenomena, which I produced by manipulating atoms,\" said Trurl. \"The point is, these births, loves, acts of heroism, and denunciations are nothing but the minuscule capering of electrons in space, precisely arranged by the skill of my nonlinear craft, which - \"\\n\\n\"Enough of your boasting, not another word!\" Klapaucius snapped. \"Are these processes self-organizing or not?\"\\n\\n\"Of course they are!\"\\n\\n\"And they occur among infinitesimal clouds of electrical charge?\"\\n\\n\"You know they do.\"\"And the phenomenological events of dawns, sunsets, and bloody battles are generated by the concatenation of real variables?\"\\n\\n\"Certainly.\"\\n\\n\"And are not we as well, if you examine us physically, mechanistically, statistically, and meticulously, nothing but the miniscule capering of electron clouds? Positive and negative charges arranged in space? And is our existence not the result of subatomic collisions and the interplay of particles, though we ourselves perceive those molecular cartwheels as fear, longing, or meditation? And when you daydream, what transpires within your brain but the binary algebra of connecting and disconnecting circuits, the continual meandering of electrons?\"\\n\\n\"What, Klapaucius, would you equate our existence with that of an imitation kingdom locked up in some glass box?!\" cried Trurl. \"No, really, that\\'s going too far! My purpose was simply to fashion a simulator of statehood, a model cybernetically perfect, nothing more!\"\\n\\n\"Trurl! Our perfection is our curse, for it draws down upon our every endeavor no end of unforeseeable consequences!\" Klapaucius said in a stentorian voice. \"If an imperfect imitator, wishing to inflict pain, were to build himself a crude idol of wood or wax, and further give it some makeshift semblance of a sentient being, his torture of the thing would be a paltry mockery indeed! But consider a succession of improvements on this practice! Consider the next sculptor, who builds a doll with a recording in its belly, that it may groan beneath his blows; consider a doll which, when beaten, begs for mercy, no longer a crude idol, but a homeostat; consider a doll that sheds tears, a doll that bleeds, a doll that fears death, though it also longs for the peace that only death can bring! Don\\'t you see, when the imitator is perfect, so must be the imitation, and the semblance becomes the truth, the pretense a reality! Trurl, you took an untold number of creatures capable of suffering and abandoned them forever to the rule of a wicked tyrant.... Trurl, you have committed a terrible crime!\"\\n\\n\"Sheer sophistry!\" shouted Trurl, all the louder because he felt the force of his friend\\'s argument. \"Electrons meander not only in our brains, but in phonograph records as well, which proves nothing, and certainly gives no grounds for such hypostatical analogies! The subjects of that monster Excelsius do in fact die when decapitated, sob, fight, and fall in love, since that is how I set up the parameters, but it\\'s impossible to say, Klapaucius, that they feel anything in the process - the electrons jumping around in their heads will tell you nothing of that!\"\\n\\n\"And if I were to look inside your head, I would also see nothing but electrons,\" replied Klapaucius. \"Come now, don\\'t pretend not to understand what I\\'m saying, I know you\\'re not that stupid! A phonograph record won\\'t run errands for you, won\\'t beg for mercy or fall on its knees! You say there\\'s no way of knowing whether Excelsius\\'s subjects groan, when beaten, purely because of the electrons hopping about inside - like wheels grinding out the mimicry of a voice - or whether they really groan, that is, because they honestly experience the pain? A pretty distinction, this! No, Trurl, a sufferer is not one who hands you his suffering, that you may touch it, weigh it, bite it like a coin; a sufferer is one who behaves like a sufferer! Prove to me here and now, once and for all, that they do not feel, that they do not think, that they do not in any way exist as being conscious of their enclosure between the two abysses of oblivion - the abyss before birth and the abyss that follows death - prove this to me, Trurl, and I\\'ll leave you be! Prove that you only imitated suffering, and did not create it!\\n\\n\"You know perfectly well that\\'s impossible,\" answered Trurl quietly. \"Even before I took my instruments in hand, when the box was still empty, I had to anticipate the possibility of precisely such a proof - in order to rule it out. For otherwise the monarch of that kingdom sooner or later would have gotten the impression that his subjects were not real subjects at all, but puppets, marionettes. Try to understand, there was no other way to do it! Anything that would have destroyed in the littlest way the illusion of complete reality would have also destroyed the importance, the dignity of governing, and turned it into nothing but a mechanical game....\"\\n\\n\"I understand, I understand all too well!\" cried Klapaucius. \"Your intentions were the noblest - you only sought to construct a kingdom as lifelike as possible, so similar to a real kingdom, that no one, absolutely no one, could ever tell the difference, and in this, I am afraid, you were successful! Only hours have passed since your return, but for them, the ones imprisoned in that box, whole centuries have gone by - how many beings, how many lives wasted, and all to gratify and feed the vanity of King Excelsius!\"\\n\\nWithout another word Trurl rushed back to his ship, but saw that his friend was coming with him. When he had blasted off into space, pointed the bow between two great clusters of eternal flame and opened the throttle all the way, Klapaucius said:\\n\\n\"Trurl, you\\'re hopeless. You always act first, think later. And now what do you intend to do when we get there?\"\\n\\n\"I\\'ll take the kingdom away from him!\"\\n\\n\"And what will you do with it?\"\\n\\n\"Destroy it!\" Trurl was about to shout, but choked on the first syllable when he realized what he was saying. Finally he mumbled:\\n\\n\"I\\'ll hold an election. Let them choose just rulers from among themselves.\"\\n\\n\"You programmed them all to be feudal lords or shiftless vassals. What good would an election do? First you\\'d have to undo the entire structure of the kingdom, then assemble from scratch ...\"\"And where,\" exclaimed Trurl, \"does the changing of structures end and the tampering with minds begin?!\" Klapaucius had no answer for this, and they flew on in gloomy silence, till the planet of Excelsius came into view. As they circled it, preparing to land, they beheld a most amazing sight.\\n\\nThe entire planet was covered with countless signs of intelligent life. Microscopic bridges, like tiny lines, spanned every rill and rivulet, while the puddles, reflecting the stars, were full of microscopic boats like floating chips.... The night side of the sphere was dotted with glimmering cities, and on the day side one could make out flourishing metropolises, though the inhabitants themselves were much too little to observe, even through the strongest lens. Of the king there was not a trace, as if the earth had swallowed him up.\\n\\n\"He isn\\'t here,\" said Trurl in an awed whisper. \"What have they done with him? Somehow they managed to break through the walls of their box and occupy the asteroid....\"\\n\\n\"Look!\" said Klapaucius, pointing to a little cloud no larger than a thimble and shaped like a mushroom; it slowly rose into the atmosphere. \"They\\'ve discovered atomic energy.... And over there - you see that bit of glass? It\\'s the remains of the box, they\\'ve made it into some sort of temple....\"\\n\\n\"I don\\'t understand. It was only a model, after all. A process with a large number of parameters, a simulation, a mock-up for a monarch to practice on, with the necessary feedback, variables, multistats ...\" muttered Trurl, dumbfounded.\\n\\n\"Yes. But you made the unforgivable mistake of overperfecting your replica. Not wanting to build a mere clocklike mechanism, you inadvertently - in your punctilious way - created that which was possible, logical, and inevitable, that which became the very antithesis of a mechanism....\"\\n\\n\"Please, no more!\" cried Trurl. And they looked out upon the asteroid in silence, when suddenly something bumped their ship, or rather grazed it slightly. They saw this object, for it was illuminated by the thin ribbon of flame that issued from its tail. A ship, probably, or perhaps an artificial satellite, though remarkably similar to one of those steel boots the tyrant Excelsius used to wear. And when the constructors raised their eyes, they beheld a heavenly body shining high above the tiny planet -  it hadn\\'t been there previously - and they recognized, in that cold, pale orb, the stern features of Excelsius himself, who had in this way become the Moon of the Microminians.'\n\n\n\n\n\n\nfrom nltk import word_tokenize, pos_tag\n\nwords = word_tokenize(story)\nwords[:20]\n\n['The',\n 'seventh',\n 'Sally',\n 'or',\n 'how',\n 'Trurl',\n \"'s\",\n 'own',\n 'perfection',\n 'led',\n 'to',\n 'no',\n 'good',\n 'By',\n 'StanisÅ‚aw',\n 'Lem',\n ',',\n '1965',\n '.',\n 'Translated']\n\n\n\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk.corpus import wordnet\n\npalabra = \"change\"\n# Instanciar los objetos\nstemmer = PorterStemmer()\nlemmatizer = WordNetLemmatizer()\n\n# Stemming\nprint(\"STEMMING:\", stemmer.stem(palabra))\n\n# Lemmatization\nprint(\"LEMMATIZATION:\", lemmatizer.lemmatize(palabra, pos=wordnet.VERB))\n\nSTEMMING: chang\nLEMMATIZATION: change\n\n\n\npos = pos_tag(words)\npos[:20]\n\n[('The', 'DT'),\n ('seventh', 'JJ'),\n ('Sally', 'NNP'),\n ('or', 'CC'),\n ('how', 'WRB'),\n ('Trurl', 'NNP'),\n (\"'s\", 'POS'),\n ('own', 'JJ'),\n ('perfection', 'NN'),\n ('led', 'VBD'),\n ('to', 'TO'),\n ('no', 'DT'),\n ('good', 'JJ'),\n ('By', 'IN'),\n ('StanisÅ‚aw', 'NNP'),\n ('Lem', 'NNP'),\n (',', ','),\n ('1965', 'CD'),\n ('.', '.'),\n ('Translated', 'VBN')]\n\n\n\nfrom nltk.corpus import stopwords as stop\n\nfor item in stop.words(\"spanish\"):\n    print(item)\n\nde\nla\nque\nel\nen\ny\na\nlos\ndel\nse\nlas\npor\nun\npara\ncon\nno\nuna\nsu\nal\nlo\ncomo\nmás\npero\nsus\nle\nya\no\neste\nsí\nporque\nesta\nentre\ncuando\nmuy\nsin\nsobre\ntambién\nme\nhasta\nhay\ndonde\nquien\ndesde\ntodo\nnos\ndurante\ntodos\nuno\nles\nni\ncontra\notros\nese\neso\nante\nellos\ne\nesto\nmí\nantes\nalgunos\nqué\nunos\nyo\notro\notras\notra\nél\ntanto\nesa\nestos\nmucho\nquienes\nnada\nmuchos\ncual\npoco\nella\nestar\nestas\nalgunas\nalgo\nnosotros\nmi\nmis\ntú\nte\nti\ntu\ntus\nellas\nnosotras\nvosotros\nvosotras\nos\nmío\nmía\nmíos\nmías\ntuyo\ntuya\ntuyos\ntuyas\nsuyo\nsuya\nsuyos\nsuyas\nnuestro\nnuestra\nnuestros\nnuestras\nvuestro\nvuestra\nvuestros\nvuestras\nesos\nesas\nestoy\nestás\nestá\nestamos\nestáis\nestán\nesté\nestés\nestemos\nestéis\nestén\nestaré\nestarás\nestará\nestaremos\nestaréis\nestarán\nestaría\nestarías\nestaríamos\nestaríais\nestarían\nestaba\nestabas\nestábamos\nestabais\nestaban\nestuve\nestuviste\nestuvo\nestuvimos\nestuvisteis\nestuvieron\nestuviera\nestuvieras\nestuviéramos\nestuvierais\nestuvieran\nestuviese\nestuvieses\nestuviésemos\nestuvieseis\nestuviesen\nestando\nestado\nestada\nestados\nestadas\nestad\nhe\nhas\nha\nhemos\nhabéis\nhan\nhaya\nhayas\nhayamos\nhayáis\nhayan\nhabré\nhabrás\nhabrá\nhabremos\nhabréis\nhabrán\nhabría\nhabrías\nhabríamos\nhabríais\nhabrían\nhabía\nhabías\nhabíamos\nhabíais\nhabían\nhube\nhubiste\nhubo\nhubimos\nhubisteis\nhubieron\nhubiera\nhubieras\nhubiéramos\nhubierais\nhubieran\nhubiese\nhubieses\nhubiésemos\nhubieseis\nhubiesen\nhabiendo\nhabido\nhabida\nhabidos\nhabidas\nsoy\neres\nes\nsomos\nsois\nson\nsea\nseas\nseamos\nseáis\nsean\nseré\nserás\nserá\nseremos\nseréis\nserán\nsería\nserías\nseríamos\nseríais\nserían\nera\neras\néramos\nerais\neran\nfui\nfuiste\nfue\nfuimos\nfuisteis\nfueron\nfuera\nfueras\nfuéramos\nfuerais\nfueran\nfuese\nfueses\nfuésemos\nfueseis\nfuesen\nsintiendo\nsentido\nsentida\nsentidos\nsentidas\nsiente\nsentid\ntengo\ntienes\ntiene\ntenemos\ntenéis\ntienen\ntenga\ntengas\ntengamos\ntengáis\ntengan\ntendré\ntendrás\ntendrá\ntendremos\ntendréis\ntendrán\ntendría\ntendrías\ntendríamos\ntendríais\ntendrían\ntenía\ntenías\nteníamos\nteníais\ntenían\ntuve\ntuviste\ntuvo\ntuvimos\ntuvisteis\ntuvieron\ntuviera\ntuvieras\ntuviéramos\ntuvierais\ntuvieran\ntuviese\ntuvieses\ntuviésemos\ntuvieseis\ntuviesen\nteniendo\ntenido\ntenida\ntenidos\ntenidas\ntened\n\n\n\nfor item in stop.words(\"english\"):\n    print(item)\n\na\nabout\nabove\nafter\nagain\nagainst\nain\nall\nam\nan\nand\nany\nare\naren\naren't\nas\nat\nbe\nbecause\nbeen\nbefore\nbeing\nbelow\nbetween\nboth\nbut\nby\ncan\ncouldn\ncouldn't\nd\ndid\ndidn\ndidn't\ndo\ndoes\ndoesn\ndoesn't\ndoing\ndon\ndon't\ndown\nduring\neach\nfew\nfor\nfrom\nfurther\nhad\nhadn\nhadn't\nhas\nhasn\nhasn't\nhave\nhaven\nhaven't\nhaving\nhe\nhe'd\nhe'll\nher\nhere\nhers\nherself\nhe's\nhim\nhimself\nhis\nhow\ni\ni'd\nif\ni'll\ni'm\nin\ninto\nis\nisn\nisn't\nit\nit'd\nit'll\nit's\nits\nitself\ni've\njust\nll\nm\nma\nme\nmightn\nmightn't\nmore\nmost\nmustn\nmustn't\nmy\nmyself\nneedn\nneedn't\nno\nnor\nnot\nnow\no\nof\noff\non\nonce\nonly\nor\nother\nour\nours\nourselves\nout\nover\nown\nre\ns\nsame\nshan\nshan't\nshe\nshe'd\nshe'll\nshe's\nshould\nshouldn\nshouldn't\nshould've\nso\nsome\nsuch\nt\nthan\nthat\nthat'll\nthe\ntheir\ntheirs\nthem\nthemselves\nthen\nthere\nthese\nthey\nthey'd\nthey'll\nthey're\nthey've\nthis\nthose\nthrough\nto\ntoo\nunder\nuntil\nup\nve\nvery\nwas\nwasn\nwasn't\nwe\nwe'd\nwe'll\nwe're\nwere\nweren\nweren't\nwe've\nwhat\nwhen\nwhere\nwhich\nwhile\nwho\nwhom\nwhy\nwill\nwith\nwon\nwon't\nwouldn\nwouldn't\ny\nyou\nyou'd\nyou'll\nyour\nyou're\nyours\nyourself\nyourselves\nyou've",
    "crumbs": [
      "Natural Language Processing (NLP)",
      "NLTK"
    ]
  },
  {
    "objectID": "nltk.html#importación-de-librerias",
    "href": "nltk.html#importación-de-librerias",
    "title": "Natural Lenguaje ToolKit",
    "section": "",
    "text": "import pandas as pd\nimport requests\n \n# Natural Language Toolkit\nimport nltk\n# downloading some additional packages and corpora\nnltk.download('punkt_tab') # necessary for tokenization\nnltk.download('wordnet') # necessary for lemmatization\nnltk.download('stopwords') # necessary for removal of stop words\nnltk.download('averaged_perceptron_tagger_eng') # necessary for POS tagging\nnltk.download('maxent_ne_chunker' ) # necessary for entity extraction\nnltk.download('omw-1.4') # necessary for lemmatization\nnltk.download('words')\n\n[nltk_data] Downloading package punkt_tab to\n[nltk_data]     /Users/usr-s3c/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n[nltk_data] Downloading package wordnet to /Users/usr-s3c/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/usr-s3c/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n[nltk_data]     /Users/usr-s3c/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package maxent_ne_chunker to\n[nltk_data]     /Users/usr-s3c/nltk_data...\n[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /Users/usr-s3c/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n[nltk_data] Downloading package words to /Users/usr-s3c/nltk_data...\n[nltk_data]   Package words is already up-to-date!\n\n\nTrue",
    "crumbs": [
      "Natural Language Processing (NLP)",
      "NLTK"
    ]
  },
  {
    "objectID": "nltk.html#dataframe-encoder",
    "href": "nltk.html#dataframe-encoder",
    "title": "Natural Lenguaje ToolKit",
    "section": "",
    "text": "Code\nurl = \"https://raw.githubusercontent.com/erickedu85/dataset/master/story.txt\"\nr = requests.get(url)\nr.encoding = \"utf-8\"\n\nstory = r.text\nstory\n\n\n'The seventh Sally or how Trurl\\'s own perfection led to no good\\nBy StanisÅ‚aw Lem, 1965.\\nTranslated by Michael Kandel, 1974.\\n\\nThe Universe is infinite but bounded, and therefore a beam of light, in whatever direction it may travel, will after billions of centuries return -  if powerful enough - to the point of its departure; and it is no different with rumor, that flies about from star to star and makes the rounds of every planet. One day Trurl heard distant reports of two mighty constructor-benefactors, so wise and so accomplished that they had no equal; with this news he ran to Klapaucius, who explained to him that these were not mysterious rivals, but only themselves, for their fame had circumnavigated space. Fame, however, has this fault, that it says nothing of one\\'s failures, even when those very failures are the product of a great perfection. And he who would doubt this, let him recall the last of the seven sallies of Trurl, which was undertaken without Klapaucius, whom certain urgent duties kept at home at the time.\\n\\nIn those days Trurl was exceedingly vain, receiving all marks of veneration and honor paid to him as his due and a perfectly normal thing. He was heading north in his ship, as he was the least familiar with that region, and had flown through the void for quite some time, passing spheres full of the clamor of war as well as spheres that had finally obtained the perfect peace of desolation, when suddenly a little planet came into view, really more of a stray fragment of matter than a planet.\\n\\nOn the surface of this chunk of rock someone was running back and forth, jumping and waving his arms in the strangest way. Astonished by a scene of such total loneliness and concerned by those wild gestures of despair, and perhaps of anger as well, Trurl quickly landed.\\n\\nHe was approached by a personage of tremendous hauteur, iridium and vanadium all over and with a great deal of clanging and clanking, who introduced himself as Excelsius the Tartarian, ruler of Pancreon and Cyspenderora; the inhabitants of both these kingdoms had, in a fit of regicidal madness, driven His Highness from the throne and exiled him to this barren asteroid, eternally adrift among the dark swells and currents of gravitation.\\n\\nLearning in turn the identity of his visitor, the deposed monarch began to insist that Trurl - who after all was something of a professional when it came to good deeds - immediately restore him to his former position. The thought of such a turn of events brought the flame of vengeance to the monarch\\'s eyes, and his iron fingers clutched the air, as if already closing around the throats of his beloved subjects.\\n\\nNow Trurl had no intention of complying with this request of Excelsius, as doing so would bring about untold evil and suffering, yet at the same time he wished somehow to comfort and console the humiliated king. Thinking a moment or two, he came to the conclusion that, even in this case, not all was lost, for it would be possible to satisfy the king completely - without putting his former subjects in jeopardy. And so, rolling up his sleeves and summoning up all his mastery, Trurl built the king an entirely new kingdom. There were plenty of towns, rivers, mountains, forests, and brooks, a sky with clouds, armies full of derring-do, citadels, castles, and ladies\\' chambers; and there were marketplaces, gaudy and gleaming in the sun, days of back-breaking labor, nights full of dancing and song until dawn, and the gay clatter of swordplay. Trurl also carefully set into this kingdom a fabulous capital, all in marble and alabaster, and assembled a council of hoary sages, and winter palaces and summer villas, plots, conspirators, false witnesses, nurses, informers, teams of magnificent steeds, and plumes waving crimson in the wind; and then he crisscrossed that atmosphere with silver fanfares and twenty-one gun salutes, also threw in the necessary handful of traitors, another of heroes, added a pinch of prophets and seers, and one messiah and one great poet each, after which he bent over and set the works in motion, deftly making last-minute adjustments with his microscopic tools as it ran, and he gave the women of that kingdom beauty, the men - sullen silence and surliness when drunk, the officials - arrogance and servility, the astronomers - an enthusiasm for stars, and the children - a great capacity for noise. And all of this, connected, mounted and ground to precision, fit into a box, and not a very large box, but just the size that could be carried about with ease. This Trurl presented to Excelsius, to rule and have dominion over forever; but first he showed him where the input and output of his brand-new kingdom were, and how to program wars, quell rebellions, exact tribute, collect taxes, and also instructed him in the critical points and transition states of that microminiaturized society - in other words the maxima and minima of palace coups and revolutions -  and explained everything so well that the king, an old hand in the running of tyrannies, instantly grasped the directions and, without hesitation, while the constructor watched, issued a few trial proclamations, correctly manipulating the control knobs, which were carved with imperial eagles and regal lions. These proclamations declared a state of emergency, martial law, a curfew, and a special levy. After a year had passed in the kingdom, which amounted to hardly a minute for Trurl and the king, by an act of the greatest magnanimity - that is, by a flick of the finger at the controls - the king abolished one death penalty, lightened the levy, and deigned to annul the state of emergency, whereupon a tumultuous cry of gratitude, like the squeaking of tiny mice lifted by their tails, rose up from the box, and through its curved glass cover one could see, on the dusty highways and along the banks of lazy rivers that reflected the fluffy clouds, the people rejoicing and praising the great and unsurpassed benevolence of their sovereign lord.\\n\\nAnd so, though at first he had felt insulted by Trurl\\'s gift, in that the kingdom was too small and very like a child\\'s toy, the monarch saw that the thick glass lid made everything inside seem large; perhaps too he dully understood that size was not what mattered here, for government is not measured in meters and kilograms, and emotions are somehow the same, whether experienced by giants or dwarfs - and so he thanked the constructor, if somewhat stiffly. Who knows, he might even have liked to order him thrown in chains and tortured to death, just to be safe - that would have been a sure way of nipping in the bud any gossip about how some common vagabond tinkerer presented a mighty monarch with a kingdom. Excelsius was sensible enough, however, to see that this was out of the question, owing to a very fundamental disproportion, for fleas could sooner take their host into captivity than the king\\'s army seize Trurl. So with another cold nod, he stuck his orb and scepter under his arm, lifted the box kingdom with a grunt, and took it to his humble hut of exile. And as blazing day alternated with murky night outside, according to the rhythm of the asteroid\\'s rotation, the king, who was acknowledged by his subjects as the greatest in the world, diligently reigned, bidding this, forbidding that, beheading, rewarding - in all these ways incessantly spurring his little ones on to perfect fealty and worship of the throne.\\n\\nAs for Trurl, he returned home and related to his friend Klapaucius, not without pride, how he had employed his constructor\\'s genius to indulge the autocratic aspirations of Excelsius and, at the same time, safeguard the democratic aspirations of his former subjects. But Klapaucius, surprisingly enough, had no words of praise for Trurl; in fact, there seemed to be rebuke in his expression.\\n\\n\"Have I understood you correctly?\" he said at last. \"You gave that brutal despot, that born slave master, that slavering sadist of a pain- monger, you gave him a whole civilization to rule and have dominion over forever? And you tell me, moreover, of the cries of joy brought on by the repeal of a fraction of his cruel decrees! Trurl, how could you have done such a thing?\"\\n\\n\"You must be joking!\" Trurl exclaimed. \"Really, the whole kingdom fits into a box three feet by two by two and a half ... it\\'s only a model....\"\\n\\n\"A model of what?\"\\n\\n\"What do you mean, of what? Of a civilization, obviously, except that it\\'s a hundred million times smaller.â€\\x9d\\n\\n\"And how do you know there aren\\'t civilizations a hundred million times larger than our own? And if there were, would ours then be a model? And what importance do dimensions have anyway? In that box kingdom, doesn\\'t a journey from the capital to one of the corners take months - for those inhabitants? And don\\'t they suffer, don\\'t they know the burden of labor, don\\'t they die?\"\\n\\n\"Now just a minute, you know yourself that all these processes take place only because I programmed them, and so they aren\\'t genuine....\"\\n\\n\"Aren\\'t genuine? You mean to say the box is empty, and the parades, tortures, and beheadings are merely an illusion?\"\\n\\n\"Not an illusion, no, since they have reality, though purely as certain microscopic phenomena, which I produced by manipulating atoms,\" said Trurl. \"The point is, these births, loves, acts of heroism, and denunciations are nothing but the minuscule capering of electrons in space, precisely arranged by the skill of my nonlinear craft, which - \"\\n\\n\"Enough of your boasting, not another word!\" Klapaucius snapped. \"Are these processes self-organizing or not?\"\\n\\n\"Of course they are!\"\\n\\n\"And they occur among infinitesimal clouds of electrical charge?\"\\n\\n\"You know they do.\"\"And the phenomenological events of dawns, sunsets, and bloody battles are generated by the concatenation of real variables?\"\\n\\n\"Certainly.\"\\n\\n\"And are not we as well, if you examine us physically, mechanistically, statistically, and meticulously, nothing but the miniscule capering of electron clouds? Positive and negative charges arranged in space? And is our existence not the result of subatomic collisions and the interplay of particles, though we ourselves perceive those molecular cartwheels as fear, longing, or meditation? And when you daydream, what transpires within your brain but the binary algebra of connecting and disconnecting circuits, the continual meandering of electrons?\"\\n\\n\"What, Klapaucius, would you equate our existence with that of an imitation kingdom locked up in some glass box?!\" cried Trurl. \"No, really, that\\'s going too far! My purpose was simply to fashion a simulator of statehood, a model cybernetically perfect, nothing more!\"\\n\\n\"Trurl! Our perfection is our curse, for it draws down upon our every endeavor no end of unforeseeable consequences!\" Klapaucius said in a stentorian voice. \"If an imperfect imitator, wishing to inflict pain, were to build himself a crude idol of wood or wax, and further give it some makeshift semblance of a sentient being, his torture of the thing would be a paltry mockery indeed! But consider a succession of improvements on this practice! Consider the next sculptor, who builds a doll with a recording in its belly, that it may groan beneath his blows; consider a doll which, when beaten, begs for mercy, no longer a crude idol, but a homeostat; consider a doll that sheds tears, a doll that bleeds, a doll that fears death, though it also longs for the peace that only death can bring! Don\\'t you see, when the imitator is perfect, so must be the imitation, and the semblance becomes the truth, the pretense a reality! Trurl, you took an untold number of creatures capable of suffering and abandoned them forever to the rule of a wicked tyrant.... Trurl, you have committed a terrible crime!\"\\n\\n\"Sheer sophistry!\" shouted Trurl, all the louder because he felt the force of his friend\\'s argument. \"Electrons meander not only in our brains, but in phonograph records as well, which proves nothing, and certainly gives no grounds for such hypostatical analogies! The subjects of that monster Excelsius do in fact die when decapitated, sob, fight, and fall in love, since that is how I set up the parameters, but it\\'s impossible to say, Klapaucius, that they feel anything in the process - the electrons jumping around in their heads will tell you nothing of that!\"\\n\\n\"And if I were to look inside your head, I would also see nothing but electrons,\" replied Klapaucius. \"Come now, don\\'t pretend not to understand what I\\'m saying, I know you\\'re not that stupid! A phonograph record won\\'t run errands for you, won\\'t beg for mercy or fall on its knees! You say there\\'s no way of knowing whether Excelsius\\'s subjects groan, when beaten, purely because of the electrons hopping about inside - like wheels grinding out the mimicry of a voice - or whether they really groan, that is, because they honestly experience the pain? A pretty distinction, this! No, Trurl, a sufferer is not one who hands you his suffering, that you may touch it, weigh it, bite it like a coin; a sufferer is one who behaves like a sufferer! Prove to me here and now, once and for all, that they do not feel, that they do not think, that they do not in any way exist as being conscious of their enclosure between the two abysses of oblivion - the abyss before birth and the abyss that follows death - prove this to me, Trurl, and I\\'ll leave you be! Prove that you only imitated suffering, and did not create it!\\n\\n\"You know perfectly well that\\'s impossible,\" answered Trurl quietly. \"Even before I took my instruments in hand, when the box was still empty, I had to anticipate the possibility of precisely such a proof - in order to rule it out. For otherwise the monarch of that kingdom sooner or later would have gotten the impression that his subjects were not real subjects at all, but puppets, marionettes. Try to understand, there was no other way to do it! Anything that would have destroyed in the littlest way the illusion of complete reality would have also destroyed the importance, the dignity of governing, and turned it into nothing but a mechanical game....\"\\n\\n\"I understand, I understand all too well!\" cried Klapaucius. \"Your intentions were the noblest - you only sought to construct a kingdom as lifelike as possible, so similar to a real kingdom, that no one, absolutely no one, could ever tell the difference, and in this, I am afraid, you were successful! Only hours have passed since your return, but for them, the ones imprisoned in that box, whole centuries have gone by - how many beings, how many lives wasted, and all to gratify and feed the vanity of King Excelsius!\"\\n\\nWithout another word Trurl rushed back to his ship, but saw that his friend was coming with him. When he had blasted off into space, pointed the bow between two great clusters of eternal flame and opened the throttle all the way, Klapaucius said:\\n\\n\"Trurl, you\\'re hopeless. You always act first, think later. And now what do you intend to do when we get there?\"\\n\\n\"I\\'ll take the kingdom away from him!\"\\n\\n\"And what will you do with it?\"\\n\\n\"Destroy it!\" Trurl was about to shout, but choked on the first syllable when he realized what he was saying. Finally he mumbled:\\n\\n\"I\\'ll hold an election. Let them choose just rulers from among themselves.\"\\n\\n\"You programmed them all to be feudal lords or shiftless vassals. What good would an election do? First you\\'d have to undo the entire structure of the kingdom, then assemble from scratch ...\"\"And where,\" exclaimed Trurl, \"does the changing of structures end and the tampering with minds begin?!\" Klapaucius had no answer for this, and they flew on in gloomy silence, till the planet of Excelsius came into view. As they circled it, preparing to land, they beheld a most amazing sight.\\n\\nThe entire planet was covered with countless signs of intelligent life. Microscopic bridges, like tiny lines, spanned every rill and rivulet, while the puddles, reflecting the stars, were full of microscopic boats like floating chips.... The night side of the sphere was dotted with glimmering cities, and on the day side one could make out flourishing metropolises, though the inhabitants themselves were much too little to observe, even through the strongest lens. Of the king there was not a trace, as if the earth had swallowed him up.\\n\\n\"He isn\\'t here,\" said Trurl in an awed whisper. \"What have they done with him? Somehow they managed to break through the walls of their box and occupy the asteroid....\"\\n\\n\"Look!\" said Klapaucius, pointing to a little cloud no larger than a thimble and shaped like a mushroom; it slowly rose into the atmosphere. \"They\\'ve discovered atomic energy.... And over there - you see that bit of glass? It\\'s the remains of the box, they\\'ve made it into some sort of temple....\"\\n\\n\"I don\\'t understand. It was only a model, after all. A process with a large number of parameters, a simulation, a mock-up for a monarch to practice on, with the necessary feedback, variables, multistats ...\" muttered Trurl, dumbfounded.\\n\\n\"Yes. But you made the unforgivable mistake of overperfecting your replica. Not wanting to build a mere clocklike mechanism, you inadvertently - in your punctilious way - created that which was possible, logical, and inevitable, that which became the very antithesis of a mechanism....\"\\n\\n\"Please, no more!\" cried Trurl. And they looked out upon the asteroid in silence, when suddenly something bumped their ship, or rather grazed it slightly. They saw this object, for it was illuminated by the thin ribbon of flame that issued from its tail. A ship, probably, or perhaps an artificial satellite, though remarkably similar to one of those steel boots the tyrant Excelsius used to wear. And when the constructors raised their eyes, they beheld a heavenly body shining high above the tiny planet -  it hadn\\'t been there previously - and they recognized, in that cold, pale orb, the stern features of Excelsius himself, who had in this way become the Moon of the Microminians.'"
  },
  {
    "objectID": "nltk.html#label-encoder-usar-solo-en-output",
    "href": "nltk.html#label-encoder-usar-solo-en-output",
    "title": "Categorical Encoding",
    "section": "",
    "text": "Code\ndf['label_encoded']=LabelEncoder().fit_transform(df['label'])\ndf\n\n\n\n\n\n\n\n\n\nsize\ncolor\nprice\nlabel\nlabel_encoded\n\n\n\n\n0\nM\ngreen\n10.1\npantalones\n1\n\n\n1\nS\nred\n13.5\ncamisetas\n0\n\n\n2\nL\nblue\n15.6\ncamisetas\n0"
  },
  {
    "objectID": "nltk.html#ordinal-encoder",
    "href": "nltk.html#ordinal-encoder",
    "title": "Categorical Encoding",
    "section": "",
    "text": "Code\nsize_order = [['S','M','L']]\ndf['size_encoded']=OrdinalEncoder(categories=size_order).fit_transform(df[['size']])\ndf\n\n\n\n\n\n\n\n\n\nsize\ncolor\nprice\nlabel\nlabel_encoded\nsize_encoded\n\n\n\n\n0\nM\ngreen\n10.1\npantalones\n1\n1.0\n\n\n1\nS\nred\n13.5\ncamisetas\n0\n0.0\n\n\n2\nL\nblue\n15.6\ncamisetas\n0\n2.0"
  },
  {
    "objectID": "nltk.html#onehotencoder",
    "href": "nltk.html#onehotencoder",
    "title": "Categorical Encoding",
    "section": "",
    "text": "Code\ncolor_encoder = OneHotEncoder (sparse_output=False, handle_unknown='ignore')\ncolor_encoded = color_encoder.fit_transform(df[['color']])\ncolor_title_encoded = color_encoder.get_feature_names_out(['color'])\n\ndf[color_title_encoded] = color_encoded\ndf\n\n\n\n\n\n\n\n\n\nsize\ncolor\nprice\nlabel\nlabel_encoded\nsize_encoded\ncolor_blue\ncolor_green\ncolor_red\n\n\n\n\n0\nM\ngreen\n10.1\npantalones\n1\n1.0\n0.0\n1.0\n0.0\n\n\n1\nS\nred\n13.5\ncamisetas\n0\n0.0\n0.0\n0.0\n1.0\n\n\n2\nL\nblue\n15.6\ncamisetas\n0\n2.0\n1.0\n0.0\n0.0"
  },
  {
    "objectID": "nltk.html#column-transformer",
    "href": "nltk.html#column-transformer",
    "title": "Categorical Encoding",
    "section": "",
    "text": "Code\ncategoricas_features = ['color']\nordinal_features = ['size']\nnumerical_features = ['price']\n\nsize_orde = [['S','M','L']]\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('color_onehot', OneHotEncoder(),categoricas_features),\n        ('size_ord',OrdinalEncoder(categories=size_orde), ordinal_features),\n        ('price_stan',StandardScaler(),numerical_features)\n    ]\n)\n\n\n\n\n\n\n\nCode\nfeature_processor = preprocessor.fit_transform(df)\nfeature_processor\n\n\narray([[ 0.        ,  1.        ,  0.        ,  1.        , -1.30910667],\n       [ 0.        ,  0.        ,  1.        ,  0.        ,  0.19121783],\n       [ 1.        ,  0.        ,  0.        ,  2.        ,  1.11788884]])"
  },
  {
    "objectID": "nltk.html#dataframe-nltk",
    "href": "nltk.html#dataframe-nltk",
    "title": "Natural Lenguaje ToolKit",
    "section": "",
    "text": "url = \"https://raw.githubusercontent.com/erickedu85/dataset/master/story.txt\"\nr = requests.get(url)\nr.encoding = \"utf-8\"\n\nstory = r.text\nstory\n\n'The seventh Sally or how Trurl\\'s own perfection led to no good\\nBy StanisÅ‚aw Lem, 1965.\\nTranslated by Michael Kandel, 1974.\\n\\nThe Universe is infinite but bounded, and therefore a beam of light, in whatever direction it may travel, will after billions of centuries return -  if powerful enough - to the point of its departure; and it is no different with rumor, that flies about from star to star and makes the rounds of every planet. One day Trurl heard distant reports of two mighty constructor-benefactors, so wise and so accomplished that they had no equal; with this news he ran to Klapaucius, who explained to him that these were not mysterious rivals, but only themselves, for their fame had circumnavigated space. Fame, however, has this fault, that it says nothing of one\\'s failures, even when those very failures are the product of a great perfection. And he who would doubt this, let him recall the last of the seven sallies of Trurl, which was undertaken without Klapaucius, whom certain urgent duties kept at home at the time.\\n\\nIn those days Trurl was exceedingly vain, receiving all marks of veneration and honor paid to him as his due and a perfectly normal thing. He was heading north in his ship, as he was the least familiar with that region, and had flown through the void for quite some time, passing spheres full of the clamor of war as well as spheres that had finally obtained the perfect peace of desolation, when suddenly a little planet came into view, really more of a stray fragment of matter than a planet.\\n\\nOn the surface of this chunk of rock someone was running back and forth, jumping and waving his arms in the strangest way. Astonished by a scene of such total loneliness and concerned by those wild gestures of despair, and perhaps of anger as well, Trurl quickly landed.\\n\\nHe was approached by a personage of tremendous hauteur, iridium and vanadium all over and with a great deal of clanging and clanking, who introduced himself as Excelsius the Tartarian, ruler of Pancreon and Cyspenderora; the inhabitants of both these kingdoms had, in a fit of regicidal madness, driven His Highness from the throne and exiled him to this barren asteroid, eternally adrift among the dark swells and currents of gravitation.\\n\\nLearning in turn the identity of his visitor, the deposed monarch began to insist that Trurl - who after all was something of a professional when it came to good deeds - immediately restore him to his former position. The thought of such a turn of events brought the flame of vengeance to the monarch\\'s eyes, and his iron fingers clutched the air, as if already closing around the throats of his beloved subjects.\\n\\nNow Trurl had no intention of complying with this request of Excelsius, as doing so would bring about untold evil and suffering, yet at the same time he wished somehow to comfort and console the humiliated king. Thinking a moment or two, he came to the conclusion that, even in this case, not all was lost, for it would be possible to satisfy the king completely - without putting his former subjects in jeopardy. And so, rolling up his sleeves and summoning up all his mastery, Trurl built the king an entirely new kingdom. There were plenty of towns, rivers, mountains, forests, and brooks, a sky with clouds, armies full of derring-do, citadels, castles, and ladies\\' chambers; and there were marketplaces, gaudy and gleaming in the sun, days of back-breaking labor, nights full of dancing and song until dawn, and the gay clatter of swordplay. Trurl also carefully set into this kingdom a fabulous capital, all in marble and alabaster, and assembled a council of hoary sages, and winter palaces and summer villas, plots, conspirators, false witnesses, nurses, informers, teams of magnificent steeds, and plumes waving crimson in the wind; and then he crisscrossed that atmosphere with silver fanfares and twenty-one gun salutes, also threw in the necessary handful of traitors, another of heroes, added a pinch of prophets and seers, and one messiah and one great poet each, after which he bent over and set the works in motion, deftly making last-minute adjustments with his microscopic tools as it ran, and he gave the women of that kingdom beauty, the men - sullen silence and surliness when drunk, the officials - arrogance and servility, the astronomers - an enthusiasm for stars, and the children - a great capacity for noise. And all of this, connected, mounted and ground to precision, fit into a box, and not a very large box, but just the size that could be carried about with ease. This Trurl presented to Excelsius, to rule and have dominion over forever; but first he showed him where the input and output of his brand-new kingdom were, and how to program wars, quell rebellions, exact tribute, collect taxes, and also instructed him in the critical points and transition states of that microminiaturized society - in other words the maxima and minima of palace coups and revolutions -  and explained everything so well that the king, an old hand in the running of tyrannies, instantly grasped the directions and, without hesitation, while the constructor watched, issued a few trial proclamations, correctly manipulating the control knobs, which were carved with imperial eagles and regal lions. These proclamations declared a state of emergency, martial law, a curfew, and a special levy. After a year had passed in the kingdom, which amounted to hardly a minute for Trurl and the king, by an act of the greatest magnanimity - that is, by a flick of the finger at the controls - the king abolished one death penalty, lightened the levy, and deigned to annul the state of emergency, whereupon a tumultuous cry of gratitude, like the squeaking of tiny mice lifted by their tails, rose up from the box, and through its curved glass cover one could see, on the dusty highways and along the banks of lazy rivers that reflected the fluffy clouds, the people rejoicing and praising the great and unsurpassed benevolence of their sovereign lord.\\n\\nAnd so, though at first he had felt insulted by Trurl\\'s gift, in that the kingdom was too small and very like a child\\'s toy, the monarch saw that the thick glass lid made everything inside seem large; perhaps too he dully understood that size was not what mattered here, for government is not measured in meters and kilograms, and emotions are somehow the same, whether experienced by giants or dwarfs - and so he thanked the constructor, if somewhat stiffly. Who knows, he might even have liked to order him thrown in chains and tortured to death, just to be safe - that would have been a sure way of nipping in the bud any gossip about how some common vagabond tinkerer presented a mighty monarch with a kingdom. Excelsius was sensible enough, however, to see that this was out of the question, owing to a very fundamental disproportion, for fleas could sooner take their host into captivity than the king\\'s army seize Trurl. So with another cold nod, he stuck his orb and scepter under his arm, lifted the box kingdom with a grunt, and took it to his humble hut of exile. And as blazing day alternated with murky night outside, according to the rhythm of the asteroid\\'s rotation, the king, who was acknowledged by his subjects as the greatest in the world, diligently reigned, bidding this, forbidding that, beheading, rewarding - in all these ways incessantly spurring his little ones on to perfect fealty and worship of the throne.\\n\\nAs for Trurl, he returned home and related to his friend Klapaucius, not without pride, how he had employed his constructor\\'s genius to indulge the autocratic aspirations of Excelsius and, at the same time, safeguard the democratic aspirations of his former subjects. But Klapaucius, surprisingly enough, had no words of praise for Trurl; in fact, there seemed to be rebuke in his expression.\\n\\n\"Have I understood you correctly?\" he said at last. \"You gave that brutal despot, that born slave master, that slavering sadist of a pain- monger, you gave him a whole civilization to rule and have dominion over forever? And you tell me, moreover, of the cries of joy brought on by the repeal of a fraction of his cruel decrees! Trurl, how could you have done such a thing?\"\\n\\n\"You must be joking!\" Trurl exclaimed. \"Really, the whole kingdom fits into a box three feet by two by two and a half ... it\\'s only a model....\"\\n\\n\"A model of what?\"\\n\\n\"What do you mean, of what? Of a civilization, obviously, except that it\\'s a hundred million times smaller.â€\\x9d\\n\\n\"And how do you know there aren\\'t civilizations a hundred million times larger than our own? And if there were, would ours then be a model? And what importance do dimensions have anyway? In that box kingdom, doesn\\'t a journey from the capital to one of the corners take months - for those inhabitants? And don\\'t they suffer, don\\'t they know the burden of labor, don\\'t they die?\"\\n\\n\"Now just a minute, you know yourself that all these processes take place only because I programmed them, and so they aren\\'t genuine....\"\\n\\n\"Aren\\'t genuine? You mean to say the box is empty, and the parades, tortures, and beheadings are merely an illusion?\"\\n\\n\"Not an illusion, no, since they have reality, though purely as certain microscopic phenomena, which I produced by manipulating atoms,\" said Trurl. \"The point is, these births, loves, acts of heroism, and denunciations are nothing but the minuscule capering of electrons in space, precisely arranged by the skill of my nonlinear craft, which - \"\\n\\n\"Enough of your boasting, not another word!\" Klapaucius snapped. \"Are these processes self-organizing or not?\"\\n\\n\"Of course they are!\"\\n\\n\"And they occur among infinitesimal clouds of electrical charge?\"\\n\\n\"You know they do.\"\"And the phenomenological events of dawns, sunsets, and bloody battles are generated by the concatenation of real variables?\"\\n\\n\"Certainly.\"\\n\\n\"And are not we as well, if you examine us physically, mechanistically, statistically, and meticulously, nothing but the miniscule capering of electron clouds? Positive and negative charges arranged in space? And is our existence not the result of subatomic collisions and the interplay of particles, though we ourselves perceive those molecular cartwheels as fear, longing, or meditation? And when you daydream, what transpires within your brain but the binary algebra of connecting and disconnecting circuits, the continual meandering of electrons?\"\\n\\n\"What, Klapaucius, would you equate our existence with that of an imitation kingdom locked up in some glass box?!\" cried Trurl. \"No, really, that\\'s going too far! My purpose was simply to fashion a simulator of statehood, a model cybernetically perfect, nothing more!\"\\n\\n\"Trurl! Our perfection is our curse, for it draws down upon our every endeavor no end of unforeseeable consequences!\" Klapaucius said in a stentorian voice. \"If an imperfect imitator, wishing to inflict pain, were to build himself a crude idol of wood or wax, and further give it some makeshift semblance of a sentient being, his torture of the thing would be a paltry mockery indeed! But consider a succession of improvements on this practice! Consider the next sculptor, who builds a doll with a recording in its belly, that it may groan beneath his blows; consider a doll which, when beaten, begs for mercy, no longer a crude idol, but a homeostat; consider a doll that sheds tears, a doll that bleeds, a doll that fears death, though it also longs for the peace that only death can bring! Don\\'t you see, when the imitator is perfect, so must be the imitation, and the semblance becomes the truth, the pretense a reality! Trurl, you took an untold number of creatures capable of suffering and abandoned them forever to the rule of a wicked tyrant.... Trurl, you have committed a terrible crime!\"\\n\\n\"Sheer sophistry!\" shouted Trurl, all the louder because he felt the force of his friend\\'s argument. \"Electrons meander not only in our brains, but in phonograph records as well, which proves nothing, and certainly gives no grounds for such hypostatical analogies! The subjects of that monster Excelsius do in fact die when decapitated, sob, fight, and fall in love, since that is how I set up the parameters, but it\\'s impossible to say, Klapaucius, that they feel anything in the process - the electrons jumping around in their heads will tell you nothing of that!\"\\n\\n\"And if I were to look inside your head, I would also see nothing but electrons,\" replied Klapaucius. \"Come now, don\\'t pretend not to understand what I\\'m saying, I know you\\'re not that stupid! A phonograph record won\\'t run errands for you, won\\'t beg for mercy or fall on its knees! You say there\\'s no way of knowing whether Excelsius\\'s subjects groan, when beaten, purely because of the electrons hopping about inside - like wheels grinding out the mimicry of a voice - or whether they really groan, that is, because they honestly experience the pain? A pretty distinction, this! No, Trurl, a sufferer is not one who hands you his suffering, that you may touch it, weigh it, bite it like a coin; a sufferer is one who behaves like a sufferer! Prove to me here and now, once and for all, that they do not feel, that they do not think, that they do not in any way exist as being conscious of their enclosure between the two abysses of oblivion - the abyss before birth and the abyss that follows death - prove this to me, Trurl, and I\\'ll leave you be! Prove that you only imitated suffering, and did not create it!\\n\\n\"You know perfectly well that\\'s impossible,\" answered Trurl quietly. \"Even before I took my instruments in hand, when the box was still empty, I had to anticipate the possibility of precisely such a proof - in order to rule it out. For otherwise the monarch of that kingdom sooner or later would have gotten the impression that his subjects were not real subjects at all, but puppets, marionettes. Try to understand, there was no other way to do it! Anything that would have destroyed in the littlest way the illusion of complete reality would have also destroyed the importance, the dignity of governing, and turned it into nothing but a mechanical game....\"\\n\\n\"I understand, I understand all too well!\" cried Klapaucius. \"Your intentions were the noblest - you only sought to construct a kingdom as lifelike as possible, so similar to a real kingdom, that no one, absolutely no one, could ever tell the difference, and in this, I am afraid, you were successful! Only hours have passed since your return, but for them, the ones imprisoned in that box, whole centuries have gone by - how many beings, how many lives wasted, and all to gratify and feed the vanity of King Excelsius!\"\\n\\nWithout another word Trurl rushed back to his ship, but saw that his friend was coming with him. When he had blasted off into space, pointed the bow between two great clusters of eternal flame and opened the throttle all the way, Klapaucius said:\\n\\n\"Trurl, you\\'re hopeless. You always act first, think later. And now what do you intend to do when we get there?\"\\n\\n\"I\\'ll take the kingdom away from him!\"\\n\\n\"And what will you do with it?\"\\n\\n\"Destroy it!\" Trurl was about to shout, but choked on the first syllable when he realized what he was saying. Finally he mumbled:\\n\\n\"I\\'ll hold an election. Let them choose just rulers from among themselves.\"\\n\\n\"You programmed them all to be feudal lords or shiftless vassals. What good would an election do? First you\\'d have to undo the entire structure of the kingdom, then assemble from scratch ...\"\"And where,\" exclaimed Trurl, \"does the changing of structures end and the tampering with minds begin?!\" Klapaucius had no answer for this, and they flew on in gloomy silence, till the planet of Excelsius came into view. As they circled it, preparing to land, they beheld a most amazing sight.\\n\\nThe entire planet was covered with countless signs of intelligent life. Microscopic bridges, like tiny lines, spanned every rill and rivulet, while the puddles, reflecting the stars, were full of microscopic boats like floating chips.... The night side of the sphere was dotted with glimmering cities, and on the day side one could make out flourishing metropolises, though the inhabitants themselves were much too little to observe, even through the strongest lens. Of the king there was not a trace, as if the earth had swallowed him up.\\n\\n\"He isn\\'t here,\" said Trurl in an awed whisper. \"What have they done with him? Somehow they managed to break through the walls of their box and occupy the asteroid....\"\\n\\n\"Look!\" said Klapaucius, pointing to a little cloud no larger than a thimble and shaped like a mushroom; it slowly rose into the atmosphere. \"They\\'ve discovered atomic energy.... And over there - you see that bit of glass? It\\'s the remains of the box, they\\'ve made it into some sort of temple....\"\\n\\n\"I don\\'t understand. It was only a model, after all. A process with a large number of parameters, a simulation, a mock-up for a monarch to practice on, with the necessary feedback, variables, multistats ...\" muttered Trurl, dumbfounded.\\n\\n\"Yes. But you made the unforgivable mistake of overperfecting your replica. Not wanting to build a mere clocklike mechanism, you inadvertently - in your punctilious way - created that which was possible, logical, and inevitable, that which became the very antithesis of a mechanism....\"\\n\\n\"Please, no more!\" cried Trurl. And they looked out upon the asteroid in silence, when suddenly something bumped their ship, or rather grazed it slightly. They saw this object, for it was illuminated by the thin ribbon of flame that issued from its tail. A ship, probably, or perhaps an artificial satellite, though remarkably similar to one of those steel boots the tyrant Excelsius used to wear. And when the constructors raised their eyes, they beheld a heavenly body shining high above the tiny planet -  it hadn\\'t been there previously - and they recognized, in that cold, pale orb, the stern features of Excelsius himself, who had in this way become the Moon of the Microminians.'",
    "crumbs": [
      "Natural Language Processing (NLP)",
      "NLTK"
    ]
  },
  {
    "objectID": "nltk.html#tokenización",
    "href": "nltk.html#tokenización",
    "title": "Natural Lenguaje ToolKit",
    "section": "",
    "text": "from nltk import word_tokenize, pos_tag\n\nwords = word_tokenize(story)\nwords[:20]\n\n['The',\n 'seventh',\n 'Sally',\n 'or',\n 'how',\n 'Trurl',\n \"'s\",\n 'own',\n 'perfection',\n 'led',\n 'to',\n 'no',\n 'good',\n 'By',\n 'StanisÅ‚aw',\n 'Lem',\n ',',\n '1965',\n '.',\n 'Translated']\n\n\n\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk.corpus import wordnet\n\npalabra = \"change\"\n# Instanciar los objetos\nstemmer = PorterStemmer()\nlemmatizer = WordNetLemmatizer()\n\n# Stemming\nprint(\"STEMMING:\", stemmer.stem(palabra))\n\n# Lemmatization\nprint(\"LEMMATIZATION:\", lemmatizer.lemmatize(palabra, pos=wordnet.VERB))\n\nSTEMMING: chang\nLEMMATIZATION: change\n\n\n\npos = pos_tag(words)\npos[:20]\n\n[('The', 'DT'),\n ('seventh', 'JJ'),\n ('Sally', 'NNP'),\n ('or', 'CC'),\n ('how', 'WRB'),\n ('Trurl', 'NNP'),\n (\"'s\", 'POS'),\n ('own', 'JJ'),\n ('perfection', 'NN'),\n ('led', 'VBD'),\n ('to', 'TO'),\n ('no', 'DT'),\n ('good', 'JJ'),\n ('By', 'IN'),\n ('StanisÅ‚aw', 'NNP'),\n ('Lem', 'NNP'),\n (',', ','),\n ('1965', 'CD'),\n ('.', '.'),\n ('Translated', 'VBN')]\n\n\n\nfrom nltk.corpus import stopwords as stop\n\nfor item in stop.words(\"spanish\"):\n    print(item)\n\nde\nla\nque\nel\nen\ny\na\nlos\ndel\nse\nlas\npor\nun\npara\ncon\nno\nuna\nsu\nal\nlo\ncomo\nmás\npero\nsus\nle\nya\no\neste\nsí\nporque\nesta\nentre\ncuando\nmuy\nsin\nsobre\ntambién\nme\nhasta\nhay\ndonde\nquien\ndesde\ntodo\nnos\ndurante\ntodos\nuno\nles\nni\ncontra\notros\nese\neso\nante\nellos\ne\nesto\nmí\nantes\nalgunos\nqué\nunos\nyo\notro\notras\notra\nél\ntanto\nesa\nestos\nmucho\nquienes\nnada\nmuchos\ncual\npoco\nella\nestar\nestas\nalgunas\nalgo\nnosotros\nmi\nmis\ntú\nte\nti\ntu\ntus\nellas\nnosotras\nvosotros\nvosotras\nos\nmío\nmía\nmíos\nmías\ntuyo\ntuya\ntuyos\ntuyas\nsuyo\nsuya\nsuyos\nsuyas\nnuestro\nnuestra\nnuestros\nnuestras\nvuestro\nvuestra\nvuestros\nvuestras\nesos\nesas\nestoy\nestás\nestá\nestamos\nestáis\nestán\nesté\nestés\nestemos\nestéis\nestén\nestaré\nestarás\nestará\nestaremos\nestaréis\nestarán\nestaría\nestarías\nestaríamos\nestaríais\nestarían\nestaba\nestabas\nestábamos\nestabais\nestaban\nestuve\nestuviste\nestuvo\nestuvimos\nestuvisteis\nestuvieron\nestuviera\nestuvieras\nestuviéramos\nestuvierais\nestuvieran\nestuviese\nestuvieses\nestuviésemos\nestuvieseis\nestuviesen\nestando\nestado\nestada\nestados\nestadas\nestad\nhe\nhas\nha\nhemos\nhabéis\nhan\nhaya\nhayas\nhayamos\nhayáis\nhayan\nhabré\nhabrás\nhabrá\nhabremos\nhabréis\nhabrán\nhabría\nhabrías\nhabríamos\nhabríais\nhabrían\nhabía\nhabías\nhabíamos\nhabíais\nhabían\nhube\nhubiste\nhubo\nhubimos\nhubisteis\nhubieron\nhubiera\nhubieras\nhubiéramos\nhubierais\nhubieran\nhubiese\nhubieses\nhubiésemos\nhubieseis\nhubiesen\nhabiendo\nhabido\nhabida\nhabidos\nhabidas\nsoy\neres\nes\nsomos\nsois\nson\nsea\nseas\nseamos\nseáis\nsean\nseré\nserás\nserá\nseremos\nseréis\nserán\nsería\nserías\nseríamos\nseríais\nserían\nera\neras\néramos\nerais\neran\nfui\nfuiste\nfue\nfuimos\nfuisteis\nfueron\nfuera\nfueras\nfuéramos\nfuerais\nfueran\nfuese\nfueses\nfuésemos\nfueseis\nfuesen\nsintiendo\nsentido\nsentida\nsentidos\nsentidas\nsiente\nsentid\ntengo\ntienes\ntiene\ntenemos\ntenéis\ntienen\ntenga\ntengas\ntengamos\ntengáis\ntengan\ntendré\ntendrás\ntendrá\ntendremos\ntendréis\ntendrán\ntendría\ntendrías\ntendríamos\ntendríais\ntendrían\ntenía\ntenías\nteníamos\nteníais\ntenían\ntuve\ntuviste\ntuvo\ntuvimos\ntuvisteis\ntuvieron\ntuviera\ntuvieras\ntuviéramos\ntuvierais\ntuvieran\ntuviese\ntuvieses\ntuviésemos\ntuvieseis\ntuviesen\nteniendo\ntenido\ntenida\ntenidos\ntenidas\ntened\n\n\n\nfor item in stop.words(\"english\"):\n    print(item)\n\na\nabout\nabove\nafter\nagain\nagainst\nain\nall\nam\nan\nand\nany\nare\naren\naren't\nas\nat\nbe\nbecause\nbeen\nbefore\nbeing\nbelow\nbetween\nboth\nbut\nby\ncan\ncouldn\ncouldn't\nd\ndid\ndidn\ndidn't\ndo\ndoes\ndoesn\ndoesn't\ndoing\ndon\ndon't\ndown\nduring\neach\nfew\nfor\nfrom\nfurther\nhad\nhadn\nhadn't\nhas\nhasn\nhasn't\nhave\nhaven\nhaven't\nhaving\nhe\nhe'd\nhe'll\nher\nhere\nhers\nherself\nhe's\nhim\nhimself\nhis\nhow\ni\ni'd\nif\ni'll\ni'm\nin\ninto\nis\nisn\nisn't\nit\nit'd\nit'll\nit's\nits\nitself\ni've\njust\nll\nm\nma\nme\nmightn\nmightn't\nmore\nmost\nmustn\nmustn't\nmy\nmyself\nneedn\nneedn't\nno\nnor\nnot\nnow\no\nof\noff\non\nonce\nonly\nor\nother\nour\nours\nourselves\nout\nover\nown\nre\ns\nsame\nshan\nshan't\nshe\nshe'd\nshe'll\nshe's\nshould\nshouldn\nshouldn't\nshould've\nso\nsome\nsuch\nt\nthan\nthat\nthat'll\nthe\ntheir\ntheirs\nthem\nthemselves\nthen\nthere\nthese\nthey\nthey'd\nthey'll\nthey're\nthey've\nthis\nthose\nthrough\nto\ntoo\nunder\nuntil\nup\nve\nvery\nwas\nwasn\nwasn't\nwe\nwe'd\nwe'll\nwe're\nwere\nweren\nweren't\nwe've\nwhat\nwhen\nwhere\nwhich\nwhile\nwho\nwhom\nwhy\nwill\nwith\nwon\nwon't\nwouldn\nwouldn't\ny\nyou\nyou'd\nyou'll\nyour\nyou're\nyours\nyourself\nyourselves\nyou've",
    "crumbs": [
      "Natural Language Processing (NLP)",
      "NLTK"
    ]
  },
  {
    "objectID": "bagofwords.html",
    "href": "bagofwords.html",
    "title": "Bag Of Words",
    "section": "",
    "text": "from sklearn.feature_extraction.text import CountVectorizer\nimport pandas as pd\nimport nltk \nnltk.download('stopwords') # necessary for removal of stop words\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize, pos_tag\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk.corpus import wordnet\n\nnltk.download('punkt_tab') # necessary for tokenization\nnltk.download('wordnet') # necessary for lemmatization\nnltk.download('stopwords') # necessary for removal of stop words\nnltk.download('averaged_perceptron_tagger_eng') # necessary for POS tagging\nnltk.download('maxent_ne_chunker' ) # necessary for entity extraction\nnltk.download('omw-1.4') # necessary for lemmatization\nnltk.download('words')\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/usr-s3c/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt_tab to\n[nltk_data]     /Users/usr-s3c/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n[nltk_data] Downloading package wordnet to /Users/usr-s3c/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/usr-s3c/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n[nltk_data]     /Users/usr-s3c/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package maxent_ne_chunker to\n[nltk_data]     /Users/usr-s3c/nltk_data...\n[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /Users/usr-s3c/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n[nltk_data] Downloading package words to /Users/usr-s3c/nltk_data...\n[nltk_data]   Package words is already up-to-date!\n\n\nTrue\n\n\n\ncorpus = [\n    \"machine learning is fun 1990\",\n    \"Machine learning is part of AI 2000\",\n    \"I loving machine learning\",\n    \"I love AI\"\n]\n\n\nstop_words = stopwords.words(\"english\")\nvectorizer = CountVectorizer(stop_words= stop_words)\n\n\n\n\nlemmatizer = WordNetLemmatizer()   \n\ndef get_wordnet_pos(treebank_tag):\n    if treebank_tag.startswith('J'):\n        return wordnet.ADJ\n    elif treebank_tag.startswith('V'):\n        return wordnet.VERB\n    elif treebank_tag.startswith('N'):\n        return wordnet.NOUN\n    elif treebank_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN\n\ndef preprocessing_document(doc):\n    #1 Transformar en minusculas\n    doc = doc.lower()\n    #2 Tokenizar\n    tokens = word_tokenize(doc)\n    #3 Lematización con etiqueta POS\n    tagged_tokens = pos_tag(tokens)\n    #4 Filtrar números y Stopwords\n    filtered_tokens = [(word,pos) for word,pos in tagged_tokens if word.isalpha()]\n    #5 Lemmatización\n    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in filtered_tokens]\n\n    return \" \". join(lemmatized_words)\n\n\n\n\n\nfor doc in corpus:\n    r = preprocessing_document(doc)\n    print(f\"doc: {doc}, preprocesamiento: {r}\")\n\ndoc: machine learning is fun 1990, preprocesamiento: machine learning be fun\ndoc: Machine learning is part of AI 2000, preprocesamiento: machine learning be part of ai\ndoc: I loving machine learning, preprocesamiento: i love machine learning\ndoc: I love AI, preprocesamiento: i love ai\n\n\n\n\n\n\ncorpus_cleanned = [preprocessing_document(doc) for doc in corpus]\nX = vectorizer.fit_transform(corpus_cleanned)\n\n\nvectorizer.get_feature_names_out()\n\narray(['ai', 'fun', 'learning', 'love', 'machine', 'part'], dtype=object)\n\n\n\n\n\n\ndf = pd.DataFrame.sparse.from_spmatrix(X, columns= vectorizer.get_feature_names_out())\ndf\n\n\n\n\n\n\n\n\nai\nfun\nlearning\nlove\nmachine\npart\n\n\n\n\n0\n0\n1\n1\n0\n1\n0\n\n\n1\n1\n0\n1\n0\n1\n1\n\n\n2\n0\n0\n1\n1\n1\n0\n\n\n3\n1\n0\n0\n1\n0\n0",
    "crumbs": [
      "Natural Language Processing (NLP)",
      "Bag Of Words"
    ]
  },
  {
    "objectID": "bagofwords.html#matriz-de-ocurrencias",
    "href": "bagofwords.html#matriz-de-ocurrencias",
    "title": "Bag Of Words",
    "section": "",
    "text": "df = pd.DataFrame.sparse.from_spmatrix(X, columns= vectorizer.get_feature_names_out())\ndf\n\n\n\n\n\n\n\n\nai\nfun\nlearning\nlove\nmachine\npart\n\n\n\n\n0\n0\n1\n1\n0\n1\n0\n\n\n1\n1\n0\n1\n0\n1\n1\n\n\n2\n0\n0\n1\n1\n1\n0\n\n\n3\n1\n0\n0\n1\n0\n0",
    "crumbs": [
      "Natural Language Processing (NLP)",
      "Bag Of Words"
    ]
  },
  {
    "objectID": "bagofwords.html#limpieza-de-datos",
    "href": "bagofwords.html#limpieza-de-datos",
    "title": "Bag Of Words",
    "section": "",
    "text": "lemmatizer = WordNetLemmatizer()   \n\ndef get_wordnet_pos(treebank_tag):\n    if treebank_tag.startswith('J'):\n        return wordnet.ADJ\n    elif treebank_tag.startswith('V'):\n        return wordnet.VERB\n    elif treebank_tag.startswith('N'):\n        return wordnet.NOUN\n    elif treebank_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN\n\ndef preprocessing_document(doc):\n    #1 Transformar en minusculas\n    doc = doc.lower()\n    #2 Tokenizar\n    tokens = word_tokenize(doc)\n    #3 Lematización con etiqueta POS\n    tagged_tokens = pos_tag(tokens)\n    #4 Filtrar números y Stopwords\n    filtered_tokens = [(word,pos) for word,pos in tagged_tokens if word.isalpha()]\n    #5 Lemmatización\n    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in filtered_tokens]\n\n    return \" \". join(lemmatized_words)",
    "crumbs": [
      "Natural Language Processing (NLP)",
      "Bag Of Words"
    ]
  },
  {
    "objectID": "bagofwords.html#fit-transform",
    "href": "bagofwords.html#fit-transform",
    "title": "Bag Of Words",
    "section": "",
    "text": "corpus_cleanned = [preprocessing_document(doc) for doc in corpus]\nX = vectorizer.fit_transform(corpus_cleanned)\n\n\nvectorizer.get_feature_names_out()\n\narray(['ai', 'fun', 'learning', 'love', 'machine', 'part'], dtype=object)",
    "crumbs": [
      "Natural Language Processing (NLP)",
      "Bag Of Words"
    ]
  },
  {
    "objectID": "bagofwords.html#preprocesamiento-manual",
    "href": "bagofwords.html#preprocesamiento-manual",
    "title": "Bag Of Words",
    "section": "",
    "text": "for doc in corpus:\n    r = preprocessing_document(doc)\n    print(f\"doc: {doc}, preprocesamiento: {r}\")\n\ndoc: machine learning is fun 1990, preprocesamiento: machine learning be fun\ndoc: Machine learning is part of AI 2000, preprocesamiento: machine learning be part of ai\ndoc: I loving machine learning, preprocesamiento: i love machine learning\ndoc: I love AI, preprocesamiento: i love ai",
    "crumbs": [
      "Natural Language Processing (NLP)",
      "Bag Of Words"
    ]
  },
  {
    "objectID": "ftidf.html",
    "href": "ftidf.html",
    "title": "Bag Of Words",
    "section": "",
    "text": "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nimport pandas as pd\nimport nltk \nnltk.download('stopwords') # necessary for removal of stop words\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize, pos_tag\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk.corpus import wordnet\n\nnltk.download('punkt_tab') # necessary for tokenization\nnltk.download('wordnet') # necessary for lemmatization\nnltk.download('stopwords') # necessary for removal of stop words\nnltk.download('averaged_perceptron_tagger_eng') # necessary for POS tagging\nnltk.download('maxent_ne_chunker' ) # necessary for entity extraction\nnltk.download('omw-1.4') # necessary for lemmatization\nnltk.download('words')\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/usr-s3c/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt_tab to\n[nltk_data]     /Users/usr-s3c/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n[nltk_data] Downloading package wordnet to /Users/usr-s3c/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/usr-s3c/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n[nltk_data]     /Users/usr-s3c/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package maxent_ne_chunker to\n[nltk_data]     /Users/usr-s3c/nltk_data...\n[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /Users/usr-s3c/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n[nltk_data] Downloading package words to /Users/usr-s3c/nltk_data...\n[nltk_data]   Package words is already up-to-date!\n\n\nTrue\n\n\n\ncorpus = [\n    \"machine learning is fun 1990\",\n    \"Machine learning is part of AI 2000\",\n    \"I loving machine learning\",\n    \"I love AI\"\n]\n\n\nstop_words = stopwords.words(\"english\")\nvectorizer = TfidfVectorizer(stop_words= stop_words)\n\n\n\n\nlemmatizer = WordNetLemmatizer()   \n\ndef get_wordnet_pos(treebank_tag):\n    if treebank_tag.startswith('J'):\n        return wordnet.ADJ\n    elif treebank_tag.startswith('V'):\n        return wordnet.VERB\n    elif treebank_tag.startswith('N'):\n        return wordnet.NOUN\n    elif treebank_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN\n\ndef preprocessing_document(doc):\n    #1 Transformar en minusculas\n    doc = doc.lower()\n    #2 Tokenizar\n    tokens = word_tokenize(doc)\n    #3 Lematización con etiqueta POS\n    tagged_tokens = pos_tag(tokens)\n    #4 Filtrar números y Stopwords\n    filtered_tokens = [(word,pos) for word,pos in tagged_tokens if word.isalpha()]\n    #5 Lemmatización\n    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in filtered_tokens]\n\n    return \" \". join(lemmatized_words)\n\n\n\n\n\nfor doc in corpus:\n    r = preprocessing_document(doc)\n    print(f\"doc: {doc}, preprocesamiento: {r}\")\n\ndoc: machine learning is fun 1990, preprocesamiento: machine learning be fun\ndoc: Machine learning is part of AI 2000, preprocesamiento: machine learning be part of ai\ndoc: I loving machine learning, preprocesamiento: i love machine learning\ndoc: I love AI, preprocesamiento: i love ai\n\n\n\n\n\n\ncorpus_cleanned = [preprocessing_document(doc) for doc in corpus]\nX = vectorizer.fit_transform(corpus_cleanned)\n\n\nvectorizer.get_feature_names_out()\n\narray(['ai', 'fun', 'learning', 'love', 'machine', 'part'], dtype=object)\n\n\n\n\n\n\ndf = pd.DataFrame.sparse.from_spmatrix(X, columns= vectorizer.get_feature_names_out())\ndf\n\n\n\n\n\n\n\n\nai\nfun\nlearning\nlove\nmachine\npart\n\n\n\n\n0\n0\n0.742306\n0.473804\n0\n0.473804\n0\n\n\n1\n0.5051\n0\n0.408922\n0\n0.408922\n0.640655\n\n\n2\n0\n0\n0.53257\n0.657829\n0.53257\n0\n\n\n3\n0.707107\n0\n0\n0.707107\n0\n0"
  },
  {
    "objectID": "ftidf.html#limpieza-de-datos",
    "href": "ftidf.html#limpieza-de-datos",
    "title": "Bag Of Words",
    "section": "",
    "text": "lemmatizer = WordNetLemmatizer()   \n\ndef get_wordnet_pos(treebank_tag):\n    if treebank_tag.startswith('J'):\n        return wordnet.ADJ\n    elif treebank_tag.startswith('V'):\n        return wordnet.VERB\n    elif treebank_tag.startswith('N'):\n        return wordnet.NOUN\n    elif treebank_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN\n\ndef preprocessing_document(doc):\n    #1 Transformar en minusculas\n    doc = doc.lower()\n    #2 Tokenizar\n    tokens = word_tokenize(doc)\n    #3 Lematización con etiqueta POS\n    tagged_tokens = pos_tag(tokens)\n    #4 Filtrar números y Stopwords\n    filtered_tokens = [(word,pos) for word,pos in tagged_tokens if word.isalpha()]\n    #5 Lemmatización\n    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in filtered_tokens]\n\n    return \" \". join(lemmatized_words)"
  },
  {
    "objectID": "ftidf.html#preprocesamiento-manual",
    "href": "ftidf.html#preprocesamiento-manual",
    "title": "Bag Of Words",
    "section": "",
    "text": "for doc in corpus:\n    r = preprocessing_document(doc)\n    print(f\"doc: {doc}, preprocesamiento: {r}\")\n\ndoc: machine learning is fun 1990, preprocesamiento: machine learning be fun\ndoc: Machine learning is part of AI 2000, preprocesamiento: machine learning be part of ai\ndoc: I loving machine learning, preprocesamiento: i love machine learning\ndoc: I love AI, preprocesamiento: i love ai"
  },
  {
    "objectID": "ftidf.html#fit-transform",
    "href": "ftidf.html#fit-transform",
    "title": "Bag Of Words",
    "section": "",
    "text": "corpus_cleanned = [preprocessing_document(doc) for doc in corpus]\nX = vectorizer.fit_transform(corpus_cleanned)\n\n\nvectorizer.get_feature_names_out()\n\narray(['ai', 'fun', 'learning', 'love', 'machine', 'part'], dtype=object)"
  },
  {
    "objectID": "ftidf.html#matriz-de-ocurrencias",
    "href": "ftidf.html#matriz-de-ocurrencias",
    "title": "Bag Of Words",
    "section": "",
    "text": "df = pd.DataFrame.sparse.from_spmatrix(X, columns= vectorizer.get_feature_names_out())\ndf\n\n\n\n\n\n\n\n\nai\nfun\nlearning\nlove\nmachine\npart\n\n\n\n\n0\n0\n0.742306\n0.473804\n0\n0.473804\n0\n\n\n1\n0.5051\n0\n0.408922\n0\n0.408922\n0.640655\n\n\n2\n0\n0\n0.53257\n0.657829\n0.53257\n0\n\n\n3\n0.707107\n0\n0\n0.707107\n0\n0"
  },
  {
    "objectID": "tfidf.html",
    "href": "tfidf.html",
    "title": "Bag Of Words",
    "section": "",
    "text": "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nimport pandas as pd\nimport nltk \nnltk.download('stopwords') # necessary for removal of stop words\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize, pos_tag\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk.corpus import wordnet\n\nnltk.download('punkt_tab') # necessary for tokenization\nnltk.download('wordnet') # necessary for lemmatization\nnltk.download('stopwords') # necessary for removal of stop words\nnltk.download('averaged_perceptron_tagger_eng') # necessary for POS tagging\nnltk.download('maxent_ne_chunker' ) # necessary for entity extraction\nnltk.download('omw-1.4') # necessary for lemmatization\nnltk.download('words')\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/usr-s3c/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt_tab to\n[nltk_data]     /Users/usr-s3c/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n[nltk_data] Downloading package wordnet to /Users/usr-s3c/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/usr-s3c/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n[nltk_data]     /Users/usr-s3c/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package maxent_ne_chunker to\n[nltk_data]     /Users/usr-s3c/nltk_data...\n[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /Users/usr-s3c/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n[nltk_data] Downloading package words to /Users/usr-s3c/nltk_data...\n[nltk_data]   Package words is already up-to-date!\n\n\nTrue\n\n\n\ncorpus = [\n    \"machine learning is fun 1990\",\n    \"Machine learning is part of AI 2000\",\n    \"I loving machine learning\",\n    \"I love AI\"\n]\n\n\nstop_words = stopwords.words(\"english\")\nvectorizer = TfidfVectorizer(stop_words= stop_words)\n\n\n\n\nlemmatizer = WordNetLemmatizer()   \n\ndef get_wordnet_pos(treebank_tag):\n    if treebank_tag.startswith('J'):\n        return wordnet.ADJ\n    elif treebank_tag.startswith('V'):\n        return wordnet.VERB\n    elif treebank_tag.startswith('N'):\n        return wordnet.NOUN\n    elif treebank_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN\n\ndef preprocessing_document(doc):\n    #1 Transformar en minusculas\n    doc = doc.lower()\n    #2 Tokenizar\n    tokens = word_tokenize(doc)\n    #3 Lematización con etiqueta POS\n    tagged_tokens = pos_tag(tokens)\n    #4 Filtrar números y Stopwords\n    filtered_tokens = [(word,pos) for word,pos in tagged_tokens if word.isalpha()]\n    #5 Lemmatización\n    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in filtered_tokens]\n\n    return \" \". join(lemmatized_words)\n\n\n\n\n\nfor doc in corpus:\n    r = preprocessing_document(doc)\n    print(f\"doc: {doc}, preprocesamiento: {r}\")\n\ndoc: machine learning is fun 1990, preprocesamiento: machine learning be fun\ndoc: Machine learning is part of AI 2000, preprocesamiento: machine learning be part of ai\ndoc: I loving machine learning, preprocesamiento: i love machine learning\ndoc: I love AI, preprocesamiento: i love ai\n\n\n\n\n\n\ncorpus_cleanned = [preprocessing_document(doc) for doc in corpus]\nX = vectorizer.fit_transform(corpus_cleanned)\n\n\nvectorizer.get_feature_names_out()\n\narray(['ai', 'fun', 'learning', 'love', 'machine', 'part'], dtype=object)\n\n\n\n\n\n\ndf = pd.DataFrame.sparse.from_spmatrix(X, columns= vectorizer.get_feature_names_out())\ndf\n\n\n\n\n\n\n\n\nai\nfun\nlearning\nlove\nmachine\npart\n\n\n\n\n0\n0\n0.742306\n0.473804\n0\n0.473804\n0\n\n\n1\n0.5051\n0\n0.408922\n0\n0.408922\n0.640655\n\n\n2\n0\n0\n0.53257\n0.657829\n0.53257\n0\n\n\n3\n0.707107\n0\n0\n0.707107\n0\n0",
    "crumbs": [
      "Natural Language Processing (NLP)",
      "Term - Frecuency | Inverse Document Freceuncy"
    ]
  },
  {
    "objectID": "tfidf.html#limpieza-de-datos",
    "href": "tfidf.html#limpieza-de-datos",
    "title": "Bag Of Words",
    "section": "",
    "text": "lemmatizer = WordNetLemmatizer()   \n\ndef get_wordnet_pos(treebank_tag):\n    if treebank_tag.startswith('J'):\n        return wordnet.ADJ\n    elif treebank_tag.startswith('V'):\n        return wordnet.VERB\n    elif treebank_tag.startswith('N'):\n        return wordnet.NOUN\n    elif treebank_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN\n\ndef preprocessing_document(doc):\n    #1 Transformar en minusculas\n    doc = doc.lower()\n    #2 Tokenizar\n    tokens = word_tokenize(doc)\n    #3 Lematización con etiqueta POS\n    tagged_tokens = pos_tag(tokens)\n    #4 Filtrar números y Stopwords\n    filtered_tokens = [(word,pos) for word,pos in tagged_tokens if word.isalpha()]\n    #5 Lemmatización\n    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in filtered_tokens]\n\n    return \" \". join(lemmatized_words)",
    "crumbs": [
      "Natural Language Processing (NLP)",
      "Term - Frecuency | Inverse Document Freceuncy"
    ]
  },
  {
    "objectID": "tfidf.html#preprocesamiento-manual",
    "href": "tfidf.html#preprocesamiento-manual",
    "title": "Bag Of Words",
    "section": "",
    "text": "for doc in corpus:\n    r = preprocessing_document(doc)\n    print(f\"doc: {doc}, preprocesamiento: {r}\")\n\ndoc: machine learning is fun 1990, preprocesamiento: machine learning be fun\ndoc: Machine learning is part of AI 2000, preprocesamiento: machine learning be part of ai\ndoc: I loving machine learning, preprocesamiento: i love machine learning\ndoc: I love AI, preprocesamiento: i love ai",
    "crumbs": [
      "Natural Language Processing (NLP)",
      "Term - Frecuency | Inverse Document Freceuncy"
    ]
  },
  {
    "objectID": "tfidf.html#fit-transform",
    "href": "tfidf.html#fit-transform",
    "title": "Bag Of Words",
    "section": "",
    "text": "corpus_cleanned = [preprocessing_document(doc) for doc in corpus]\nX = vectorizer.fit_transform(corpus_cleanned)\n\n\nvectorizer.get_feature_names_out()\n\narray(['ai', 'fun', 'learning', 'love', 'machine', 'part'], dtype=object)",
    "crumbs": [
      "Natural Language Processing (NLP)",
      "Term - Frecuency | Inverse Document Freceuncy"
    ]
  },
  {
    "objectID": "tfidf.html#matriz-de-ocurrencias",
    "href": "tfidf.html#matriz-de-ocurrencias",
    "title": "Bag Of Words",
    "section": "",
    "text": "df = pd.DataFrame.sparse.from_spmatrix(X, columns= vectorizer.get_feature_names_out())\ndf\n\n\n\n\n\n\n\n\nai\nfun\nlearning\nlove\nmachine\npart\n\n\n\n\n0\n0\n0.742306\n0.473804\n0\n0.473804\n0\n\n\n1\n0.5051\n0\n0.408922\n0\n0.408922\n0.640655\n\n\n2\n0\n0\n0.53257\n0.657829\n0.53257\n0\n\n\n3\n0.707107\n0\n0\n0.707107\n0\n0",
    "crumbs": [
      "Natural Language Processing (NLP)",
      "Term - Frecuency | Inverse Document Freceuncy"
    ]
  },
  {
    "objectID": "categorical_pipeline.html#seleccionar-las-columnas-por-tipo",
    "href": "categorical_pipeline.html#seleccionar-las-columnas-por-tipo",
    "title": "Categorical Encoding",
    "section": "Seleccionar las columnas por tipo",
    "text": "Seleccionar las columnas por tipo\n\n#for i in df['education'].unique():\n#    print(i)\n\neducation_order = [\"Preschool\",\n                    \"1st-4th\",\n                    \"5th-6th\",\n                    \"7th-8th\",\n                    \"9th\",\n                    \"10th\",\n                    \"11th\",\n                    \"12th\",\n                    \"HS-grad\",\n                    \"Some-college\",\n                    \"Assoc-acdm\",\n                    \"Assoc-voc\",\n                    \"Prof-school\",\n                    \"Bachelors\",\n                    \"Masters\",\n                    \"Doctorate\"]",
    "crumbs": [
      "Home",
      "Classification",
      "Categorización con Pipeline"
    ]
  },
  {
    "objectID": "categorical_pipeline.html#categorical-column-ordinal-education",
    "href": "categorical_pipeline.html#categorical-column-ordinal-education",
    "title": "Categorical Encoding",
    "section": "Categorical column ordinal education",
    "text": "Categorical column ordinal education\n\neducation_order\n\n['Preschool',\n '1st-4th',\n '5th-6th',\n '7th-8th',\n '9th',\n '10th',\n '11th',\n '12th',\n 'HS-grad',\n 'Some-college',\n 'Assoc-acdm',\n 'Assoc-voc',\n 'Prof-school',\n 'Bachelors',\n 'Masters',\n 'Doctorate']",
    "crumbs": [
      "Home",
      "Classification",
      "Categorización con Pipeline"
    ]
  },
  {
    "objectID": "categorical_pipeline.html#categorical-columns-nominal",
    "href": "categorical_pipeline.html#categorical-columns-nominal",
    "title": "Categorical Encoding",
    "section": "Categorical columns nominal",
    "text": "Categorical columns nominal\n\ncategorical_columns = X.select_dtypes(include='category').columns.tolist()\ncategorical_columns_nominal = [col for col in categorical_columns if col != 'education']\ncategorical_columns_nominal\n\n['workclass',\n 'marital-status',\n 'occupation',\n 'relationship',\n 'race',\n 'sex',\n 'native-country']",
    "crumbs": [
      "Home",
      "Classification",
      "Categorización con Pipeline"
    ]
  },
  {
    "objectID": "categorical_pipeline.html#numerical-columns",
    "href": "categorical_pipeline.html#numerical-columns",
    "title": "Categorical Encoding",
    "section": "Numerical columns",
    "text": "Numerical columns\n\nnumerical_columns = X.select_dtypes(include='number').columns.tolist()\nnumerical_columns\n\n['age',\n 'fnlwgt',\n 'education-num',\n 'capital-gain',\n 'capital-loss',\n 'hours-per-week']",
    "crumbs": [
      "Home",
      "Classification",
      "Categorización con Pipeline"
    ]
  },
  {
    "objectID": "categorical_pipeline.html#features-transform",
    "href": "categorical_pipeline.html#features-transform",
    "title": "Categorical Encoding",
    "section": "Features transform",
    "text": "Features transform\n\nprocessor = ColumnTransformer(\n    transformers=[\n        ('num',StandardScaler(),numerical_columns),\n        ('edu',OrdinalEncoder(categories=[education_order]),['education']),\n        ('onehot',OneHotEncoder(handle_unknown='ignore'),categorical_columns_nominal)\n    ]\n)",
    "crumbs": [
      "Home",
      "Classification",
      "Categorización con Pipeline"
    ]
  },
  {
    "objectID": "categorical_pipeline.html#target-transform",
    "href": "categorical_pipeline.html#target-transform",
    "title": "Categorical Encoding",
    "section": "Target transform",
    "text": "Target transform\n\nlabel_target_encoded = LabelEncoder()\ny_train_encoded = label_target_encoded.fit_transform(y_train)\ny_test_encoded = label_target_encoded.fit_transform(y_test)",
    "crumbs": [
      "Home",
      "Classification",
      "Categorización con Pipeline"
    ]
  },
  {
    "objectID": "categorical_pipeline.html#pipeline",
    "href": "categorical_pipeline.html#pipeline",
    "title": "Categorical Encoding",
    "section": "Pipeline",
    "text": "Pipeline\n\npipeline = Pipeline([\n    ('preprocesamiento',processor),\n    ('classifier',LogisticRegression(max_iter=10000))\n])",
    "crumbs": [
      "Home",
      "Classification",
      "Categorización con Pipeline"
    ]
  },
  {
    "objectID": "categorical_pipeline.html#entrenamiento",
    "href": "categorical_pipeline.html#entrenamiento",
    "title": "Categorical Encoding",
    "section": "Entrenamiento",
    "text": "Entrenamiento\n\npipeline.fit(X_train,y_train_encoded)\n\nPipeline(steps=[('preprocesamiento',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['age', 'fnlwgt',\n                                                   'education-num',\n                                                   'capital-gain',\n                                                   'capital-loss',\n                                                   'hours-per-week']),\n                                                 ('edu',\n                                                  OrdinalEncoder(categories=[['Preschool',\n                                                                              '1st-4th',\n                                                                              '5th-6th',\n                                                                              '7th-8th',\n                                                                              '9th',\n                                                                              '10th',\n                                                                              '11th',\n                                                                              '12th',\n                                                                              'HS-grad',\n                                                                              'Some-college',\n                                                                              'Assoc-acdm',\n                                                                              'Assoc-voc',\n                                                                              'Prof-school',\n                                                                              'Bachelors',\n                                                                              'Masters',\n                                                                              'Doctorate']]),\n                                                  ['education']),\n                                                 ('onehot',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['workclass',\n                                                   'marital-status',\n                                                   'occupation', 'relationship',\n                                                   'race', 'sex',\n                                                   'native-country'])])),\n                ('classifier', LogisticRegression(max_iter=10000))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps \n[('preprocesamiento', ...), ('classifier', ...)]\n\n\n\ntransform_input \nNone\n\n\n\nmemory \nNone\n\n\n\nverbose \nFalse\n\n\n\n\n            \n        \n    preprocesamiento: ColumnTransformer?Documentation for preprocesamiento: ColumnTransformer\n        \n            \n                Parameters\n                \n\n\n\n\ntransformers \n[('num', ...), ('edu', ...), ...]\n\n\n\nremainder \n'drop'\n\n\n\nsparse_threshold \n0.3\n\n\n\nn_jobs \nNone\n\n\n\ntransformer_weights \nNone\n\n\n\nverbose \nFalse\n\n\n\nverbose_feature_names_out \nTrue\n\n\n\nforce_int_remainder_cols \n'deprecated'\n\n\n\n\n            \n        \n    num['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    edu['education']OrdinalEncoder?Documentation for OrdinalEncoder\n        \n            \n                Parameters\n                \n\n\n\n\ncategories \n[['Preschool', '1st-4th', ...]]\n\n\n\ndtype \n&lt;class 'numpy.float64'&gt;\n\n\n\nhandle_unknown \n'error'\n\n\n\nunknown_value \nNone\n\n\n\nencoded_missing_value \nnan\n\n\n\nmin_frequency \nNone\n\n\n\nmax_categories \nNone\n\n\n\n\n            \n        \n    onehot['workclass', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']OneHotEncoder?Documentation for OneHotEncoder\n        \n            \n                Parameters\n                \n\n\n\n\ncategories \n'auto'\n\n\n\ndrop \nNone\n\n\n\nsparse_output \nTrue\n\n\n\ndtype \n&lt;class 'numpy.float64'&gt;\n\n\n\nhandle_unknown \n'ignore'\n\n\n\nmin_frequency \nNone\n\n\n\nmax_categories \nNone\n\n\n\nfeature_name_combiner \n'concat'\n\n\n\n\n            \n        \n    LogisticRegression?Documentation for LogisticRegression\n        \n            \n                Parameters\n                \n\n\n\n\npenalty \n'l2'\n\n\n\ndual \nFalse\n\n\n\ntol \n0.0001\n\n\n\nC \n1.0\n\n\n\nfit_intercept \nTrue\n\n\n\nintercept_scaling \n1\n\n\n\nclass_weight \nNone\n\n\n\nrandom_state \nNone\n\n\n\nsolver \n'lbfgs'\n\n\n\nmax_iter \n10000\n\n\n\nmulti_class \n'deprecated'\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nl1_ratio \nNone",
    "crumbs": [
      "Home",
      "Classification",
      "Categorización con Pipeline"
    ]
  },
  {
    "objectID": "categorical_pipeline.html#predecir",
    "href": "categorical_pipeline.html#predecir",
    "title": "Categorical Encoding",
    "section": "Predecir",
    "text": "Predecir\n\ny_pred = pipeline.predict(X_test)",
    "crumbs": [
      "Home",
      "Classification",
      "Categorización con Pipeline"
    ]
  },
  {
    "objectID": "categorical_pipeline.html#evaluar-los-resultados",
    "href": "categorical_pipeline.html#evaluar-los-resultados",
    "title": "Categorical Encoding",
    "section": "Evaluar los resultados",
    "text": "Evaluar los resultados\n\nprint(classification_report(y_test_encoded,y_pred,target_names=label_target_encoded.classes_))\n\n              precision    recall  f1-score   support\n\n       &lt;=50K       0.88      0.93      0.90      6803\n        &gt;50K       0.73      0.60      0.66      2242\n\n    accuracy                           0.85      9045\n   macro avg       0.80      0.76      0.78      9045\nweighted avg       0.84      0.85      0.84      9045",
    "crumbs": [
      "Home",
      "Classification",
      "Categorización con Pipeline"
    ]
  },
  {
    "objectID": "categorical_pipeline.html#matriz-de-confusión",
    "href": "categorical_pipeline.html#matriz-de-confusión",
    "title": "Categorical Encoding",
    "section": "Matriz de confusión",
    "text": "Matriz de confusión\n\nConfusionMatrixDisplay.from_predictions(y_test_encoded,y_pred)",
    "crumbs": [
      "Home",
      "Classification",
      "Categorización con Pipeline"
    ]
  }
]